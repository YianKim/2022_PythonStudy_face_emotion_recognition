{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras_UPS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YianKim/2022_uncertainty_aware_semisupervise/blob/main/Keras_UPS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ik7Qx5iO8lQ_"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import PIL\n",
        "import pickle\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "from keras.layers.core import Lambda\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import MaxPooling2D, AveragePooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras import optimizers\n",
        "from keras.callbacks import *\n",
        "from sklearn.metrics import *\n",
        "from keras.models import load_model\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as backend\n",
        "import math\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# cifar10 dataset"
      ],
      "metadata": {
        "id": "2A3-ednCG7ZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cifar10 = keras.datasets.cifar10 \n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "train_images = train_images/255\n",
        "test_images = test_images/255"
      ],
      "metadata": {
        "id": "LAzGxvve8pgp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dummy_labels(labels):\n",
        "  zero_labels = np.zeros([labels.shape[0], 10], np.int8)  \n",
        "  for i in range(labels.shape[0]):\n",
        "    zero_labels[i][labels[i]] = 1\n",
        "  return(zero_labels)"
      ],
      "metadata": {
        "id": "zqUm92WTGW3p"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = dummy_labels(train_labels)\n",
        "test_labels = dummy_labels(test_labels)"
      ],
      "metadata": {
        "id": "vFVXfHQNGtmq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1000 labeled, 49000 unlabeled\n",
        "indx = random.sample(range(train_labels.shape[0]),train_labels.shape[0])\n",
        "\n",
        "lbl_train_images = train_images[indx[:1000]]\n",
        "ubl_train_images = train_images[indx[1000:]]\n",
        "\n",
        "lbl_train_labels = train_labels[indx[:1000]]\n",
        "ubl_train_labels = train_labels[indx[1000:]]"
      ],
      "metadata": {
        "id": "WOqDJtXCG2ov"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pseudo labeling"
      ],
      "metadata": {
        "id": "CqLLHcZLuZmP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 스케줄러"
      ],
      "metadata": {
        "id": "ewpemDXc8nEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SGDR(Callback):\n",
        "    \"\"\"This callback implements the learning rate schedule for\n",
        "    Stochastic Gradient Descent with warm Restarts (SGDR),\n",
        "    as proposed by Loshchilov & Hutter (https://arxiv.org/abs/1608.03983).\n",
        "    \n",
        "    The learning rate at each epoch is computed as:\n",
        "    lr(i) = min_lr + 0.5 * (max_lr - min_lr) * (1 + cos(pi * i/num_epochs))\n",
        "    \n",
        "    Here, num_epochs is the number of epochs in the current cycle, which starts\n",
        "    with base_epochs initially and is multiplied by mul_epochs after each cycle.\n",
        "    \n",
        "    # Example\n",
        "        ```python\n",
        "            sgdr = CyclicLR(min_lr=0.0, max_lr=0.05,\n",
        "                                base_epochs=10, mul_epochs=2)\n",
        "            model.compile(optimizer=keras.optimizers.SGD(decay=1e-4, momentum=0.9),\n",
        "                          loss=loss)\n",
        "            model.fit(X_train, Y_train, callbacks=[sgdr])\n",
        "        ```\n",
        "    \n",
        "    # Arguments\n",
        "        min_lr: minimum learning rate reached at the end of each cycle.\n",
        "        max_lr: maximum learning rate used at the beginning of each cycle.\n",
        "        base_epochs: number of epochs in the first cycle.\n",
        "        mul_epochs: factor with which the number of epochs is multiplied\n",
        "                after each cycle.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, min_lr=0.0, max_lr=0.03, base_epochs=20, mul_epochs=2):\n",
        "        super(SGDR, self).__init__()\n",
        "\n",
        "        self.min_lr = min_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.base_epochs = base_epochs\n",
        "        self.mul_epochs = mul_epochs\n",
        "\n",
        "        self.cycles = 0.\n",
        "        self.cycle_iterations = 0.\n",
        "        self.trn_iterations = 0.\n",
        "\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self, new_min_lr=None, new_max_lr=None,\n",
        "               new_base_epochs=None, new_mul_epochs=None):\n",
        "        \"\"\"Resets cycle iterations.\"\"\"\n",
        "        \n",
        "        if new_min_lr != None:\n",
        "            self.min_lr = new_min_lr\n",
        "        if new_max_lr != None:\n",
        "            self.max_lr = new_max_lr\n",
        "        if new_base_epochs != None:\n",
        "            self.base_epochs = new_base_epochs\n",
        "        if new_mul_epochs != None:\n",
        "            self.mul_epochs = new_mul_epochs\n",
        "        self.cycles = 0.\n",
        "        self.cycle_iterations = 0.\n",
        "        \n",
        "    def sgdr(self):\n",
        "        \n",
        "        cycle_epochs = self.base_epochs * (self.mul_epochs ** self.cycles)\n",
        "        tide = ((self.cycles == 0) * 1) * (self.cycle_iterations*self.max_lr + (self.base_epochs - self.cycle_iterations)*self.min_lr) / self.base_epochs + ((self.cycles != 0) * 1)*(self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(np.pi * (self.cycle_iterations + 1) / cycle_epochs)))\n",
        "        return tide\n",
        "        \n",
        "    def on_train_begin(self, logs=None):\n",
        "        \n",
        "        if self.cycle_iterations == 0:\n",
        "            K.set_value(self.model.optimizer.lr, self.max_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.sgdr())\n",
        "            \n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        \n",
        "        logs = logs or {}\n",
        "        logs['lr'] = K.get_value(self.model.optimizer.lr)\n",
        "        \n",
        "        self.trn_iterations += 1\n",
        "        self.cycle_iterations += 1\n",
        "        if self.cycle_iterations >= self.base_epochs * (self.mul_epochs ** self.cycles):\n",
        "            self.cycles += 1\n",
        "            self.cycle_iterations = 0\n",
        "            K.set_value(self.model.optimizer.lr, self.max_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.sgdr())"
      ],
      "metadata": {
        "id": "iKyn6Njs7vqa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### main"
      ],
      "metadata": {
        "id": "DjpoH_dl8qPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def PermaDropout(rate):\n",
        "    return Lambda(lambda x: K.dropout(x, level=rate))\n",
        "\n",
        "def create_cnn_13():\n",
        "  conv1a = Conv2D(128, (3,3), padding = 'same', activation='relu')\n",
        "  bn1a = BatchNormalization()\n",
        "  conv1b = Conv2D(128, (3,3), padding = 'same', activation='relu')\n",
        "  bn1b = BatchNormalization()\n",
        "  conv1c = Conv2D(128, (3,3), padding = 'same', activation='relu')\n",
        "  bn1c = BatchNormalization()\n",
        "  pl1 = MaxPooling2D(2, 2)\n",
        "  MCdrop1 = Dropout(0.3)\n",
        "\n",
        "  conv2a = Conv2D(256, (3,3), padding = 'same', activation='relu')\n",
        "  bn2a = BatchNormalization()\n",
        "  conv2b = Conv2D(256, (3,3), padding = 'same', activation='relu')\n",
        "  bn2b = BatchNormalization()\n",
        "  conv2c = Conv2D(256, (3,3), padding = 'same', activation='relu')\n",
        "  bn2c = BatchNormalization()\n",
        "  pl2 = MaxPooling2D(2, 2)\n",
        "  MCdrop2 = Dropout(0.3)\n",
        "\n",
        "  conv3a = Conv2D(512, (3,3), activation='relu')\n",
        "  bn3a = BatchNormalization()\n",
        "  conv3b = Conv2D(256, (1,1), activation='relu')\n",
        "  bn3b = BatchNormalization()\n",
        "  conv3c = Conv2D(128, (1,1), activation='relu')\n",
        "  bn3c = BatchNormalization()\n",
        "  pl3 = AveragePooling2D(6, 2)\n",
        "\n",
        "  fc = Dense(10, activation='softmax')\n",
        "  activ = keras.layers.LeakyReLU(0.1)\n",
        "\n",
        "  model = Sequential([\n",
        "                      keras.Input(shape=(32, 32, 3)), \n",
        "                      conv1a, bn1a, activ,\n",
        "                      conv1b, bn1b, activ,\n",
        "                      conv1c, bn1c, activ,\n",
        "                      pl1, MCdrop1,\n",
        "\n",
        "                      conv2a, bn2a, activ,\n",
        "                      conv2b, bn2b, activ,\n",
        "                      conv2c, bn2c, activ,\n",
        "                      pl2, MCdrop2,\n",
        "\n",
        "                      conv3a, bn3a, activ,\n",
        "                      conv3b, bn3b, activ,\n",
        "                      conv3c, bn3c, activ,\n",
        "                      pl3, Flatten(),\n",
        "                      \n",
        "                      fc\n",
        "                      ])\n",
        "  \n",
        "  return model\n",
        "\n",
        "def compile_cnn_13(model):\n",
        "\n",
        "  opt = keras.optimizers.SGD(0.03, momentum=0.9)\n",
        "\n",
        "  model.compile(\n",
        "    optimizer = opt,\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        "  )\n",
        "\n",
        "  return model\n",
        "\n",
        "def cnn_13():\n",
        "\n",
        "  model = create_cnn_13()\n",
        "  model = compile_cnn_13(model)\n",
        "\n",
        "  return model\n",
        "\n",
        "def fit_and_labeling_cnn_13(Epoch, Batch):\n",
        "  \n",
        "  numsamples = int(np.min(list(Counter(np.argmax(lbl_train_labels, axis=1)).values()))*0.9)\n",
        "  multlabel = np.argmax(lbl_train_labels, axis=1)\n",
        "  sufindx = random.sample(range(len(multlabel)), len(multlabel))\n",
        "\n",
        "  idxcounter = [0,0,0,0,0,0,0,0,0,0]\n",
        "  idxsample = []\n",
        "\n",
        "  for i in sufindx:\n",
        "    if idxcounter[multlabel[i]] <= numsamples:\n",
        "      idxcounter[multlabel[i]] += 1\n",
        "      idxsample.append(i)\n",
        "\n",
        "  lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
        "  early_stopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=30, mode='auto')\n",
        "  sgdr = SGDR(min_lr=0.0, max_lr=0.03, base_epochs=20) #스케줄러\n",
        "  \n",
        "  model.fit(\n",
        "      x=lbl_train_images[idxsample],\n",
        "      y=lbl_train_labels[idxsample],\n",
        "      epochs=Epoch,\n",
        "      verbose=0,\n",
        "      validation_split=0.2,\n",
        "      batch_size=Batch,\n",
        "      callbacks=[sgdr, early_stopper]\n",
        "  )\n",
        "\n",
        "  pred = model.predict(test_images)\n",
        "  acc = (np.argmax(pred,axis=1) == np.argmax(test_labels,axis=1))*1\n",
        "  print(\"test set 성능 : \" + str(sum(acc)/len(acc)))\n",
        "  print(Counter(np.argmax(lbl_train_labels[idxsample], axis=1)))\n",
        "\n",
        "  for predsamples in tqdm(range(30)):\n",
        "    if predsamples == 0 :\n",
        "      predictions = model.predict(ubl_train_images)\n",
        "      predictions = predictions.reshape((1,) + predictions.shape)\n",
        "    else:\n",
        "      pred = model.predict(ubl_train_images)\n",
        "      pred = pred.reshape((1,) + pred.shape)\n",
        "      predictions = np.concatenate((predictions, pred))\n",
        "\n",
        "  return predictions"
      ],
      "metadata": {
        "id": "YRCDNliwfudJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_selecting():\n",
        "  K_conf = 0.9\n",
        "  K_uncert = 1e-8\n",
        "\n",
        "  pseudo = np.argmax(np.mean(predictions, axis=0), axis=1)\n",
        "  conf = np.max(np.mean(predictions, axis=0), axis=1)\n",
        "  uncert = np.std(predictions, axis=0)\n",
        "  uncert = np.array([uncert[i][pseudo[i]] for i in range(len(pseudo))])\n",
        "\n",
        "  select_pseudo = (1*(conf > K_conf)) * (1*(uncert < K_uncert))\n",
        "\n",
        "  labels = []\n",
        "  for i in pseudo:\n",
        "    temp = [0,0,0,0,0,0,0,0,0,0]\n",
        "    temp[i] = 1\n",
        "    labels.append(temp)\n",
        "  pseudo = np.array(labels)\n",
        "\n",
        "  lbl_idx = []\n",
        "  ubl_idx = []\n",
        "  k = 0\n",
        "  for i in select_pseudo:\n",
        "    if i == 1:\n",
        "      lbl_idx.append(k)\n",
        "    if i == 0:\n",
        "      ubl_idx.append(k)\n",
        "    k += 1\n",
        "\n",
        "  image1 = np.concatenate((lbl_train_images, ubl_train_images[lbl_idx]))\n",
        "  label1 = np.concatenate((lbl_train_labels, pseudo[lbl_idx]))\n",
        "  image2 = ubl_train_images[ubl_idx]\n",
        "\n",
        "  return image1, label1, image2"
      ],
      "metadata": {
        "id": "EEBsyUAAg-W2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for itr in range(500):\n",
        "  model = cnn_13()\n",
        "  predictions = fit_and_labeling_cnn_13(3000, 64)\n",
        "  lbl_train_images, lbl_train_labels, ubl_train_images = label_selecting()"
      ],
      "metadata": {
        "id": "uyAPJv8yDPYy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}