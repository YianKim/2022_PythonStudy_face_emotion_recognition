{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras_UPS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMx2QwAqWeHqddTTttTzGuG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YianKim/2022_uncertainty_aware_semisupervise/blob/main/Keras_UPS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Ik7Qx5iO8lQ_"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import PIL\n",
        "import pickle\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "from keras.layers.core import Lambda\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import MaxPooling2D, AveragePooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import LSTM\n",
        "from keras import optimizers\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard, ModelCheckpoint\n",
        "from sklearn.metrics import *\n",
        "from keras.models import load_model\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# cifar10 dataset"
      ],
      "metadata": {
        "id": "2A3-ednCG7ZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cifar10 = keras.datasets.cifar10 \n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "train_images = train_images/255\n",
        "test_images = test_images/255"
      ],
      "metadata": {
        "id": "LAzGxvve8pgp"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dummy_labels(labels):\n",
        "  zero_labels = np.zeros([labels.shape[0], 10], np.int8)  \n",
        "  for i in range(labels.shape[0]):\n",
        "    zero_labels[i][labels[i]] = 1\n",
        "  return(zero_labels)"
      ],
      "metadata": {
        "id": "zqUm92WTGW3p"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = dummy_labels(train_labels)\n",
        "test_labels = dummy_labels(test_labels)"
      ],
      "metadata": {
        "id": "vFVXfHQNGtmq"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1000 labeled, 49000 unlabeled\n",
        "indx = random.sample(range(train_labels.shape[0]),train_labels.shape[0])\n",
        "\n",
        "lbl_train_images = train_images[indx[:1000]]\n",
        "ubl_train_images = train_images[indx[1000:]]\n",
        "\n",
        "lbl_train_labels = train_labels[indx[:1000]]\n",
        "ubl_train_labels = train_labels[indx[1000:]]"
      ],
      "metadata": {
        "id": "WOqDJtXCG2ov"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def PermaDropout(rate):\n",
        "    return Lambda(lambda x: K.dropout(x, level=rate))\n",
        "\n",
        "def create_cnn_13():\n",
        "  conv1a = Conv2D(128, 3, padding = 'same', activation='relu')\n",
        "  bn1a = BatchNormalization()\n",
        "  conv1b = Conv2D(128, 3, padding = 'same', activation='relu')\n",
        "  bn1b = BatchNormalization()\n",
        "  conv1c = Conv2D(128, 3, padding = 'same', activation='relu')\n",
        "  bn1c = BatchNormalization()\n",
        "  pl1 = MaxPooling2D(2, 2, padding = 'same')\n",
        "  MCdrop1 = PermaDropout(0.5)\n",
        "\n",
        "  conv2a = Conv2D(256, 3, padding = 'same', activation='relu')\n",
        "  bn2a = BatchNormalization()\n",
        "  conv2b = Conv2D(256, 3, padding = 'same', activation='relu')\n",
        "  bn2b = BatchNormalization()\n",
        "  conv2c = Conv2D(256, 3, padding = 'same', activation='relu')\n",
        "  bn2c = BatchNormalization()\n",
        "  pl2 = MaxPooling2D(2, 2, padding = 'same')\n",
        "  MCdrop2 = PermaDropout(0.5)\n",
        "\n",
        "  conv3a = Conv2D(512, 3, padding = 'same', activation='relu')\n",
        "  bn3a = BatchNormalization()\n",
        "  conv3b = Conv2D(256, 3, padding = 'same', activation='relu')\n",
        "  bn3b = BatchNormalization()\n",
        "  conv3c = Conv2D(128, 3, padding = 'same', activation='relu')\n",
        "  bn3c = BatchNormalization()\n",
        "  pl3 = AveragePooling2D(6, 2, padding = 'same')\n",
        "\n",
        "  fc = Dense(10)\n",
        "  activ = keras.layers.LeakyReLU(0.1)\n",
        "\n",
        "  model = Sequential([\n",
        "                      keras.Input(shape=(32, 32, 3)), \n",
        "                      conv1a, bn1a, activ,\n",
        "                      conv1b, bn1b, activ,\n",
        "                      conv1c, bn1c, activ,\n",
        "                      pl1, MCdrop1,\n",
        "\n",
        "                      conv2a, bn2a, activ,\n",
        "                      conv2b, bn2b, activ,\n",
        "                      conv2c, bn2c, activ,\n",
        "                      pl2, MCdrop2,\n",
        "\n",
        "                      conv3a, bn3a, activ,\n",
        "                      conv3b, bn3b, activ,\n",
        "                      conv3c, bn3c, activ,\n",
        "                      pl3, Flatten(),\n",
        "                      \n",
        "                      fc\n",
        "                      ])\n",
        "  \n",
        "  return model\n",
        "\n",
        "def compile_cnn_13(model):\n",
        "\n",
        "  adam = keras.optimizers.Adam(0.001)\n",
        "\n",
        "  model.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        "  )\n",
        "\n",
        "  return model\n",
        "\n",
        "def cnn_13():\n",
        "\n",
        "  model = create_cnn_13()\n",
        "  model = compile_cnn_13(model)\n",
        "\n",
        "  return model\n",
        "\n",
        "def fit_and_labeling_cnn_13(Epoch, Batch):\n",
        "  \n",
        "  lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
        "  early_stopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=30, mode='auto')\n",
        "\n",
        "  model.fit(\n",
        "      x=lbl_train_images,\n",
        "      y=lbl_train_labels,\n",
        "      epochs=Epoch,\n",
        "      verbose=1,\n",
        "      validation_split=0.2,\n",
        "      batch_size=Batch,\n",
        "      callbacks=[lr_reducer, early_stopper]\n",
        "  )\n",
        "\n",
        "  pred = model.predict(test_images)\n",
        "  acc = (np.argmax(pred,axis=1) == np.argmax(test_labels,axis=1))*1\n",
        "  print(\"test set 성능 : \" + str(sum(acc)/len(acc)))\n",
        "\n",
        "  for predsamples in tqdm(range(20)):\n",
        "    if predsamples == 0 :\n",
        "      predictions = model.predict(ubl_train_images)\n",
        "    else:\n",
        "      pred = model.predict(ubl_train_images)\n",
        "      predictions = np.stack((predictions, pred))\n",
        "\n",
        "  return predictions"
      ],
      "metadata": {
        "id": "YRCDNliwfudJ"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = cnn_13()\n",
        "fit_and_labeling_cnn_13(1000, 64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0BtpFFHkS1w",
        "outputId": "aca9d0ce-3022-467d-c10f-de5aa4fbc444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LFT9Nvr-tYt3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}