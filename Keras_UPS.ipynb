{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras_UPS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YianKim/2022_uncertainty_aware_semisupervise/blob/main/Keras_UPS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ik7Qx5iO8lQ_"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import PIL\n",
        "import pickle\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "from keras.layers.core import Lambda\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import MaxPooling2D, AveragePooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras import optimizers\n",
        "from keras.callbacks import *\n",
        "from sklearn.metrics import *\n",
        "from keras.models import load_model\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as backend\n",
        "import math\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# cifar10 dataset"
      ],
      "metadata": {
        "id": "2A3-ednCG7ZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cifar10 = keras.datasets.cifar10 \n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "train_images = train_images/255\n",
        "test_images = test_images/255"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAzGxvve8pgp",
        "outputId": "65ee75b7-8a9a-4fa3-c2c0-466af96d73d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "170508288/170498071 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dummy_labels(labels):\n",
        "  zero_labels = np.zeros([labels.shape[0], 10], np.int8)  \n",
        "  for i in range(labels.shape[0]):\n",
        "    zero_labels[i][labels[i]] = 1\n",
        "  return(zero_labels)"
      ],
      "metadata": {
        "id": "zqUm92WTGW3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = dummy_labels(train_labels)\n",
        "test_labels = dummy_labels(test_labels)"
      ],
      "metadata": {
        "id": "vFVXfHQNGtmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1000 labeled, 49000 unlabeled\n",
        "indx = random.sample(range(train_labels.shape[0]),train_labels.shape[0])\n",
        "\n",
        "lbl_train_images = train_images[indx[:1000]]\n",
        "ubl_train_images = train_images[indx[1000:]]\n",
        "\n",
        "lbl_train_labels = train_labels[indx[:1000]]\n",
        "ubl_train_labels = train_labels[indx[1000:]]"
      ],
      "metadata": {
        "id": "WOqDJtXCG2ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pseudo labeling"
      ],
      "metadata": {
        "id": "CqLLHcZLuZmP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 스케줄러"
      ],
      "metadata": {
        "id": "ewpemDXc8nEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SGDR(Callback):\n",
        "    \"\"\"This callback implements the learning rate schedule for\n",
        "    Stochastic Gradient Descent with warm Restarts (SGDR),\n",
        "    as proposed by Loshchilov & Hutter (https://arxiv.org/abs/1608.03983).\n",
        "    \n",
        "    The learning rate at each epoch is computed as:\n",
        "    lr(i) = min_lr + 0.5 * (max_lr - min_lr) * (1 + cos(pi * i/num_epochs))\n",
        "    \n",
        "    Here, num_epochs is the number of epochs in the current cycle, which starts\n",
        "    with base_epochs initially and is multiplied by mul_epochs after each cycle.\n",
        "    \n",
        "    # Example\n",
        "        ```python\n",
        "            sgdr = CyclicLR(min_lr=0.0, max_lr=0.05,\n",
        "                                base_epochs=10, mul_epochs=2)\n",
        "            model.compile(optimizer=keras.optimizers.SGD(decay=1e-4, momentum=0.9),\n",
        "                          loss=loss)\n",
        "            model.fit(X_train, Y_train, callbacks=[sgdr])\n",
        "        ```\n",
        "    \n",
        "    # Arguments\n",
        "        min_lr: minimum learning rate reached at the end of each cycle.\n",
        "        max_lr: maximum learning rate used at the beginning of each cycle.\n",
        "        base_epochs: number of epochs in the first cycle.\n",
        "        mul_epochs: factor with which the number of epochs is multiplied\n",
        "                after each cycle.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, min_lr=0.0, max_lr=0.03, base_epochs=20, mul_epochs=2):\n",
        "        super(SGDR, self).__init__()\n",
        "\n",
        "        self.min_lr = min_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.base_epochs = base_epochs\n",
        "        self.mul_epochs = mul_epochs\n",
        "\n",
        "        self.cycles = 0.\n",
        "        self.cycle_iterations = 0.\n",
        "        self.trn_iterations = 0.\n",
        "\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self, new_min_lr=None, new_max_lr=None,\n",
        "               new_base_epochs=None, new_mul_epochs=None):\n",
        "        \"\"\"Resets cycle iterations.\"\"\"\n",
        "        \n",
        "        if new_min_lr != None:\n",
        "            self.min_lr = new_min_lr\n",
        "        if new_max_lr != None:\n",
        "            self.max_lr = new_max_lr\n",
        "        if new_base_epochs != None:\n",
        "            self.base_epochs = new_base_epochs\n",
        "        if new_mul_epochs != None:\n",
        "            self.mul_epochs = new_mul_epochs\n",
        "        self.cycles = 0.\n",
        "        self.cycle_iterations = 0.\n",
        "        \n",
        "    def sgdr(self):\n",
        "        \n",
        "        cycle_epochs = self.base_epochs * (self.mul_epochs ** self.cycles)\n",
        "        tide = ((self.cycles == 0) * 1) * (self.cycle_iterations*self.max_lr + (self.base_epochs - self.cycle_iterations)*self.min_lr) / self.base_epochs + ((self.cycles != 0) * 1)*(self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(np.pi * (self.cycle_iterations + 1) / cycle_epochs)))\n",
        "        return tide\n",
        "        \n",
        "    def on_train_begin(self, logs=None):\n",
        "        \n",
        "        if self.cycle_iterations == 0:\n",
        "            K.set_value(self.model.optimizer.lr, self.max_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.sgdr())\n",
        "            \n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        \n",
        "        logs = logs or {}\n",
        "        logs['lr'] = K.get_value(self.model.optimizer.lr)\n",
        "        \n",
        "        self.trn_iterations += 1\n",
        "        self.cycle_iterations += 1\n",
        "        if self.cycle_iterations >= self.base_epochs * (self.mul_epochs ** self.cycles):\n",
        "            self.cycles += 1\n",
        "            self.cycle_iterations = 0\n",
        "            K.set_value(self.model.optimizer.lr, self.max_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.sgdr())"
      ],
      "metadata": {
        "id": "iKyn6Njs7vqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### main"
      ],
      "metadata": {
        "id": "DjpoH_dl8qPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def PermaDropout(rate):\n",
        "    return Lambda(lambda x: K.dropout(x, level=rate))\n",
        "\n",
        "def create_cnn_13():\n",
        "  conv1a = Conv2D(128, (3,3), padding = 'same', activation='relu')\n",
        "  bn1a = BatchNormalization()\n",
        "  conv1b = Conv2D(128, (3,3), padding = 'same', activation='relu')\n",
        "  bn1b = BatchNormalization()\n",
        "  conv1c = Conv2D(128, (3,3), padding = 'same', activation='relu')\n",
        "  bn1c = BatchNormalization()\n",
        "  pl1 = MaxPooling2D(2, 2)\n",
        "  MCdrop1 = Dropout(0.3)\n",
        "\n",
        "  conv2a = Conv2D(256, (3,3), padding = 'same', activation='relu')\n",
        "  bn2a = BatchNormalization()\n",
        "  conv2b = Conv2D(256, (3,3), padding = 'same', activation='relu')\n",
        "  bn2b = BatchNormalization()\n",
        "  conv2c = Conv2D(256, (3,3), padding = 'same', activation='relu')\n",
        "  bn2c = BatchNormalization()\n",
        "  pl2 = MaxPooling2D(2, 2)\n",
        "  MCdrop2 = Dropout(0.3)\n",
        "\n",
        "  conv3a = Conv2D(512, (3,3), activation='relu')\n",
        "  bn3a = BatchNormalization()\n",
        "  conv3b = Conv2D(256, (1,1), activation='relu')\n",
        "  bn3b = BatchNormalization()\n",
        "  conv3c = Conv2D(128, (1,1), activation='relu')\n",
        "  bn3c = BatchNormalization()\n",
        "  pl3 = AveragePooling2D(6, 2)\n",
        "\n",
        "  fc = Dense(10, activation='softmax')\n",
        "  activ = keras.layers.LeakyReLU(0.1)\n",
        "\n",
        "  model = Sequential([\n",
        "                      keras.Input(shape=(32, 32, 3)), \n",
        "                      conv1a, bn1a, activ,\n",
        "                      conv1b, bn1b, activ,\n",
        "                      conv1c, bn1c, activ,\n",
        "                      pl1, MCdrop1,\n",
        "\n",
        "                      conv2a, bn2a, activ,\n",
        "                      conv2b, bn2b, activ,\n",
        "                      conv2c, bn2c, activ,\n",
        "                      pl2, MCdrop2,\n",
        "\n",
        "                      conv3a, bn3a, activ,\n",
        "                      conv3b, bn3b, activ,\n",
        "                      conv3c, bn3c, activ,\n",
        "                      pl3, Flatten(),\n",
        "                      \n",
        "                      fc\n",
        "                      ])\n",
        "  \n",
        "  return model\n",
        "\n",
        "def compile_cnn_13(model):\n",
        "\n",
        "  opt = keras.optimizers.SGD(0.03, momentum=0.9)\n",
        "\n",
        "  model.compile(\n",
        "    optimizer = opt,\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        "  )\n",
        "\n",
        "  return model\n",
        "\n",
        "def cnn_13():\n",
        "\n",
        "  model = create_cnn_13()\n",
        "  model = compile_cnn_13(model)\n",
        "\n",
        "  return model\n",
        "\n",
        "def fit_and_labeling_cnn_13(Epoch, Batch):\n",
        "  \n",
        "  lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
        "  early_stopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=30, mode='auto')\n",
        "  sgdr = SGDR(min_lr=0.0, max_lr=0.03, base_epochs=20) #스케줄러\n",
        "\n",
        "  model.fit(\n",
        "      x=lbl_train_images,\n",
        "      y=lbl_train_labels,\n",
        "      epochs=Epoch,\n",
        "      verbose=1,\n",
        "      validation_split=0.2,\n",
        "      batch_size=Batch,\n",
        "      callbacks=[sgdr, early_stopper]\n",
        "  )\n",
        "\n",
        "  pred = model.predict(test_images)\n",
        "  acc = (np.argmax(pred,axis=1) == np.argmax(test_labels,axis=1))*1\n",
        "  print(\"test set 성능 : \" + str(sum(acc)/len(acc)))\n",
        "\n",
        "  for predsamples in tqdm(range(20)):\n",
        "    if predsamples == 0 :\n",
        "      predictions = model.predict(ubl_train_images)\n",
        "      predictions = predictions.reshape((1,) + predictions.shape)\n",
        "    else:\n",
        "      pred = model.predict(ubl_train_images)\n",
        "      pred = pred.reshape((1,) + pred.shape)\n",
        "      predictions = np.concatenate((predictions, pred))\n",
        "\n",
        "  return predictions"
      ],
      "metadata": {
        "id": "YRCDNliwfudJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = cnn_13()\n",
        "fit_and_labeling_cnn_13(3000, 64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0BtpFFHkS1w",
        "outputId": "4e19cccd-a317-47e7-a66c-747734735a51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3000\n",
            " 6/13 [============>.................] - ETA: 1s - loss: 2.2918 - accuracy: 0.1328WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0534s vs `on_train_batch_end` time: 0.1082s). Check your callbacks.\n",
            "13/13 [==============================] - 4s 196ms/step - loss: 2.2200 - accuracy: 0.1762 - val_loss: 2.3819 - val_accuracy: 0.0950 - lr: 0.0300\n",
            "Epoch 2/3000\n",
            "13/13 [==============================] - 2s 169ms/step - loss: 2.0957 - accuracy: 0.2225 - val_loss: 11.5764 - val_accuracy: 0.0950 - lr: 0.0015\n",
            "Epoch 3/3000\n",
            "13/13 [==============================] - 2s 169ms/step - loss: 2.0508 - accuracy: 0.2637 - val_loss: 8.6930 - val_accuracy: 0.0950 - lr: 0.0030\n",
            "Epoch 4/3000\n",
            "13/13 [==============================] - 2s 169ms/step - loss: 1.8331 - accuracy: 0.3388 - val_loss: 4.1396 - val_accuracy: 0.1050 - lr: 0.0045\n",
            "Epoch 5/3000\n",
            "13/13 [==============================] - 2s 169ms/step - loss: 1.7081 - accuracy: 0.3875 - val_loss: 2.5443 - val_accuracy: 0.1500 - lr: 0.0060\n",
            "Epoch 6/3000\n",
            "13/13 [==============================] - 2s 169ms/step - loss: 1.6230 - accuracy: 0.4288 - val_loss: 2.2249 - val_accuracy: 0.1500 - lr: 0.0075\n",
            "Epoch 7/3000\n",
            "13/13 [==============================] - 2s 169ms/step - loss: 1.5131 - accuracy: 0.4600 - val_loss: 2.2508 - val_accuracy: 0.1450 - lr: 0.0090\n",
            "Epoch 8/3000\n",
            "13/13 [==============================] - 2s 168ms/step - loss: 1.4423 - accuracy: 0.4875 - val_loss: 2.2741 - val_accuracy: 0.1300 - lr: 0.0105\n",
            "Epoch 9/3000\n",
            "13/13 [==============================] - 2s 169ms/step - loss: 1.3505 - accuracy: 0.5088 - val_loss: 2.3118 - val_accuracy: 0.1250 - lr: 0.0120\n",
            "Epoch 10/3000\n",
            "13/13 [==============================] - 2s 169ms/step - loss: 1.2722 - accuracy: 0.5612 - val_loss: 2.3129 - val_accuracy: 0.1550 - lr: 0.0135\n",
            "Epoch 11/3000\n",
            "13/13 [==============================] - 2s 168ms/step - loss: 1.2191 - accuracy: 0.5813 - val_loss: 2.3292 - val_accuracy: 0.1150 - lr: 0.0150\n",
            "Epoch 12/3000\n",
            "13/13 [==============================] - 2s 169ms/step - loss: 1.0848 - accuracy: 0.6325 - val_loss: 2.3681 - val_accuracy: 0.1150 - lr: 0.0165\n",
            "Epoch 13/3000\n",
            "13/13 [==============================] - 2s 169ms/step - loss: 1.0057 - accuracy: 0.6837 - val_loss: 2.4110 - val_accuracy: 0.1150 - lr: 0.0180\n",
            "Epoch 14/3000\n",
            "13/13 [==============================] - 2s 169ms/step - loss: 0.9579 - accuracy: 0.6837 - val_loss: 2.4562 - val_accuracy: 0.1300 - lr: 0.0195\n",
            "Epoch 15/3000\n",
            "13/13 [==============================] - 2s 168ms/step - loss: 0.7944 - accuracy: 0.7650 - val_loss: 2.4619 - val_accuracy: 0.1800 - lr: 0.0210\n",
            "Epoch 16/3000\n",
            "13/13 [==============================] - 2s 171ms/step - loss: 0.7349 - accuracy: 0.7625 - val_loss: 2.5652 - val_accuracy: 0.1450 - lr: 0.0225\n",
            "Epoch 17/3000\n",
            "13/13 [==============================] - 2s 170ms/step - loss: 0.7437 - accuracy: 0.7688 - val_loss: 2.6519 - val_accuracy: 0.1400 - lr: 0.0240\n",
            "Epoch 18/3000\n",
            "13/13 [==============================] - 2s 169ms/step - loss: 0.6930 - accuracy: 0.7738 - val_loss: 2.5675 - val_accuracy: 0.1950 - lr: 0.0255\n",
            "Epoch 19/3000\n",
            "13/13 [==============================] - 2s 169ms/step - loss: 0.6320 - accuracy: 0.8075 - val_loss: 2.8876 - val_accuracy: 0.1750 - lr: 0.0270\n",
            "Epoch 20/3000\n",
            "13/13 [==============================] - 2s 169ms/step - loss: 0.5332 - accuracy: 0.8400 - val_loss: 2.6392 - val_accuracy: 0.2100 - lr: 0.0285\n",
            "Epoch 21/3000\n",
            "13/13 [==============================] - 2s 168ms/step - loss: 0.4940 - accuracy: 0.8600 - val_loss: 2.9380 - val_accuracy: 0.2150 - lr: 0.0300\n",
            "Epoch 22/3000\n",
            "13/13 [==============================] - 2s 169ms/step - loss: 0.4876 - accuracy: 0.8462 - val_loss: 2.6467 - val_accuracy: 0.2150 - lr: 0.0298\n",
            "Epoch 23/3000\n",
            "13/13 [==============================] - 2s 170ms/step - loss: 0.4090 - accuracy: 0.8712 - val_loss: 2.9301 - val_accuracy: 0.2250 - lr: 0.0296\n",
            "Epoch 24/3000\n",
            "13/13 [==============================] - 2s 168ms/step - loss: 0.2986 - accuracy: 0.9137 - val_loss: 3.1225 - val_accuracy: 0.2500 - lr: 0.0293\n",
            "Epoch 25/3000\n",
            "13/13 [==============================] - 2s 168ms/step - loss: 0.2964 - accuracy: 0.9125 - val_loss: 2.6226 - val_accuracy: 0.3550 - lr: 0.0289\n",
            "Epoch 26/3000\n",
            "13/13 [==============================] - 2s 170ms/step - loss: 0.2359 - accuracy: 0.9287 - val_loss: 2.4737 - val_accuracy: 0.2950 - lr: 0.0284\n",
            "Epoch 27/3000\n",
            "13/13 [==============================] - 2s 168ms/step - loss: 0.1441 - accuracy: 0.9688 - val_loss: 3.5061 - val_accuracy: 0.2350 - lr: 0.0278\n",
            "Epoch 28/3000\n",
            "13/13 [==============================] - 2s 179ms/step - loss: 0.1144 - accuracy: 0.9812 - val_loss: 3.2441 - val_accuracy: 0.2750 - lr: 0.0271\n",
            "Epoch 29/3000\n",
            "13/13 [==============================] - 2s 171ms/step - loss: 0.0911 - accuracy: 0.9825 - val_loss: 2.8039 - val_accuracy: 0.3050 - lr: 0.0264\n",
            "Epoch 30/3000\n",
            "13/13 [==============================] - 2s 170ms/step - loss: 0.0690 - accuracy: 0.9862 - val_loss: 2.6422 - val_accuracy: 0.3100 - lr: 0.0256\n",
            "Epoch 31/3000\n",
            "13/13 [==============================] - 2s 168ms/step - loss: 0.0452 - accuracy: 0.9937 - val_loss: 3.2263 - val_accuracy: 0.2700 - lr: 0.0247\n",
            "Epoch 32/3000\n",
            "13/13 [==============================] - 2s 169ms/step - loss: 0.0416 - accuracy: 0.9950 - val_loss: 3.1069 - val_accuracy: 0.2850 - lr: 0.0238\n",
            "Epoch 33/3000\n",
            "13/13 [==============================] - 2s 169ms/step - loss: 0.0197 - accuracy: 1.0000 - val_loss: 2.7865 - val_accuracy: 0.3150 - lr: 0.0228\n",
            "Epoch 34/3000\n",
            "13/13 [==============================] - 2s 169ms/step - loss: 0.0195 - accuracy: 1.0000 - val_loss: 2.6192 - val_accuracy: 0.3200 - lr: 0.0218\n",
            "Epoch 35/3000\n",
            "13/13 [==============================] - 2s 169ms/step - loss: 0.0173 - accuracy: 0.9975 - val_loss: 2.5051 - val_accuracy: 0.3450 - lr: 0.0207\n",
            "Epoch 36/3000\n",
            "13/13 [==============================] - 2s 170ms/step - loss: 0.0359 - accuracy: 0.9937 - val_loss: 2.5843 - val_accuracy: 0.3200 - lr: 0.0196\n",
            "test set 성능 : 0.3709\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 10/20 [06:14<06:36, 39.61s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Too-Aq98zjzW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}