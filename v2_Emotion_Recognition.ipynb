{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YianKim/2022_PythonStudy_face_emotion_recognition/blob/main/v2_Emotion_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FQpjWg4Ld6m"
      },
      "source": [
        "## 구글 드라이브\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyTEuhg7fR9P",
        "outputId": "5f4f02c9-5741-450b-8b0d-41a9ae381031"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQy7KUb6NBs-"
      },
      "source": [
        "## 라이브러리 불러오기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XRPQRtXGmm2W"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import PIL\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "from keras.layers.core import Lambda\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import LSTM\n",
        "from keras import optimizers\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard, ModelCheckpoint\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.models import load_model\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpAjYytuPDaU"
      },
      "source": [
        "## 데이터 불러오기\n",
        "\n",
        "FER2013.csv : 35887 images\n",
        "\n",
        "![Dataset overview](https://i.imgur.com/vqlEXw2.png) \n",
        "\n",
        "48*48 이미지. value 0~255\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "H2XHQzzzrRyF"
      },
      "outputs": [],
      "source": [
        "def load_data(dataset_path):\n",
        "  \n",
        "  #classes = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprsie', 'Neutral']\n",
        "\n",
        "  data = []\n",
        "  test_data = []\n",
        "  test_labels = []\n",
        "  labels =[]\n",
        "\n",
        "  with open(dataset_path, 'r') as file:\n",
        "      for line_no, line in enumerate(file.readlines()):\n",
        "          if 0 < line_no <= 35887:\n",
        "            curr_class, line, set_type = line.split(',')\n",
        "            image_data = np.asarray([int(x) for x in line.split()]).reshape(48, 48)\n",
        "            image_data =image_data.astype(np.uint8)/255.0\n",
        "\n",
        "            if (set_type.strip() == 'PrivateTest'):\n",
        "              \n",
        "              test_data.append(image_data)\n",
        "              test_labels.append(curr_class)\n",
        "            else:\n",
        "              data.append(image_data)\n",
        "              labels.append(curr_class)\n",
        "      \n",
        "      test_data = np.expand_dims(test_data, -1)\n",
        "      test_labels = to_categorical(test_labels, num_classes = 7)\n",
        "      data = np.expand_dims(data, -1)   \n",
        "      labels = to_categorical(labels, num_classes = 7)\n",
        "    \n",
        "      return np.array(data), np.array(labels), np.array(test_data), np.array(test_labels)\n",
        "\n",
        "dataset_path = \"/content/drive/MyDrive/파이썬스터디 프로젝트/fer2013.csv/fer2013.csv\"\n",
        "train_data, train_labels, test_data, test_labels = load_data(dataset_path)\n",
        "#train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size = test_size,random_state = seed)\n",
        "\n",
        "#평가용\n",
        "test_labels2 = np.argmax(test_labels, axis=1)\n",
        "\n",
        "# print(\"Number of images in Training set:\", len(train_data))\n",
        "# print(\"Number of images in Test set:\", len(test_data))\n",
        "\n",
        "with open('/content/drive/MyDrive/파이썬스터디 프로젝트/aug_array1.pkl', 'rb') as f:\n",
        "\taug_array1 = pickle.load(f)\n",
        " \n",
        "train_data_aug = np.concatenate((train_data, aug_array1), axis=0)\n",
        "train_labels_aug = np.concatenate((train_labels, train_labels), axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN v2.\n",
        "\n",
        "A. Vulpe-Grigoraşi and O. Grigore, \"Convolutional Neural Network Hyperparameters optimization for Facial Emotion Recognition,\" 2021 12th International Symposium on Advanced Topics in Electrical Engineering (ATEE), 2021, pp. 1-5, doi: 10.1109/ATEE52255.2021.9425073."
      ],
      "metadata": {
        "id": "Sa7KYMfrkKU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def PermaDropout(rate):\n",
        "#     return Lambda(lambda x: K.dropout(x, level=rate))"
      ],
      "metadata": {
        "id": "zn3v_W8wJQ68"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #######HYPERPARAMATERS###########\n",
        "# epochs = 30\n",
        "# batch_size = 64\n",
        "# learning_rate = 0.001\n",
        "# #################################\n",
        "  \n",
        "# model = Sequential()\n",
        "    \n",
        "# model.add(Conv2D(256, (3, 3), activation='relu', input_shape=(48, 48, 1), kernel_regularizer=l2(0.01)))\n",
        "# model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "# model.add(PermaDropout(0.4))\n",
        "\n",
        "# model.add(Conv2D(384, (3, 3), padding='same', activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "# model.add(PermaDropout(0.4))\n",
        "\n",
        "# model.add(Conv2D(192, (3, 3), padding='same', activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "# model.add(PermaDropout(0.4))\n",
        "\n",
        "# model.add(Conv2D(384, (3, 3), padding='same', activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "# model.add(PermaDropout(0.4))\n",
        "\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(256, activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(PermaDropout(0.3))\n",
        "# model.add(Dense(7, activation='softmax'))\n",
        "\n",
        "# adam = keras.optimizers.Adam(learning_rate)\n",
        "# model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    \n",
        "# # print(model.summary())\n",
        "\n",
        "# lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
        "# early_stopper = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=30, mode='auto')\n",
        "# checkpointer = ModelCheckpoint('weights.hd5', monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "# model.fit(\n",
        "#           train_data_aug,\n",
        "#           train_labels_aug,\n",
        "#           epochs = epochs,\n",
        "#           batch_size = batch_size,\n",
        "#           validation_split = 0.3,\n",
        "#           shuffle = True,\n",
        "# #           callbacks=[lr_reducer, checkpointer, early_stopper]\n",
        "#           callbacks=[lr_reducer, early_stopper]\n",
        "#           )"
      ],
      "metadata": {
        "id": "L5scE9yKkIZb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.save('/content/drive/MyDrive/파이썬스터디 프로젝트/cnn_base_model_v2.h5')"
      ],
      "metadata": {
        "id": "LVOc3hYIkXrl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = keras.models.load_model('/content/drive/MyDrive/파이썬스터디 프로젝트/cnn_base_model_v2.h5')"
      ],
      "metadata": {
        "id": "n5D8i_BCkdjS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TD8ppILhFUrz"
      },
      "source": [
        "## 모델 결과\n",
        "\n",
        "CNN\n",
        "1. no augmentation : 62~63%\n",
        "2. 2x augmentation : 67~68%\n",
        "3. 4x augmentation : 67~68%\n",
        "\n",
        "CNN v2 with MC Dropout, 30 times\n",
        "1. 2x augmentation : 67~68%"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def model_eval():\n",
        "  pred_mu = model.predict(test_data)\n",
        "  for i in range(1, 30):\n",
        "    pred_mu += model.predict(test_data)\n",
        "  pred_mu = pred_mu/30\n",
        "  predicted_test_labels = np.argmax(pred_mu, axis=1)\n",
        "  return(accuracy_score(test_labels2, predicted_test_labels))"
      ],
      "metadata": {
        "id": "7Ljx2EWA2v2n"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OzhQw9NOWjl3"
      },
      "outputs": [],
      "source": [
        "# model = keras.models.load_model('/content/drive/MyDrive/파이썬스터디 프로젝트/cnn_base_model_v2.h5')\n",
        "# model_eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9MfwvYALW8x"
      },
      "source": [
        "## 준지도 pseudo labeling (self train)\n",
        "\n",
        "모델성능이 70% 미만 -> 틀렸을 가능성이 높은 데이터들은 사용 X (최대한 70% 안에 있는 것을 맞춰서 사용)\n",
        "\n",
        "수 회 예측결과의 분산이 작은 이미지일수록 기존 데이터셋(labeled)과 비슷하고, 분산이 클 수록 새로운 이미지일까?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dataselection 함수\n",
        "\n",
        "1. model과 data_no_label을 받고\n",
        "\n",
        "2. uncertainty에 따라 그룹을 나눠주는 함수"
      ],
      "metadata": {
        "id": "-FaArGPM5PQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 진짜 그럴까?"
      ],
      "metadata": {
        "id": "h3T6PgIiYp8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model = keras.models.load_model('/content/drive/MyDrive/파이썬스터디 프로젝트/cnn_base_model_v2.h5')\n",
        "# data_no_label = test_data"
      ],
      "metadata": {
        "id": "xf_67cJyWg8p"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pred1 = model.predict(data_no_label)\n",
        "# pred2 = model.predict(data_no_label)\n",
        "# pred3 = model.predict(data_no_label)\n",
        "# pred4 = model.predict(data_no_label)\n",
        "# pred5 = model.predict(data_no_label)\n",
        "# pred6 = model.predict(data_no_label)\n",
        "# pred7 = model.predict(data_no_label)\n",
        "# pred8 = model.predict(data_no_label)\n",
        "# pred9 = model.predict(data_no_label)\n",
        "# pred10 = model.predict(data_no_label)\n",
        "# pred_mu = (pred1+pred2+pred3+pred4+pred5+pred6+pred7+pred8+pred9+pred10)/10\n",
        "# pred_var=[]\n",
        "\n",
        "# for i in range(data_no_label.shape[0]):\n",
        "#   temp = np.max(list(pred_mu[i]))\n",
        "#   temp = list(pred_mu[i]).index(temp)\n",
        "#   pred_var.append(np.var([pred1[i][temp],pred2[i][temp],pred3[i][temp],pred4[i][temp],pred5[i][temp],\n",
        "#                           pred6[i][temp],pred7[i][temp],pred8[i][temp],pred9[i][temp],pred10[i][temp]]))\n",
        "  \n",
        "# group_a = []\n",
        "# group_a_label = []\n",
        "# group_b = []\n",
        "# group_b_label = []\n",
        "\n",
        "# var_25 = np.percentile(pred_var,25)\n",
        "# for i in range(len(pred_var)):\n",
        "#   if pred_var[i] < var_25:\n",
        "#     group_a.append(i)\n",
        "#     group_a_label.append(list(pred_mu[i]).index(np.max(list(pred_mu[i]))))\n",
        "#   else:\n",
        "#     group_b.append(i)\n",
        "#     group_b_label.append(list(pred_mu[i]).index(np.max(list(pred_mu[i]))))\n",
        "\n",
        "# a_train = data_no_label[group_a]\n",
        "# b_train = data_no_label[group_b]\n",
        "# a_label = []\n",
        "# b_label = []\n",
        "\n",
        "# for i in group_a_label:\n",
        "#   temp = [0,0,0,0,0,0,0]\n",
        "#   temp[i] = 1\n",
        "#   a_label.append(temp)\n",
        "# for i in group_b_label:\n",
        "#   temp = [0,0,0,0,0,0,0]\n",
        "#   temp[i] = 1\n",
        "#   b_label.append(temp)\n",
        "\n",
        "# a_label = np.array(a_label)\n",
        "# b_label = np.array(b_label)"
      ],
      "metadata": {
        "id": "qgQOKAm3XD0A"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# var_25 #약 0.014"
      ],
      "metadata": {
        "id": "_7HeeWihYYGQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "쉬운 샘플은 잘 구분하고(9n%) 어려운 샘플은 못 구분한다(5n%)"
      ],
      "metadata": {
        "id": "3Z3WOeEsYOJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 확실함 = accuracy_score(test_labels2[group_a], group_a_label)\n",
        "# 불확실함 = accuracy_score(test_labels2[group_b], group_b_label)\n",
        "# print((확실함,불확실함)) # 유의한 차이가 있다."
      ],
      "metadata": {
        "id": "tyvtWqGgXmLM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0"
      ],
      "metadata": {
        "id": "8s2ElWjiYof3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dataselection(cutoff):\n",
        "\n",
        "  # predict\n",
        "  pred1 = model.predict(data_no_label)\n",
        "  pred2 = model.predict(data_no_label)\n",
        "  pred3 = model.predict(data_no_label)\n",
        "  pred4 = model.predict(data_no_label)\n",
        "  pred5 = model.predict(data_no_label)\n",
        "  pred6 = model.predict(data_no_label)\n",
        "  pred7 = model.predict(data_no_label)\n",
        "  pred8 = model.predict(data_no_label)\n",
        "  pred9 = model.predict(data_no_label)\n",
        "  pred10 = model.predict(data_no_label)\n",
        "  pred_mu = (pred1+pred2+pred3+pred4+pred5+pred6+pred7+pred8+pred9+pred10)/10\n",
        "  pred_var=[]\n",
        "\n",
        "  for i in range(data_no_label.shape[0]):\n",
        "    temp = np.max(list(pred_mu[i]))\n",
        "    temp = list(pred_mu[i]).index(temp)\n",
        "    pred_var.append(np.var([pred1[i][temp],pred2[i][temp],pred3[i][temp],pred4[i][temp],pred5[i][temp],\n",
        "                            pred6[i][temp],pred7[i][temp],pred8[i][temp],pred9[i][temp],pred10[i][temp]]))\n",
        "  \n",
        "  group_a = []\n",
        "  group_a_label = []\n",
        "  group_b = []\n",
        "  group_b_label = []\n",
        "\n",
        "  var_25 = np.percentile(pred_var,cutoff)\n",
        "  for i in range(len(pred_var)):\n",
        "    if pred_var[i] < var_25:\n",
        "      group_a.append(i)\n",
        "      group_a_label.append(list(pred_mu[i]).index(np.max(list(pred_mu[i]))))\n",
        "    else:\n",
        "      group_b.append(i)\n",
        "      group_b_label.append(list(pred_mu[i]).index(np.max(list(pred_mu[i]))))\n",
        "\n",
        "  a_train = data_no_label[group_a]\n",
        "  b_train = data_no_label[group_b]\n",
        "  a_label = []\n",
        "  b_label = []\n",
        "\n",
        "  for i in group_a_label:\n",
        "    temp = [0,0,0,0,0,0,0]\n",
        "    temp[i] = 1\n",
        "    a_label.append(temp)\n",
        "  for i in group_b_label:\n",
        "    temp = [0,0,0,0,0,0,0]\n",
        "    temp[i] = 1\n",
        "    b_label.append(temp)\n",
        "\n",
        "  a_label = np.array(a_label)\n",
        "  b_label = np.array(b_label)\n",
        "\n",
        "  return(a_train, a_label, b_train, b_label)"
      ],
      "metadata": {
        "id": "7t0bwmvA3Akj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1\n"
      ],
      "metadata": {
        "id": "fX2A4ATHM8mS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('/content/drive/MyDrive/파이썬스터디 프로젝트/face_array.pkl', 'rb') as f:\n",
        "# \tdata_no_label = pickle.load(f)\n",
        "# model = keras.models.load_model('/content/drive/MyDrive/파이썬스터디 프로젝트/cnn_base_model_v2.h5')"
      ],
      "metadata": {
        "id": "H-4aNkdbwfAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a_train, a_label, b_train, b_label = dataselection(25)\n",
        "\n",
        "# with open('/content/drive/MyDrive/파이썬스터디 프로젝트/a_train2.pkl', 'wb') as f:\n",
        "#   pickle.dump(a_train, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# with open('/content/drive/MyDrive/파이썬스터디 프로젝트/a_label2.pkl', 'wb') as f:\n",
        "#   pickle.dump(a_label, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# with open('/content/drive/MyDrive/파이썬스터디 프로젝트/b_train2.pkl', 'wb') as f:\n",
        "#   pickle.dump(b_train, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# with open('/content/drive/MyDrive/파이썬스터디 프로젝트/b_label2.pkl', 'wb') as f:\n",
        "#   pickle.dump(b_label, f, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "eBed2AL2iXda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/파이썬스터디 프로젝트/a_train2.pkl', 'rb') as f:\n",
        "\ta_train = pickle.load(f)\n",
        " \n",
        "with open('/content/drive/MyDrive/파이썬스터디 프로젝트/a_label2.pkl', 'rb') as f:\n",
        "\ta_label = pickle.load(f)\n",
        " \n",
        "with open('/content/drive/MyDrive/파이썬스터디 프로젝트/b_train2.pkl', 'rb') as f:\n",
        "\tb_train = pickle.load(f)\n",
        " \n",
        "with open('/content/drive/MyDrive/파이썬스터디 프로젝트/b_label2.pkl', 'rb') as f:\n",
        "\tb_label = pickle.load(f)"
      ],
      "metadata": {
        "id": "T7kDb_kQiXuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a_train = np.concatenate((train_data_aug, a_train), 0)\n",
        "a_label = np.concatenate((train_labels_aug, a_label), 0)"
      ],
      "metadata": {
        "id": "DzgcJnQiQRLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = keras.models.load_model('/content/drive/MyDrive/파이썬스터디 프로젝트/cnn_base_model_v2.h5')\n",
        "\n",
        "# #######HYPERPARAMATERS###########\n",
        "# epochs = 10 #재학습\n",
        "# batch_size = 64\n",
        "# learning_rate = 1e-7 #재학습\n",
        "# #################################\n",
        "\n",
        "# lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
        "# early_stopper = EarlyStopping(monitor='val_acc', min_delta=0, patience=6, mode='auto')\n",
        "# checkpointer = ModelCheckpoint('/content/drive/MyDrive/파이썬스터디 프로젝트/Model/weights.hd5', monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "# model.fit(\n",
        "#           a_train,\n",
        "#           a_label,\n",
        "#           epochs = epochs,\n",
        "#           batch_size = batch_size,\n",
        "#           # validation_split = 0.5,\n",
        "#           shuffle = True,\n",
        "#           # callbacks=[lr_reducer, checkpointer, early_stopper],\n",
        "#           # class_weight=class_weight\n",
        "#           )"
      ],
      "metadata": {
        "id": "BhIJ-TbzfEgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.save('/content/drive/MyDrive/파이썬스터디 프로젝트/cnn_220113_model_v2.h5')"
      ],
      "metadata": {
        "id": "MEVm4GRpskds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "67~68%"
      ],
      "metadata": {
        "id": "_u4HzNR6c2XB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model = keras.models.load_model('/content/drive/MyDrive/파이썬스터디 프로젝트/cnn_220113_model_v2.h5')\n",
        "# model_eval()"
      ],
      "metadata": {
        "id": "qC5XR37jQ2BN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2"
      ],
      "metadata": {
        "id": "xkoGfxkjrTvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model = keras.models.load_model('/content/drive/MyDrive/파이썬스터디 프로젝트/cnn_220113_model_v2.h5')\n",
        "# data_no_label = b_train"
      ],
      "metadata": {
        "id": "D_u6jE0erRHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_aug = a_train\n",
        "train_labels_aug = a_label"
      ],
      "metadata": {
        "id": "8m5LP_V9haik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a_train, a_label, b_train, b_label = dataselection(33)\n",
        "\n",
        "# with open('/content/drive/MyDrive/파이썬스터디 프로젝트/a_train3.pkl', 'wb') as f:\n",
        "#   pickle.dump(a_train, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# with open('/content/drive/MyDrive/파이썬스터디 프로젝트/a_label3.pkl', 'wb') as f:\n",
        "#   pickle.dump(a_label, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# with open('/content/drive/MyDrive/파이썬스터디 프로젝트/b_train3.pkl', 'wb') as f:\n",
        "#   pickle.dump(b_train, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# with open('/content/drive/MyDrive/파이썬스터디 프로젝트/b_label3.pkl', 'wb') as f:\n",
        "#   pickle.dump(b_label, f, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "kC9ql5k8uj2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/파이썬스터디 프로젝트/a_train3.pkl', 'rb') as f:\n",
        "\ta_train = pickle.load(f)\n",
        " \n",
        "with open('/content/drive/MyDrive/파이썬스터디 프로젝트/a_label3.pkl', 'rb') as f:\n",
        "\ta_label = pickle.load(f)\n",
        " \n",
        "with open('/content/drive/MyDrive/파이썬스터디 프로젝트/b_train3.pkl', 'rb') as f:\n",
        "\tb_train = pickle.load(f)\n",
        " \n",
        "with open('/content/drive/MyDrive/파이썬스터디 프로젝트/b_label3.pkl', 'rb') as f:\n",
        "\tb_label = pickle.load(f)"
      ],
      "metadata": {
        "id": "FI6xnFH3u_fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a_train = np.concatenate((train_data_aug, a_train), 0)\n",
        "a_label = np.concatenate((train_labels_aug, a_label), 0)"
      ],
      "metadata": {
        "id": "6VC9-BZBTh1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = keras.models.load_model('/content/drive/MyDrive/파이썬스터디 프로젝트/cnn_220113_model_v2.h5')\n",
        "\n",
        "# #######HYPERPARAMATERS###########\n",
        "# epochs = 10 #재학습\n",
        "# batch_size = 64\n",
        "# learning_rate = 0.1**7 #재학습\n",
        "# #################################\n",
        "\n",
        "# lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
        "# early_stopper = EarlyStopping(monitor='val_acc', min_delta=0, patience=6, mode='auto')\n",
        "# checkpointer = ModelCheckpoint('/content/drive/MyDrive/파이썬스터디 프로젝트/Model/weights.hd5', monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "# model.fit(\n",
        "#           a_train,\n",
        "#           a_label,\n",
        "#           epochs = epochs,\n",
        "#           batch_size = batch_size,\n",
        "#           # validation_split = 0.5,\n",
        "#           shuffle = True,\n",
        "#           # callbacks=[lr_reducer, checkpointer, early_stopper],\n",
        "#           # class_weight=class_weight\n",
        "#           )"
      ],
      "metadata": {
        "id": "rjmSA0qqsenu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.save('/content/drive/MyDrive/파이썬스터디 프로젝트/cnn_220114_model_v2.h5')"
      ],
      "metadata": {
        "id": "-rcxXsD7spBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "68~69%"
      ],
      "metadata": {
        "id": "DpJS_a3PcxkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model = keras.models.load_model('/content/drive/MyDrive/파이썬스터디 프로젝트/cnn_220114_model_v2.h5')\n",
        "# model_eval()"
      ],
      "metadata": {
        "id": "Enp2PMywst2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3"
      ],
      "metadata": {
        "id": "1JHdwlOedG6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model = keras.models.load_model('/content/drive/MyDrive/파이썬스터디 프로젝트/cnn_220114_model_v2.h5')\n",
        "# data_no_label = b_train"
      ],
      "metadata": {
        "id": "WY10pUhmdG6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_aug = a_train\n",
        "train_labels_aug = a_label"
      ],
      "metadata": {
        "id": "3zZfdI2dhlHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a_train, a_label, b_train, b_label = dataselection(50)\n",
        "\n",
        "# with open('/content/drive/MyDrive/파이썬스터디 프로젝트/a_train4.pkl', 'wb') as f:\n",
        "#   pickle.dump(a_train, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# with open('/content/drive/MyDrive/파이썬스터디 프로젝트/a_label4.pkl', 'wb') as f:\n",
        "#   pickle.dump(a_label, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# with open('/content/drive/MyDrive/파이썬스터디 프로젝트/b_train4.pkl', 'wb') as f:\n",
        "#   pickle.dump(b_train, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# with open('/content/drive/MyDrive/파이썬스터디 프로젝트/b_label4.pkl', 'wb') as f:\n",
        "#   pickle.dump(b_label, f, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "ipoaFeUkdG6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/파이썬스터디 프로젝트/a_train4.pkl', 'rb') as f:\n",
        "\ta_train = pickle.load(f)\n",
        " \n",
        "with open('/content/drive/MyDrive/파이썬스터디 프로젝트/a_label4.pkl', 'rb') as f:\n",
        "\ta_label = pickle.load(f)\n",
        " \n",
        "with open('/content/drive/MyDrive/파이썬스터디 프로젝트/b_train4.pkl', 'rb') as f:\n",
        "\tb_train = pickle.load(f)\n",
        " \n",
        "with open('/content/drive/MyDrive/파이썬스터디 프로젝트/b_label4.pkl', 'rb') as f:\n",
        "\tb_label = pickle.load(f)"
      ],
      "metadata": {
        "id": "ZfWqmUV4dG6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a_train = np.concatenate((train_data_aug, a_train), 0)\n",
        "a_label = np.concatenate((train_labels_aug, a_label), 0)"
      ],
      "metadata": {
        "id": "i0RDPJYzdG6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = keras.models.load_model('/content/drive/MyDrive/파이썬스터디 프로젝트/cnn_220114_model_v2.h5')\n",
        "\n",
        "# #######HYPERPARAMATERS###########\n",
        "# epochs = 10 #재학습\n",
        "# batch_size = 64\n",
        "# learning_rate = 0.1**7 #재학습\n",
        "# #################################\n",
        "\n",
        "# lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
        "# early_stopper = EarlyStopping(monitor='val_acc', min_delta=0, patience=6, mode='auto')\n",
        "# checkpointer = ModelCheckpoint('/content/drive/MyDrive/파이썬스터디 프로젝트/Model/weights.hd5', monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "# model.fit(\n",
        "#           a_train,\n",
        "#           a_label,\n",
        "#           epochs = epochs,\n",
        "#           batch_size = batch_size,\n",
        "#           # validation_split = 0.5,\n",
        "#           shuffle = True,\n",
        "#           # callbacks=[lr_reducer, checkpointer, early_stopper],\n",
        "#           # class_weight=class_weight\n",
        "#           )"
      ],
      "metadata": {
        "id": "r9dHEuobdG6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.save('/content/drive/MyDrive/파이썬스터디 프로젝트/cnn_220115_model_v2.h5')"
      ],
      "metadata": {
        "id": "VxSqUxNhdG6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "67~68%"
      ],
      "metadata": {
        "id": "RzLvpeJ6dG6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model = keras.models.load_model('/content/drive/MyDrive/파이썬스터디 프로젝트/cnn_220115_model_v2.h5')\n",
        "# model_eval()"
      ],
      "metadata": {
        "id": "DstEj5fjdG6r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba762caa-b331-4699-cf8a-ff78759db31b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.679019225410978"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ogAdnFlNlXuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 앙상블로 접근"
      ],
      "metadata": {
        "id": "-9AQQM4PlX9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/파이썬스터디 프로젝트/a_train3.pkl', 'rb') as f:\n",
        "\ta_train = pickle.load(f)\n",
        " \n",
        "with open('/content/drive/MyDrive/파이썬스터디 프로젝트/a_label3.pkl', 'rb') as f:\n",
        "\ta_label = pickle.load(f)\n",
        " \n",
        "with open('/content/drive/MyDrive/파이썬스터디 프로젝트/b_train3.pkl', 'rb') as f:\n",
        "\tb_train = pickle.load(f)\n",
        " \n",
        "with open('/content/drive/MyDrive/파이썬스터디 프로젝트/b_label3.pkl', 'rb') as f:\n",
        "\tb_label = pickle.load(f)"
      ],
      "metadata": {
        "id": "1JXSaqg_laG_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indx = random.sample(range(train_data_aug.shape[0]), np.int(0.75*train_data_aug.shape[0]))"
      ],
      "metadata": {
        "id": "dUkUJfT7l0rf"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_aug = np.concatenate((train_data_aug[indx], a_train), 0)\n",
        "train_labels_aug = np.concatenate((train_labels_aug[indx], a_label), 0)"
      ],
      "metadata": {
        "id": "Rq2v0jKUleEp"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def PermaDropout(rate):\n",
        "    return Lambda(lambda x: K.dropout(x, level=rate))"
      ],
      "metadata": {
        "id": "YCf8Xy9uo-IL"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######HYPERPARAMATERS###########\n",
        "epochs = 30\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "#################################\n",
        "  \n",
        "model = Sequential()\n",
        "    \n",
        "model.add(Conv2D(256, (3, 3), activation='relu', input_shape=(48, 48, 1), kernel_regularizer=l2(0.01)))\n",
        "model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(PermaDropout(0.4))\n",
        "\n",
        "model.add(Conv2D(384, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(PermaDropout(0.4))\n",
        "\n",
        "model.add(Conv2D(192, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(PermaDropout(0.4))\n",
        "\n",
        "model.add(Conv2D(384, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(PermaDropout(0.4))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(PermaDropout(0.3))\n",
        "model.add(Dense(7, activation='softmax'))\n",
        "\n",
        "adam = keras.optimizers.Adam(learning_rate)\n",
        "model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    \n",
        "# print(model.summary())\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
        "early_stopper = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=30, mode='auto')\n",
        "checkpointer = ModelCheckpoint('weights.hd5', monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "model.fit(\n",
        "          train_data_aug,\n",
        "          train_labels_aug,\n",
        "          epochs = epochs,\n",
        "          batch_size = batch_size,\n",
        "          validation_split = 0.3,\n",
        "          shuffle = True,\n",
        "#           callbacks=[lr_reducer, checkpointer, early_stopper]\n",
        "          callbacks=[lr_reducer, early_stopper]\n",
        "          )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGGYc0lIlvC2",
        "outputId": "c9db0fe3-ef57-4255-8db8-eda30e203117"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "712/712 [==============================] - 151s 191ms/step - loss: 1.9090 - accuracy: 0.2771 - val_loss: 1.9267 - val_accuracy: 0.2023 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "712/712 [==============================] - 132s 185ms/step - loss: 1.4566 - accuracy: 0.4425 - val_loss: 1.2442 - val_accuracy: 0.4798 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "712/712 [==============================] - 131s 183ms/step - loss: 1.2725 - accuracy: 0.5180 - val_loss: 1.2389 - val_accuracy: 0.5408 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "712/712 [==============================] - 130s 183ms/step - loss: 1.1808 - accuracy: 0.5537 - val_loss: 1.4379 - val_accuracy: 0.4552 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "712/712 [==============================] - 130s 183ms/step - loss: 1.1260 - accuracy: 0.5750 - val_loss: 0.8398 - val_accuracy: 0.7193 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "712/712 [==============================] - 130s 183ms/step - loss: 1.0837 - accuracy: 0.5942 - val_loss: 0.9272 - val_accuracy: 0.6618 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "712/712 [==============================] - 130s 182ms/step - loss: 1.0490 - accuracy: 0.6047 - val_loss: 0.7957 - val_accuracy: 0.7279 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "712/712 [==============================] - 130s 182ms/step - loss: 1.0131 - accuracy: 0.6200 - val_loss: 0.9452 - val_accuracy: 0.6683 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "712/712 [==============================] - 130s 182ms/step - loss: 0.9847 - accuracy: 0.6327 - val_loss: 0.9942 - val_accuracy: 0.6432 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "712/712 [==============================] - 130s 182ms/step - loss: 0.9537 - accuracy: 0.6426 - val_loss: 0.9879 - val_accuracy: 0.6404 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "712/712 [==============================] - 130s 182ms/step - loss: 0.9215 - accuracy: 0.6540 - val_loss: 0.7084 - val_accuracy: 0.7403 - lr: 9.0000e-04\n",
            "Epoch 12/30\n",
            "712/712 [==============================] - 130s 182ms/step - loss: 0.8924 - accuracy: 0.6655 - val_loss: 0.8502 - val_accuracy: 0.6904 - lr: 9.0000e-04\n",
            "Epoch 13/30\n",
            "712/712 [==============================] - 130s 182ms/step - loss: 0.8728 - accuracy: 0.6748 - val_loss: 0.6750 - val_accuracy: 0.7653 - lr: 9.0000e-04\n",
            "Epoch 14/30\n",
            "712/712 [==============================] - 129s 182ms/step - loss: 0.8521 - accuracy: 0.6819 - val_loss: 0.6988 - val_accuracy: 0.7471 - lr: 9.0000e-04\n",
            "Epoch 15/30\n",
            "712/712 [==============================] - 129s 182ms/step - loss: 0.8246 - accuracy: 0.6897 - val_loss: 0.7983 - val_accuracy: 0.7083 - lr: 9.0000e-04\n",
            "Epoch 16/30\n",
            "712/712 [==============================] - 129s 182ms/step - loss: 0.8076 - accuracy: 0.6982 - val_loss: 0.8235 - val_accuracy: 0.7020 - lr: 9.0000e-04\n",
            "Epoch 17/30\n",
            "712/712 [==============================] - 130s 182ms/step - loss: 0.7801 - accuracy: 0.7091 - val_loss: 0.6997 - val_accuracy: 0.7411 - lr: 8.1000e-04\n",
            "Epoch 18/30\n",
            "712/712 [==============================] - 133s 187ms/step - loss: 0.7593 - accuracy: 0.7174 - val_loss: 0.7655 - val_accuracy: 0.7198 - lr: 8.1000e-04\n",
            "Epoch 19/30\n",
            "712/712 [==============================] - 129s 182ms/step - loss: 0.7477 - accuracy: 0.7242 - val_loss: 0.7338 - val_accuracy: 0.7278 - lr: 8.1000e-04\n",
            "Epoch 20/30\n",
            "712/712 [==============================] - 129s 182ms/step - loss: 0.7178 - accuracy: 0.7339 - val_loss: 0.8738 - val_accuracy: 0.6787 - lr: 7.2900e-04\n",
            "Epoch 21/30\n",
            "712/712 [==============================] - 129s 182ms/step - loss: 0.7076 - accuracy: 0.7356 - val_loss: 0.7322 - val_accuracy: 0.7286 - lr: 7.2900e-04\n",
            "Epoch 22/30\n",
            "712/712 [==============================] - 129s 181ms/step - loss: 0.6923 - accuracy: 0.7424 - val_loss: 0.7955 - val_accuracy: 0.7096 - lr: 7.2900e-04\n",
            "Epoch 23/30\n",
            "712/712 [==============================] - 129s 182ms/step - loss: 0.6704 - accuracy: 0.7483 - val_loss: 0.6695 - val_accuracy: 0.7586 - lr: 6.5610e-04\n",
            "Epoch 24/30\n",
            "712/712 [==============================] - 129s 182ms/step - loss: 0.6538 - accuracy: 0.7557 - val_loss: 0.7687 - val_accuracy: 0.7136 - lr: 6.5610e-04\n",
            "Epoch 25/30\n",
            "712/712 [==============================] - 130s 182ms/step - loss: 0.6414 - accuracy: 0.7601 - val_loss: 0.7744 - val_accuracy: 0.7174 - lr: 6.5610e-04\n",
            "Epoch 26/30\n",
            "712/712 [==============================] - 130s 182ms/step - loss: 0.6368 - accuracy: 0.7653 - val_loss: 0.7638 - val_accuracy: 0.7208 - lr: 6.5610e-04\n",
            "Epoch 27/30\n",
            "712/712 [==============================] - 129s 182ms/step - loss: 0.6150 - accuracy: 0.7732 - val_loss: 0.7342 - val_accuracy: 0.7364 - lr: 5.9049e-04\n",
            "Epoch 28/30\n",
            "712/712 [==============================] - 129s 182ms/step - loss: 0.5996 - accuracy: 0.7772 - val_loss: 0.7926 - val_accuracy: 0.7112 - lr: 5.9049e-04\n",
            "Epoch 29/30\n",
            "712/712 [==============================] - 129s 182ms/step - loss: 0.5870 - accuracy: 0.7830 - val_loss: 0.7353 - val_accuracy: 0.7358 - lr: 5.9049e-04\n",
            "Epoch 30/30\n",
            "712/712 [==============================] - 129s 182ms/step - loss: 0.5752 - accuracy: 0.7881 - val_loss: 0.7776 - val_accuracy: 0.7244 - lr: 5.3144e-04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8f200324d0>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/파이썬스터디 프로젝트/cnn_ensemble1_model_v2.h5')"
      ],
      "metadata": {
        "id": "x6YHwnQloleD"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "새로 만든 모델만의 검증 결과는 66%대 : 오히려 안좋음"
      ],
      "metadata": {
        "id": "PW1CcDu5_0OJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model('/content/drive/MyDrive/파이썬스터디 프로젝트/cnn_ensemble1_model_v2.h5')\n",
        "model_eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGazsFtGovJ2",
        "outputId": "cbe745af-c4f0-4509-bee2-53f0af12dd04"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.663137364168292"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "그런데 앙상블로 활용하면 69%대 : 좋아짐"
      ],
      "metadata": {
        "id": "TypvHvDkAwNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = keras.models.load_model('/content/drive/MyDrive/파이썬스터디 프로젝트/cnn_base_model_v2.h5')\n",
        "model2 = keras.models.load_model('/content/drive/MyDrive/파이썬스터디 프로젝트/cnn_ensemble1_model_v2.h5')\n",
        "\n",
        "pred_mu1 = model1.predict(test_data)\n",
        "pred_mu2 = model2.predict(test_data)\n",
        "for i in range(1, 30):\n",
        "  pred_mu1 += model1.predict(test_data)\n",
        "  pred_mu2 += model2.predict(test_data)\n",
        "pred_mu1 = pred_mu1/30\n",
        "pred_mu2 = pred_mu2/30\n",
        "\n",
        "predicted_test_labels1 = np.argmax(pred_mu1, axis=1)\n",
        "predicted_test_labels2 = np.argmax(pred_mu2, axis=1)\n",
        "accuracy_score(test_labels2, predicted_test_labels1)\n",
        "accuracy_score(test_labels2, predicted_test_labels2)"
      ],
      "metadata": {
        "id": "qaJO154e8mOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]:\n",
        "  predicted_test_labels12 = np.argmax(k*pred_mu1+(1-k)*pred_mu2, axis=1)\n",
        "  print(str(np.int(k*10)) + \" : \" + str(np.int((1-k)*10)) + \"  앙상블 결과 >>>\" + str(accuracy_score(test_labels2, predicted_test_labels12)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-Reyfkuoz2K",
        "outputId": "0ddebc51-4536-4baf-9120-62bd511b5149"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 : 9  앙상블 결과 >>>0.6765115631095012\n",
            "2 : 8  앙상블 결과 >>>0.6818055168570633\n",
            "3 : 7  앙상블 결과 >>>0.6873780997492338\n",
            "4 : 6  앙상블 결과 >>>0.6937865700752298\n",
            "5 : 5  앙상블 결과 >>>0.6965728615213151\n",
            "6 : 4  앙상블 결과 >>>0.6962942323767066\n",
            "7 : 3  앙상블 결과 >>>0.6946224575090555\n",
            "8 : 1  앙상블 결과 >>>0.6915575369183616\n",
            "9 : 0  앙상블 결과 >>>0.6890498746168849\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "비교: 그냥 모델을 여러번 돌린다고 좋아지는게 아님"
      ],
      "metadata": {
        "id": "-9HgJdmpC99K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = keras.models.load_model('/content/drive/MyDrive/파이썬스터디 프로젝트/cnn_base_model_v2.h5')\n",
        "model2 = keras.models.load_model('/content/drive/MyDrive/파이썬스터디 프로젝트/cnn_ensemble1_model_v2.h5')\n",
        "\n",
        "pred_mu = model1.predict(test_data)\n",
        "# pred_mu += model2.predict(test_data)\n",
        "for i in range(1, 60):\n",
        "  pred_mu += model1.predict(test_data)\n",
        "  # pred_mu += model2.predict(test_data)\n",
        "# pred_mu = pred_mu/30\n",
        "\n",
        "predicted_test_labels = np.argmax(pred_mu, axis=1)\n",
        "accuracy_score(test_labels2, predicted_test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tP9udE67A4Lp",
        "outputId": "35707ff3-c265-44ee-dfec-d389ee919e84"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6823627751462803"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "h3T6PgIiYp8C"
      ],
      "name": "v2.Emotion_Recognition.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}