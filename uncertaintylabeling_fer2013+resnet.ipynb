{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YianKim/2022_uncertainty_aware_semisupervise/blob/main/uncertaintylabeling_fer2013%2Bresnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhyYeRj0SI3d",
        "outputId": "11d5e544-53cc-49f8-ae0a-c38266084009"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQy7KUb6NBs-"
      },
      "source": [
        "## 라이브러리 불러오기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XRPQRtXGmm2W"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import PIL\n",
        "import pickle\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "from keras.layers.core import Lambda\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import LSTM\n",
        "from keras import optimizers\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard, ModelCheckpoint\n",
        "from sklearn.metrics import *\n",
        "from keras.models import load_model\n",
        "\n",
        "from tensorflow import Tensor\n",
        "from tensorflow.keras.layers import Input, Conv2D, ReLU, BatchNormalization,\\\n",
        "                                    Add, AveragePooling2D, Flatten, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpAjYytuPDaU"
      },
      "source": [
        "## 데이터 불러오기\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(dataset_path):\n",
        "  \n",
        "  #classes = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprsie', 'Neutral']\n",
        "\n",
        "  data = []\n",
        "  test_data = []\n",
        "  test_labels = []\n",
        "  labels =[]\n",
        "\n",
        "  with open(dataset_path, 'r') as file:\n",
        "      for line_no, line in enumerate(file.readlines()):\n",
        "          if 0 < line_no <= 35887:\n",
        "            curr_class, line, set_type = line.split(',')\n",
        "            image_data = np.asarray([int(x) for x in line.split()]).reshape(48, 48)\n",
        "            image_data =image_data.astype(np.uint8)/255.0\n",
        "\n",
        "            if (set_type.strip() == 'PrivateTest'):\n",
        "              \n",
        "              test_data.append(image_data)\n",
        "              test_labels.append(curr_class)\n",
        "            else:\n",
        "              data.append(image_data)\n",
        "              labels.append(curr_class)\n",
        "      \n",
        "      test_data = np.expand_dims(test_data, -1)\n",
        "      test_labels = to_categorical(test_labels, num_classes = 7)\n",
        "      data = np.expand_dims(data, -1)   \n",
        "      labels = to_categorical(labels, num_classes = 7)\n",
        "    \n",
        "      return np.array(data), np.array(labels), np.array(test_data), np.array(test_labels)\n",
        "\n",
        "dataset_path = \"/content/drive/MyDrive/파이썬스터디 프로젝트/fer2013.csv/fer2013.csv\"\n",
        "train_data, train_labels, test_data, test_labels = load_data(dataset_path)\n",
        "#train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size = test_size,random_state = seed)\n",
        "\n",
        "#평가용\n",
        "test_labels2 = np.argmax(test_labels, axis=1)\n",
        "\n",
        "# print(\"Number of images in Training set:\", len(train_data))\n",
        "# print(\"Number of images in Test set:\", len(test_data))\n",
        "\n",
        "with open('/content/drive/MyDrive/파이썬스터디 프로젝트/aug_array1.pkl', 'rb') as f:\n",
        "\taug_array1 = pickle.load(f)\n",
        " \n",
        "train_data_aug = np.concatenate((train_data, aug_array1), axis=0)\n",
        "train_labels_aug = np.concatenate((train_labels, train_labels), axis=0)"
      ],
      "metadata": {
        "id": "U_ZB0uDTSGGC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_aug.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COMQNhDiSkGT",
        "outputId": "f5e2e030-5f93-41ca-8105-2576b33da8ef"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64596, 48, 48, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels_aug.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdGqU-jSSlvF",
        "outputId": "e092620d-5621-49cc-966d-fdba63aec2e3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64596, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_data_aug"
      ],
      "metadata": {
        "id": "--YdRp5-S7oi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = train_labels_aug"
      ],
      "metadata": {
        "id": "SGX3CSwwS7_v"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIAa0HMvS5CT"
      },
      "source": [
        "## resnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "P64aH7gGPPTY"
      },
      "outputs": [],
      "source": [
        "def PermaDropout(rate):\n",
        "    return Lambda(lambda x: K.dropout(x, level=rate))\n",
        "\n",
        "def relu_bn(inputs: Tensor) -> Tensor:\n",
        "    relu = ReLU()(inputs)\n",
        "    bn = BatchNormalization()(relu)\n",
        "    return bn\n",
        "\n",
        "def residual_block(x: Tensor, downsample: bool, filters: int, kernel_size: int = 3) -> Tensor:\n",
        "    y = Conv2D(kernel_size=kernel_size,\n",
        "               strides= (1 if not downsample else 2),\n",
        "               filters=filters,\n",
        "               padding=\"same\")(x)\n",
        "    y = relu_bn(y)\n",
        "    y = Conv2D(kernel_size=kernel_size,\n",
        "               strides=1,\n",
        "               filters=filters,\n",
        "               padding=\"same\")(y)\n",
        "\n",
        "    if downsample:\n",
        "        x = Conv2D(kernel_size=1,\n",
        "                   strides=2,\n",
        "                   filters=filters,\n",
        "                   padding=\"same\")(x)\n",
        "    out = Add()([x, y])\n",
        "    out = relu_bn(out)\n",
        "    return out\n",
        "\n",
        "def create_res_net():\n",
        "    \n",
        "    inputs = Input(shape=(48, 48, 1))\n",
        "    num_filters = 32\n",
        "    \n",
        "    t = BatchNormalization()(inputs)\n",
        "    # t = Conv2D(kernel_size=3,\n",
        "    #            strides=1,\n",
        "    #            filters=num_filters,\n",
        "    #            padding=\"same\")(t)\n",
        "    # t = relu_bn(t)\n",
        "    # t = PermaDropout(0.5)(t)\n",
        "    \n",
        "    num_blocks_list = [3,4,6,3]\n",
        "    for i in range(len(num_blocks_list)):\n",
        "        num_blocks = num_blocks_list[i]\n",
        "        for j in range(num_blocks):\n",
        "            t = residual_block(t, downsample=(j==0 and i!=0), filters=num_filters)\n",
        "        num_filters *= 2\n",
        "        t = PermaDropout(0.5)(t)\n",
        "    \n",
        "    t = AveragePooling2D(6)(t)\n",
        "    t = Flatten()(t)\n",
        "    t = PermaDropout(0.5)(t)\n",
        "    outputs = Dense(7, activation='softmax')(t)\n",
        "    \n",
        "    model = Model(inputs, outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NNvVlDJSCt5",
        "outputId": "ee38b97a-8235-4384-b7db-bb86da3c4dba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 48, 48, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 48, 48, 1)   4           ['input_1[0][0]']                \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 48, 48, 32)   320         ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " re_lu (ReLU)                   (None, 48, 48, 32)   0           ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 48, 48, 32)  128         ['re_lu[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 48, 48, 32)   9248        ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 48, 48, 32)   0           ['batch_normalization[0][0]',    \n",
            "                                                                  'conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " re_lu_1 (ReLU)                 (None, 48, 48, 32)   0           ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 48, 48, 32)  128         ['re_lu_1[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 48, 48, 32)   9248        ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " re_lu_2 (ReLU)                 (None, 48, 48, 32)   0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 48, 48, 32)  128         ['re_lu_2[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 48, 48, 32)   9248        ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 48, 48, 32)   0           ['batch_normalization_2[0][0]',  \n",
            "                                                                  'conv2d_3[0][0]']               \n",
            "                                                                                                  \n",
            " re_lu_3 (ReLU)                 (None, 48, 48, 32)   0           ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 48, 48, 32)  128         ['re_lu_3[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 48, 48, 32)   9248        ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " re_lu_4 (ReLU)                 (None, 48, 48, 32)   0           ['conv2d_4[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 48, 48, 32)  128         ['re_lu_4[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 48, 48, 32)   9248        ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 48, 48, 32)   0           ['batch_normalization_4[0][0]',  \n",
            "                                                                  'conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " re_lu_5 (ReLU)                 (None, 48, 48, 32)   0           ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 48, 48, 32)  128         ['re_lu_5[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 48, 48, 32)   0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 24, 24, 64)   18496       ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " re_lu_6 (ReLU)                 (None, 24, 24, 64)   0           ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 24, 24, 64)  256         ['re_lu_6[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 24, 24, 64)   2112        ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 24, 24, 64)   36928       ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 24, 24, 64)   0           ['conv2d_8[0][0]',               \n",
            "                                                                  'conv2d_7[0][0]']               \n",
            "                                                                                                  \n",
            " re_lu_7 (ReLU)                 (None, 24, 24, 64)   0           ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 24, 24, 64)  256         ['re_lu_7[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 24, 24, 64)   36928       ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " re_lu_8 (ReLU)                 (None, 24, 24, 64)   0           ['conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 24, 24, 64)  256         ['re_lu_8[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 24, 24, 64)   36928       ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 24, 24, 64)   0           ['batch_normalization_8[0][0]',  \n",
            "                                                                  'conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " re_lu_9 (ReLU)                 (None, 24, 24, 64)   0           ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 24, 24, 64)  256         ['re_lu_9[0][0]']                \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 24, 24, 64)   36928       ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " re_lu_10 (ReLU)                (None, 24, 24, 64)   0           ['conv2d_11[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 24, 24, 64)  256         ['re_lu_10[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 24, 24, 64)   36928       ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 24, 24, 64)   0           ['batch_normalization_10[0][0]', \n",
            "                                                                  'conv2d_12[0][0]']              \n",
            "                                                                                                  \n",
            " re_lu_11 (ReLU)                (None, 24, 24, 64)   0           ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 24, 24, 64)  256         ['re_lu_11[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 24, 24, 64)   36928       ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " re_lu_12 (ReLU)                (None, 24, 24, 64)   0           ['conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 24, 24, 64)  256         ['re_lu_12[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 24, 24, 64)   36928       ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 24, 24, 64)   0           ['batch_normalization_12[0][0]', \n",
            "                                                                  'conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " re_lu_13 (ReLU)                (None, 24, 24, 64)   0           ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 24, 24, 64)  256         ['re_lu_13[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " lambda_1 (Lambda)              (None, 24, 24, 64)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 12, 12, 128)  73856       ['lambda_1[0][0]']               \n",
            "                                                                                                  \n",
            " re_lu_14 (ReLU)                (None, 12, 12, 128)  0           ['conv2d_15[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 12, 12, 128)  512        ['re_lu_14[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 12, 12, 128)  8320        ['lambda_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 12, 12, 128)  147584      ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 12, 12, 128)  0           ['conv2d_17[0][0]',              \n",
            "                                                                  'conv2d_16[0][0]']              \n",
            "                                                                                                  \n",
            " re_lu_15 (ReLU)                (None, 12, 12, 128)  0           ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 12, 12, 128)  512        ['re_lu_15[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 12, 12, 128)  147584      ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " re_lu_16 (ReLU)                (None, 12, 12, 128)  0           ['conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 12, 12, 128)  512        ['re_lu_16[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 12, 12, 128)  147584      ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 12, 12, 128)  0           ['batch_normalization_16[0][0]', \n",
            "                                                                  'conv2d_19[0][0]']              \n",
            "                                                                                                  \n",
            " re_lu_17 (ReLU)                (None, 12, 12, 128)  0           ['add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 12, 12, 128)  512        ['re_lu_17[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 12, 12, 128)  147584      ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " re_lu_18 (ReLU)                (None, 12, 12, 128)  0           ['conv2d_20[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 12, 12, 128)  512        ['re_lu_18[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 12, 12, 128)  147584      ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " add_9 (Add)                    (None, 12, 12, 128)  0           ['batch_normalization_18[0][0]', \n",
            "                                                                  'conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " re_lu_19 (ReLU)                (None, 12, 12, 128)  0           ['add_9[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 12, 12, 128)  512        ['re_lu_19[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 12, 12, 128)  147584      ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " re_lu_20 (ReLU)                (None, 12, 12, 128)  0           ['conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 12, 12, 128)  512        ['re_lu_20[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 12, 12, 128)  147584      ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " add_10 (Add)                   (None, 12, 12, 128)  0           ['batch_normalization_20[0][0]', \n",
            "                                                                  'conv2d_23[0][0]']              \n",
            "                                                                                                  \n",
            " re_lu_21 (ReLU)                (None, 12, 12, 128)  0           ['add_10[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 12, 12, 128)  512        ['re_lu_21[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 12, 12, 128)  147584      ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " re_lu_22 (ReLU)                (None, 12, 12, 128)  0           ['conv2d_24[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 12, 12, 128)  512        ['re_lu_22[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 12, 12, 128)  147584      ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " add_11 (Add)                   (None, 12, 12, 128)  0           ['batch_normalization_22[0][0]', \n",
            "                                                                  'conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " re_lu_23 (ReLU)                (None, 12, 12, 128)  0           ['add_11[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 12, 12, 128)  512        ['re_lu_23[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 12, 12, 128)  147584      ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " re_lu_24 (ReLU)                (None, 12, 12, 128)  0           ['conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 12, 12, 128)  512        ['re_lu_24[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 12, 12, 128)  147584      ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " add_12 (Add)                   (None, 12, 12, 128)  0           ['batch_normalization_24[0][0]', \n",
            "                                                                  'conv2d_27[0][0]']              \n",
            "                                                                                                  \n",
            " re_lu_25 (ReLU)                (None, 12, 12, 128)  0           ['add_12[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 12, 12, 128)  512        ['re_lu_25[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " lambda_2 (Lambda)              (None, 12, 12, 128)  0           ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 6, 6, 256)    295168      ['lambda_2[0][0]']               \n",
            "                                                                                                  \n",
            " re_lu_26 (ReLU)                (None, 6, 6, 256)    0           ['conv2d_28[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 6, 6, 256)   1024        ['re_lu_26[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 6, 6, 256)    33024       ['lambda_2[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 6, 6, 256)    590080      ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " add_13 (Add)                   (None, 6, 6, 256)    0           ['conv2d_30[0][0]',              \n",
            "                                                                  'conv2d_29[0][0]']              \n",
            "                                                                                                  \n",
            " re_lu_27 (ReLU)                (None, 6, 6, 256)    0           ['add_13[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 6, 6, 256)   1024        ['re_lu_27[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 6, 6, 256)    590080      ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " re_lu_28 (ReLU)                (None, 6, 6, 256)    0           ['conv2d_31[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 6, 6, 256)   1024        ['re_lu_28[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 6, 6, 256)    590080      ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " add_14 (Add)                   (None, 6, 6, 256)    0           ['batch_normalization_28[0][0]', \n",
            "                                                                  'conv2d_32[0][0]']              \n",
            "                                                                                                  \n",
            " re_lu_29 (ReLU)                (None, 6, 6, 256)    0           ['add_14[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 6, 6, 256)   1024        ['re_lu_29[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 6, 6, 256)    590080      ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " re_lu_30 (ReLU)                (None, 6, 6, 256)    0           ['conv2d_33[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 6, 6, 256)   1024        ['re_lu_30[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 6, 6, 256)    590080      ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " add_15 (Add)                   (None, 6, 6, 256)    0           ['batch_normalization_30[0][0]', \n",
            "                                                                  'conv2d_34[0][0]']              \n",
            "                                                                                                  \n",
            " re_lu_31 (ReLU)                (None, 6, 6, 256)    0           ['add_15[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 6, 6, 256)   1024        ['re_lu_31[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " lambda_3 (Lambda)              (None, 6, 6, 256)    0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " average_pooling2d (AveragePool  (None, 1, 1, 256)   0           ['lambda_3[0][0]']               \n",
            " ing2D)                                                                                           \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 256)          0           ['average_pooling2d[0][0]']      \n",
            "                                                                                                  \n",
            " lambda_4 (Lambda)              (None, 256)          0           ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 7)            1799        ['lambda_4[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5,326,763\n",
            "Trainable params: 5,319,209\n",
            "Non-trainable params: 7,554\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = create_res_net() # or create_plain_net()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KwkScUQtSJfJ",
        "outputId": "a55972b2-f20b-4225-9390-a55ca5feaa32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "270/270 [==============================] - 77s 234ms/step - loss: 1.9290 - accuracy: 0.2261 - val_loss: 2.1136 - val_accuracy: 0.2114 - lr: 0.0010\n",
            "Epoch 2/500\n",
            "270/270 [==============================] - 62s 229ms/step - loss: 1.7212 - accuracy: 0.3035 - val_loss: 1.6915 - val_accuracy: 0.3434 - lr: 0.0010\n",
            "Epoch 3/500\n",
            "270/270 [==============================] - 62s 229ms/step - loss: 1.5936 - accuracy: 0.3734 - val_loss: 1.5849 - val_accuracy: 0.3837 - lr: 0.0010\n",
            "Epoch 4/500\n",
            "270/270 [==============================] - 65s 240ms/step - loss: 1.4847 - accuracy: 0.4232 - val_loss: 1.5827 - val_accuracy: 0.4178 - lr: 0.0010\n",
            "Epoch 5/500\n",
            "270/270 [==============================] - 63s 232ms/step - loss: 1.3995 - accuracy: 0.4555 - val_loss: 1.4634 - val_accuracy: 0.4513 - lr: 0.0010\n",
            "Epoch 6/500\n",
            "270/270 [==============================] - 62s 229ms/step - loss: 1.3411 - accuracy: 0.4818 - val_loss: 1.4733 - val_accuracy: 0.4703 - lr: 0.0010\n",
            "Epoch 7/500\n",
            "270/270 [==============================] - 62s 229ms/step - loss: 1.2932 - accuracy: 0.5018 - val_loss: 1.4177 - val_accuracy: 0.4709 - lr: 0.0010\n",
            "Epoch 8/500\n",
            "270/270 [==============================] - 65s 240ms/step - loss: 1.2518 - accuracy: 0.5219 - val_loss: 1.3947 - val_accuracy: 0.4881 - lr: 0.0010\n",
            "Epoch 9/500\n",
            "270/270 [==============================] - 62s 229ms/step - loss: 1.2220 - accuracy: 0.5341 - val_loss: 1.2477 - val_accuracy: 0.5356 - lr: 0.0010\n",
            "Epoch 10/500\n",
            "270/270 [==============================] - 65s 241ms/step - loss: 1.1830 - accuracy: 0.5523 - val_loss: 1.4085 - val_accuracy: 0.5020 - lr: 0.0010\n",
            "Epoch 11/500\n",
            "270/270 [==============================] - 62s 229ms/step - loss: 1.1539 - accuracy: 0.5626 - val_loss: 1.4075 - val_accuracy: 0.5079 - lr: 0.0010\n",
            "Epoch 12/500\n",
            "270/270 [==============================] - 62s 229ms/step - loss: 1.1302 - accuracy: 0.5770 - val_loss: 1.2911 - val_accuracy: 0.5306 - lr: 0.0010\n",
            "Epoch 13/500\n",
            "270/270 [==============================] - 62s 229ms/step - loss: 1.0895 - accuracy: 0.5918 - val_loss: 1.2683 - val_accuracy: 0.5386 - lr: 9.0000e-04\n",
            "Epoch 14/500\n",
            "270/270 [==============================] - 62s 229ms/step - loss: 1.0547 - accuracy: 0.6036 - val_loss: 1.2550 - val_accuracy: 0.5407 - lr: 9.0000e-04\n",
            "Epoch 15/500\n",
            "270/270 [==============================] - 65s 240ms/step - loss: 1.0324 - accuracy: 0.6145 - val_loss: 1.2072 - val_accuracy: 0.5606 - lr: 9.0000e-04\n",
            "Epoch 16/500\n",
            "270/270 [==============================] - 62s 229ms/step - loss: 1.0167 - accuracy: 0.6211 - val_loss: 1.2048 - val_accuracy: 0.5666 - lr: 9.0000e-04\n",
            "Epoch 17/500\n",
            "270/270 [==============================] - 65s 240ms/step - loss: 0.9913 - accuracy: 0.6302 - val_loss: 1.2015 - val_accuracy: 0.5669 - lr: 9.0000e-04\n",
            "Epoch 18/500\n",
            "270/270 [==============================] - 62s 229ms/step - loss: 0.9610 - accuracy: 0.6415 - val_loss: 1.1837 - val_accuracy: 0.5753 - lr: 9.0000e-04\n",
            "Epoch 19/500\n",
            "270/270 [==============================] - 62s 229ms/step - loss: 0.9404 - accuracy: 0.6526 - val_loss: 1.2816 - val_accuracy: 0.5606 - lr: 9.0000e-04\n",
            "Epoch 20/500\n",
            "270/270 [==============================] - 65s 240ms/step - loss: 0.9077 - accuracy: 0.6635 - val_loss: 1.2212 - val_accuracy: 0.5678 - lr: 9.0000e-04\n",
            "Epoch 21/500\n",
            "270/270 [==============================] - 62s 229ms/step - loss: 0.8726 - accuracy: 0.6805 - val_loss: 1.2219 - val_accuracy: 0.5601 - lr: 9.0000e-04\n",
            "Epoch 22/500\n",
            "270/270 [==============================] - 62s 229ms/step - loss: 0.8177 - accuracy: 0.7016 - val_loss: 1.2175 - val_accuracy: 0.5859 - lr: 8.1000e-04\n",
            "Epoch 23/500\n",
            "270/270 [==============================] - 62s 229ms/step - loss: 0.7823 - accuracy: 0.7130 - val_loss: 1.2172 - val_accuracy: 0.5867 - lr: 8.1000e-04\n",
            "Epoch 24/500\n",
            "270/270 [==============================] - 65s 240ms/step - loss: 0.7447 - accuracy: 0.7247 - val_loss: 1.2437 - val_accuracy: 0.5690 - lr: 8.1000e-04\n",
            "Epoch 25/500\n",
            "270/270 [==============================] - 62s 229ms/step - loss: 0.6844 - accuracy: 0.7527 - val_loss: 1.2574 - val_accuracy: 0.5941 - lr: 7.2900e-04\n",
            "Epoch 26/500\n",
            "270/270 [==============================] - 65s 240ms/step - loss: 0.6418 - accuracy: 0.7697 - val_loss: 1.2650 - val_accuracy: 0.5918 - lr: 7.2900e-04\n",
            "Epoch 27/500\n",
            "270/270 [==============================] - 65s 240ms/step - loss: 0.5941 - accuracy: 0.7873 - val_loss: 1.3669 - val_accuracy: 0.5714 - lr: 7.2900e-04\n",
            "Epoch 28/500\n",
            "270/270 [==============================] - 62s 229ms/step - loss: 0.5353 - accuracy: 0.8081 - val_loss: 1.3831 - val_accuracy: 0.5925 - lr: 6.5610e-04\n",
            "Epoch 29/500\n",
            "270/270 [==============================] - 62s 229ms/step - loss: 0.4853 - accuracy: 0.8283 - val_loss: 1.3821 - val_accuracy: 0.5891 - lr: 6.5610e-04\n",
            "Epoch 30/500\n",
            "270/270 [==============================] - 62s 229ms/step - loss: 0.4455 - accuracy: 0.8411 - val_loss: 1.4575 - val_accuracy: 0.5791 - lr: 6.5610e-04\n",
            "Epoch 31/500\n",
            "270/270 [==============================] - 65s 240ms/step - loss: 0.3770 - accuracy: 0.8712 - val_loss: 1.5977 - val_accuracy: 0.5812 - lr: 5.9049e-04\n",
            "Epoch 32/500\n",
            "270/270 [==============================] - 62s 229ms/step - loss: 0.3410 - accuracy: 0.8813 - val_loss: 1.5942 - val_accuracy: 0.5903 - lr: 5.9049e-04\n",
            "Epoch 33/500\n",
            "270/270 [==============================] - 62s 229ms/step - loss: 0.2967 - accuracy: 0.8987 - val_loss: 1.6753 - val_accuracy: 0.5845 - lr: 5.9049e-04\n",
            "Epoch 34/500\n",
            "270/270 [==============================] - 62s 229ms/step - loss: 0.2505 - accuracy: 0.9130 - val_loss: 1.7473 - val_accuracy: 0.5818 - lr: 5.3144e-04\n",
            "Epoch 35/500\n",
            "270/270 [==============================] - 62s 230ms/step - loss: 0.2247 - accuracy: 0.9225 - val_loss: 1.7830 - val_accuracy: 0.5932 - lr: 5.3144e-04\n",
            "Epoch 36/500\n",
            "270/270 [==============================] - 62s 229ms/step - loss: 0.2099 - accuracy: 0.9294 - val_loss: 1.8858 - val_accuracy: 0.5882 - lr: 5.3144e-04\n",
            "Epoch 37/500\n",
            "270/270 [==============================] - 62s 230ms/step - loss: 0.1654 - accuracy: 0.9449 - val_loss: 1.9538 - val_accuracy: 0.5952 - lr: 4.7830e-04\n",
            "Epoch 38/500\n",
            "270/270 [==============================] - 62s 230ms/step - loss: 0.1568 - accuracy: 0.9449 - val_loss: 2.0552 - val_accuracy: 0.5925 - lr: 4.7830e-04\n",
            "Epoch 39/500\n",
            "270/270 [==============================] - 62s 230ms/step - loss: 0.1505 - accuracy: 0.9485 - val_loss: 2.1682 - val_accuracy: 0.5949 - lr: 4.7830e-04\n",
            "Epoch 40/500\n",
            "270/270 [==============================] - 62s 230ms/step - loss: 0.1160 - accuracy: 0.9608 - val_loss: 2.1466 - val_accuracy: 0.5938 - lr: 4.3047e-04\n",
            "Epoch 41/500\n",
            "270/270 [==============================] - 62s 231ms/step - loss: 0.1091 - accuracy: 0.9643 - val_loss: 2.1158 - val_accuracy: 0.6046 - lr: 4.3047e-04\n",
            "Epoch 42/500\n",
            "270/270 [==============================] - 62s 230ms/step - loss: 0.1082 - accuracy: 0.9628 - val_loss: 2.1890 - val_accuracy: 0.6022 - lr: 4.3047e-04\n",
            "Epoch 43/500\n",
            "270/270 [==============================] - 62s 229ms/step - loss: 0.0948 - accuracy: 0.9679 - val_loss: 2.1983 - val_accuracy: 0.6071 - lr: 3.8742e-04\n",
            "Epoch 44/500\n",
            "270/270 [==============================] - 65s 239ms/step - loss: 0.0874 - accuracy: 0.9709 - val_loss: 2.3295 - val_accuracy: 0.5999 - lr: 3.8742e-04\n",
            "Epoch 45/500\n",
            "270/270 [==============================] - 62s 228ms/step - loss: 0.0772 - accuracy: 0.9742 - val_loss: 2.3719 - val_accuracy: 0.5934 - lr: 3.8742e-04\n",
            "Epoch 46/500\n",
            "270/270 [==============================] - 61s 228ms/step - loss: 0.0694 - accuracy: 0.9764 - val_loss: 2.4098 - val_accuracy: 0.6012 - lr: 3.4868e-04\n",
            "Epoch 47/500\n",
            "270/270 [==============================] - 65s 239ms/step - loss: 0.0585 - accuracy: 0.9806 - val_loss: 2.4330 - val_accuracy: 0.5982 - lr: 3.4868e-04\n",
            "Epoch 48/500\n",
            "270/270 [==============================] - 62s 229ms/step - loss: 0.0633 - accuracy: 0.9797 - val_loss: 2.4672 - val_accuracy: 0.5979 - lr: 3.4868e-04\n",
            "Epoch 49/500\n",
            "270/270 [==============================] - 62s 228ms/step - loss: 0.0596 - accuracy: 0.9807 - val_loss: 2.4366 - val_accuracy: 0.6028 - lr: 3.1381e-04\n",
            "Epoch 50/500\n",
            "270/270 [==============================] - 65s 239ms/step - loss: 0.0515 - accuracy: 0.9837 - val_loss: 2.5119 - val_accuracy: 0.6036 - lr: 3.1381e-04\n",
            "Epoch 51/500\n",
            "270/270 [==============================] - 62s 228ms/step - loss: 0.0545 - accuracy: 0.9834 - val_loss: 2.5243 - val_accuracy: 0.6092 - lr: 3.1381e-04\n",
            "Epoch 52/500\n",
            "270/270 [==============================] - 65s 240ms/step - loss: 0.0473 - accuracy: 0.9834 - val_loss: 2.5140 - val_accuracy: 0.6012 - lr: 2.8243e-04\n",
            "Epoch 53/500\n",
            "270/270 [==============================] - 65s 239ms/step - loss: 0.0404 - accuracy: 0.9860 - val_loss: 2.5393 - val_accuracy: 0.6088 - lr: 2.8243e-04\n",
            "Epoch 54/500\n",
            "270/270 [==============================] - 62s 228ms/step - loss: 0.0363 - accuracy: 0.9888 - val_loss: 2.6108 - val_accuracy: 0.6036 - lr: 2.8243e-04\n",
            "Epoch 55/500\n",
            "270/270 [==============================] - 62s 228ms/step - loss: 0.0396 - accuracy: 0.9879 - val_loss: 2.6055 - val_accuracy: 0.6024 - lr: 2.5419e-04\n",
            "Epoch 56/500\n",
            "270/270 [==============================] - 62s 229ms/step - loss: 0.0328 - accuracy: 0.9899 - val_loss: 2.5373 - val_accuracy: 0.6075 - lr: 2.5419e-04\n",
            "Epoch 57/500\n",
            "270/270 [==============================] - 62s 229ms/step - loss: 0.0336 - accuracy: 0.9884 - val_loss: 2.5805 - val_accuracy: 0.6071 - lr: 2.5419e-04\n",
            "Epoch 58/500\n",
            "270/270 [==============================] - 61s 228ms/step - loss: 0.0327 - accuracy: 0.9899 - val_loss: 2.7035 - val_accuracy: 0.6001 - lr: 2.2877e-04\n",
            "Epoch 59/500\n",
            "270/270 [==============================] - 62s 228ms/step - loss: 0.0394 - accuracy: 0.9876 - val_loss: 2.5898 - val_accuracy: 0.6105 - lr: 2.2877e-04\n",
            "Epoch 60/500\n",
            "270/270 [==============================] - 62s 228ms/step - loss: 0.0300 - accuracy: 0.9913 - val_loss: 2.6626 - val_accuracy: 0.5951 - lr: 2.2877e-04\n",
            "Epoch 61/500\n",
            "270/270 [==============================] - 65s 239ms/step - loss: 0.0295 - accuracy: 0.9908 - val_loss: 2.7496 - val_accuracy: 0.6014 - lr: 2.0589e-04\n",
            "Epoch 62/500\n",
            "270/270 [==============================] - 62s 229ms/step - loss: 0.0293 - accuracy: 0.9908 - val_loss: 2.7534 - val_accuracy: 0.5999 - lr: 2.0589e-04\n",
            "Epoch 63/500\n",
            "270/270 [==============================] - 62s 228ms/step - loss: 0.0226 - accuracy: 0.9926 - val_loss: 2.7335 - val_accuracy: 0.6109 - lr: 2.0589e-04\n",
            "Epoch 64/500\n",
            "270/270 [==============================] - 61s 228ms/step - loss: 0.0245 - accuracy: 0.9928 - val_loss: 2.7767 - val_accuracy: 0.6069 - lr: 1.8530e-04\n",
            "Epoch 65/500\n",
            "270/270 [==============================] - 62s 228ms/step - loss: 0.0216 - accuracy: 0.9933 - val_loss: 2.7665 - val_accuracy: 0.6132 - lr: 1.8530e-04\n",
            "Epoch 66/500\n",
            "270/270 [==============================] - 62s 228ms/step - loss: 0.0192 - accuracy: 0.9937 - val_loss: 2.8217 - val_accuracy: 0.6059 - lr: 1.8530e-04\n",
            "Epoch 67/500\n",
            "270/270 [==============================] - 62s 229ms/step - loss: 0.0196 - accuracy: 0.9932 - val_loss: 2.7567 - val_accuracy: 0.6139 - lr: 1.6677e-04\n",
            "Epoch 68/500\n",
            "270/270 [==============================] - 62s 228ms/step - loss: 0.0163 - accuracy: 0.9945 - val_loss: 2.7562 - val_accuracy: 0.6060 - lr: 1.6677e-04\n",
            "Epoch 69/500\n",
            "270/270 [==============================] - 62s 228ms/step - loss: 0.0177 - accuracy: 0.9937 - val_loss: 2.8014 - val_accuracy: 0.6121 - lr: 1.6677e-04\n",
            "Epoch 70/500\n",
            "270/270 [==============================] - 62s 228ms/step - loss: 0.0195 - accuracy: 0.9933 - val_loss: 2.7894 - val_accuracy: 0.6132 - lr: 1.5009e-04\n",
            "Epoch 71/500\n",
            "270/270 [==============================] - 62s 228ms/step - loss: 0.0176 - accuracy: 0.9947 - val_loss: 2.7494 - val_accuracy: 0.6040 - lr: 1.5009e-04\n",
            "Epoch 72/500\n",
            "270/270 [==============================] - 65s 239ms/step - loss: 0.0183 - accuracy: 0.9940 - val_loss: 2.7502 - val_accuracy: 0.6079 - lr: 1.5009e-04\n",
            "Epoch 73/500\n",
            "270/270 [==============================] - 65s 239ms/step - loss: 0.0142 - accuracy: 0.9955 - val_loss: 2.8479 - val_accuracy: 0.6094 - lr: 1.3509e-04\n",
            "Epoch 74/500\n",
            "270/270 [==============================] - 62s 228ms/step - loss: 0.0148 - accuracy: 0.9944 - val_loss: 2.7843 - val_accuracy: 0.6093 - lr: 1.3509e-04\n",
            "Epoch 75/500\n",
            "270/270 [==============================] - 61s 228ms/step - loss: 0.0166 - accuracy: 0.9941 - val_loss: 2.8799 - val_accuracy: 0.6004 - lr: 1.3509e-04\n",
            "Epoch 76/500\n",
            "270/270 [==============================] - 62s 228ms/step - loss: 0.0146 - accuracy: 0.9948 - val_loss: 2.8814 - val_accuracy: 0.6058 - lr: 1.2158e-04\n",
            "Epoch 77/500\n",
            "270/270 [==============================] - 65s 240ms/step - loss: 0.0131 - accuracy: 0.9958 - val_loss: 2.8472 - val_accuracy: 0.6073 - lr: 1.2158e-04\n",
            "Epoch 78/500\n",
            "270/270 [==============================] - 65s 240ms/step - loss: 0.0123 - accuracy: 0.9962 - val_loss: 2.8507 - val_accuracy: 0.6107 - lr: 1.2158e-04\n",
            "Epoch 79/500\n",
            "270/270 [==============================] - 62s 228ms/step - loss: 0.0126 - accuracy: 0.9963 - val_loss: 2.8886 - val_accuracy: 0.6067 - lr: 1.0942e-04\n",
            "Epoch 80/500\n",
            "270/270 [==============================] - 61s 227ms/step - loss: 0.0116 - accuracy: 0.9962 - val_loss: 2.8466 - val_accuracy: 0.6149 - lr: 1.0942e-04\n",
            "Epoch 81/500\n",
            "270/270 [==============================] - 61s 228ms/step - loss: 0.0125 - accuracy: 0.9961 - val_loss: 2.9189 - val_accuracy: 0.6025 - lr: 1.0942e-04\n",
            "Epoch 82/500\n",
            "270/270 [==============================] - 65s 239ms/step - loss: 0.0132 - accuracy: 0.9956 - val_loss: 2.8863 - val_accuracy: 0.6081 - lr: 9.8477e-05\n",
            "Epoch 83/500\n",
            "270/270 [==============================] - 62s 228ms/step - loss: 0.0109 - accuracy: 0.9961 - val_loss: 2.8763 - val_accuracy: 0.6085 - lr: 9.8477e-05\n",
            "Epoch 84/500\n",
            "270/270 [==============================] - 61s 228ms/step - loss: 0.0102 - accuracy: 0.9963 - val_loss: 2.8956 - val_accuracy: 0.6052 - lr: 9.8477e-05\n",
            "Epoch 85/500\n",
            "270/270 [==============================] - 65s 239ms/step - loss: 0.0122 - accuracy: 0.9959 - val_loss: 2.8758 - val_accuracy: 0.6115 - lr: 8.8629e-05\n",
            "Epoch 86/500\n",
            "270/270 [==============================] - 65s 240ms/step - loss: 0.0102 - accuracy: 0.9961 - val_loss: 2.9177 - val_accuracy: 0.6120 - lr: 8.8629e-05\n",
            "Epoch 87/500\n",
            "270/270 [==============================] - 62s 229ms/step - loss: 0.0105 - accuracy: 0.9967 - val_loss: 2.9616 - val_accuracy: 0.6113 - lr: 8.8629e-05\n",
            "Epoch 88/500\n",
            "270/270 [==============================] - 62s 228ms/step - loss: 0.0105 - accuracy: 0.9965 - val_loss: 2.9005 - val_accuracy: 0.6113 - lr: 7.9766e-05\n",
            "Epoch 89/500\n",
            "270/270 [==============================] - 62s 229ms/step - loss: 0.0088 - accuracy: 0.9972 - val_loss: 2.8940 - val_accuracy: 0.6062 - lr: 7.9766e-05\n",
            "Epoch 90/500\n",
            "270/270 [==============================] - 62s 228ms/step - loss: 0.0098 - accuracy: 0.9966 - val_loss: 2.8614 - val_accuracy: 0.6117 - lr: 7.9766e-05\n",
            "Epoch 91/500\n",
            "270/270 [==============================] - 62s 229ms/step - loss: 0.0085 - accuracy: 0.9972 - val_loss: 2.9236 - val_accuracy: 0.6109 - lr: 7.1790e-05\n",
            "Epoch 92/500\n",
            "270/270 [==============================] - 62s 230ms/step - loss: 0.0081 - accuracy: 0.9974 - val_loss: 2.8857 - val_accuracy: 0.6149 - lr: 7.1790e-05\n",
            "Epoch 93/500\n",
            "270/270 [==============================] - 65s 241ms/step - loss: 0.0080 - accuracy: 0.9974 - val_loss: 2.9639 - val_accuracy: 0.6109 - lr: 7.1790e-05\n",
            "Epoch 94/500\n",
            "270/270 [==============================] - 65s 241ms/step - loss: 0.0090 - accuracy: 0.9969 - val_loss: 2.9339 - val_accuracy: 0.6081 - lr: 6.4611e-05\n",
            "Epoch 95/500\n",
            "270/270 [==============================] - 65s 242ms/step - loss: 0.0073 - accuracy: 0.9974 - val_loss: 2.9472 - val_accuracy: 0.6058 - lr: 6.4611e-05\n",
            "Epoch 96/500\n",
            "270/270 [==============================] - 62s 230ms/step - loss: 0.0074 - accuracy: 0.9973 - val_loss: 2.9111 - val_accuracy: 0.6155 - lr: 6.4611e-05\n",
            "Epoch 97/500\n",
            "270/270 [==============================] - 62s 230ms/step - loss: 0.0061 - accuracy: 0.9977 - val_loss: 2.9537 - val_accuracy: 0.6079 - lr: 5.8150e-05\n",
            "Epoch 98/500\n",
            "270/270 [==============================] - 65s 241ms/step - loss: 0.0066 - accuracy: 0.9975 - val_loss: 2.9553 - val_accuracy: 0.6147 - lr: 5.8150e-05\n",
            "Epoch 99/500\n",
            "270/270 [==============================] - 62s 230ms/step - loss: 0.0064 - accuracy: 0.9976 - val_loss: 2.9325 - val_accuracy: 0.6132 - lr: 5.8150e-05\n",
            "Epoch 100/500\n",
            "270/270 [==============================] - 65s 241ms/step - loss: 0.0068 - accuracy: 0.9976 - val_loss: 2.9841 - val_accuracy: 0.6158 - lr: 5.2335e-05\n",
            "Epoch 101/500\n",
            "270/270 [==============================] - 62s 230ms/step - loss: 0.0062 - accuracy: 0.9979 - val_loss: 2.9503 - val_accuracy: 0.6165 - lr: 5.2335e-05\n",
            "Epoch 102/500\n",
            "270/270 [==============================] - 65s 241ms/step - loss: 0.0073 - accuracy: 0.9971 - val_loss: 2.9868 - val_accuracy: 0.6100 - lr: 5.2335e-05\n",
            "Epoch 103/500\n",
            "270/270 [==============================] - 62s 230ms/step - loss: 0.0062 - accuracy: 0.9976 - val_loss: 2.9358 - val_accuracy: 0.6168 - lr: 4.7101e-05\n",
            "Epoch 104/500\n",
            "270/270 [==============================] - 62s 230ms/step - loss: 0.0062 - accuracy: 0.9980 - val_loss: 2.9572 - val_accuracy: 0.6108 - lr: 4.7101e-05\n",
            "Epoch 105/500\n",
            "270/270 [==============================] - 62s 230ms/step - loss: 0.0065 - accuracy: 0.9973 - val_loss: 2.9663 - val_accuracy: 0.6166 - lr: 4.7101e-05\n",
            "Epoch 106/500\n",
            "270/270 [==============================] - 62s 230ms/step - loss: 0.0052 - accuracy: 0.9980 - val_loss: 2.9516 - val_accuracy: 0.6121 - lr: 4.2391e-05\n",
            "Epoch 107/500\n",
            "270/270 [==============================] - 62s 229ms/step - loss: 0.0058 - accuracy: 0.9976 - val_loss: 2.9619 - val_accuracy: 0.6131 - lr: 4.2391e-05\n",
            "Epoch 108/500\n",
            "270/270 [==============================] - 62s 229ms/step - loss: 0.0067 - accuracy: 0.9974 - val_loss: 2.9735 - val_accuracy: 0.6169 - lr: 4.2391e-05\n",
            "Epoch 109/500\n",
            "270/270 [==============================] - 62s 230ms/step - loss: 0.0059 - accuracy: 0.9976 - val_loss: 2.9628 - val_accuracy: 0.6146 - lr: 3.8152e-05\n",
            "Epoch 110/500\n",
            "270/270 [==============================] - 65s 241ms/step - loss: 0.0054 - accuracy: 0.9980 - val_loss: 2.9943 - val_accuracy: 0.6104 - lr: 3.8152e-05\n",
            "Epoch 111/500\n",
            "270/270 [==============================] - 62s 230ms/step - loss: 0.0053 - accuracy: 0.9981 - val_loss: 2.9744 - val_accuracy: 0.6146 - lr: 3.8152e-05\n",
            "Epoch 112/500\n",
            "270/270 [==============================] - 62s 230ms/step - loss: 0.0056 - accuracy: 0.9978 - val_loss: 3.0210 - val_accuracy: 0.6157 - lr: 3.4337e-05\n",
            "Epoch 113/500\n",
            "270/270 [==============================] - 65s 241ms/step - loss: 0.0056 - accuracy: 0.9977 - val_loss: 2.9752 - val_accuracy: 0.6149 - lr: 3.4337e-05\n",
            "Epoch 114/500\n",
            "270/270 [==============================] - 62s 230ms/step - loss: 0.0046 - accuracy: 0.9985 - val_loss: 2.9871 - val_accuracy: 0.6107 - lr: 3.4337e-05\n",
            "Epoch 115/500\n",
            "270/270 [==============================] - 62s 230ms/step - loss: 0.0047 - accuracy: 0.9980 - val_loss: 2.9754 - val_accuracy: 0.6149 - lr: 3.0903e-05\n",
            "Epoch 116/500\n",
            "270/270 [==============================] - 62s 228ms/step - loss: 0.0058 - accuracy: 0.9975 - val_loss: 2.9821 - val_accuracy: 0.6139 - lr: 3.0903e-05\n",
            "Epoch 117/500\n",
            "270/270 [==============================] - 61s 228ms/step - loss: 0.0050 - accuracy: 0.9983 - val_loss: 2.9867 - val_accuracy: 0.6115 - lr: 3.0903e-05\n",
            "Epoch 118/500\n",
            "270/270 [==============================] - 62s 228ms/step - loss: 0.0059 - accuracy: 0.9976 - val_loss: 2.9690 - val_accuracy: 0.6200 - lr: 2.7813e-05\n",
            "Epoch 119/500\n",
            "270/270 [==============================] - 62s 228ms/step - loss: 0.0054 - accuracy: 0.9979 - val_loss: 2.9403 - val_accuracy: 0.6162 - lr: 2.7813e-05\n",
            "Epoch 120/500\n",
            "270/270 [==============================] - 62s 228ms/step - loss: 0.0042 - accuracy: 0.9981 - val_loss: 2.9964 - val_accuracy: 0.6159 - lr: 2.7813e-05\n",
            "Epoch 121/500\n",
            "270/270 [==============================] - 61s 228ms/step - loss: 0.0044 - accuracy: 0.9985 - val_loss: 2.9800 - val_accuracy: 0.6196 - lr: 2.5032e-05\n",
            "Epoch 122/500\n",
            "270/270 [==============================] - 62s 228ms/step - loss: 0.0061 - accuracy: 0.9977 - val_loss: 3.0113 - val_accuracy: 0.6140 - lr: 2.5032e-05\n",
            "Epoch 123/500\n",
            "270/270 [==============================] - 62s 228ms/step - loss: 0.0057 - accuracy: 0.9978 - val_loss: 3.0240 - val_accuracy: 0.6107 - lr: 2.5032e-05\n",
            "Epoch 124/500\n",
            "270/270 [==============================] - 62s 228ms/step - loss: 0.0049 - accuracy: 0.9980 - val_loss: 3.0265 - val_accuracy: 0.6143 - lr: 2.2528e-05\n",
            "Epoch 125/500\n",
            "270/270 [==============================] - 61s 228ms/step - loss: 0.0051 - accuracy: 0.9982 - val_loss: 2.9785 - val_accuracy: 0.6166 - lr: 2.2528e-05\n",
            "Epoch 126/500\n",
            "270/270 [==============================] - 61s 227ms/step - loss: 0.0053 - accuracy: 0.9980 - val_loss: 2.9676 - val_accuracy: 0.6193 - lr: 2.2528e-05\n",
            "Epoch 127/500\n",
            "270/270 [==============================] - 62s 228ms/step - loss: 0.0044 - accuracy: 0.9981 - val_loss: 2.9806 - val_accuracy: 0.6161 - lr: 2.0276e-05\n",
            "Epoch 128/500\n",
            "270/270 [==============================] - 61s 228ms/step - loss: 0.0047 - accuracy: 0.9983 - val_loss: 2.9885 - val_accuracy: 0.6162 - lr: 2.0276e-05\n",
            "Epoch 129/500\n",
            "270/270 [==============================] - 65s 239ms/step - loss: 0.0042 - accuracy: 0.9983 - val_loss: 2.9969 - val_accuracy: 0.6121 - lr: 2.0276e-05\n",
            "Epoch 130/500\n",
            "270/270 [==============================] - 62s 228ms/step - loss: 0.0038 - accuracy: 0.9985 - val_loss: 3.0247 - val_accuracy: 0.6184 - lr: 1.8248e-05\n",
            "Epoch 131/500\n",
            "270/270 [==============================] - 62s 228ms/step - loss: 0.0051 - accuracy: 0.9978 - val_loss: 3.0259 - val_accuracy: 0.6197 - lr: 1.8248e-05\n",
            "Epoch 132/500\n",
            "270/270 [==============================] - 62s 228ms/step - loss: 0.0050 - accuracy: 0.9979 - val_loss: 3.0520 - val_accuracy: 0.6161 - lr: 1.8248e-05\n",
            "Epoch 133/500\n",
            "270/270 [==============================] - 62s 228ms/step - loss: 0.0048 - accuracy: 0.9983 - val_loss: 3.0305 - val_accuracy: 0.6166 - lr: 1.6423e-05\n",
            "Epoch 134/500\n",
            "270/270 [==============================] - 62s 228ms/step - loss: 0.0044 - accuracy: 0.9981 - val_loss: 3.0599 - val_accuracy: 0.6154 - lr: 1.6423e-05\n",
            "Epoch 135/500\n",
            "270/270 [==============================] - 61s 228ms/step - loss: 0.0037 - accuracy: 0.9987 - val_loss: 3.0383 - val_accuracy: 0.6115 - lr: 1.6423e-05\n",
            "Epoch 136/500\n",
            "270/270 [==============================] - 65s 239ms/step - loss: 0.0040 - accuracy: 0.9985 - val_loss: 2.9983 - val_accuracy: 0.6125 - lr: 1.4781e-05\n",
            "Epoch 137/500\n",
            "270/270 [==============================] - 62s 229ms/step - loss: 0.0039 - accuracy: 0.9984 - val_loss: 3.0370 - val_accuracy: 0.6161 - lr: 1.4781e-05\n",
            "Epoch 138/500\n",
            "270/270 [==============================] - 65s 240ms/step - loss: 0.0054 - accuracy: 0.9979 - val_loss: 3.0226 - val_accuracy: 0.6158 - lr: 1.4781e-05\n",
            "Epoch 139/500\n",
            "270/270 [==============================] - 62s 228ms/step - loss: 0.0048 - accuracy: 0.9980 - val_loss: 3.0468 - val_accuracy: 0.6172 - lr: 1.3303e-05\n",
            "Epoch 140/500\n",
            "270/270 [==============================] - 65s 240ms/step - loss: 0.0045 - accuracy: 0.9980 - val_loss: 3.0349 - val_accuracy: 0.6168 - lr: 1.3303e-05\n",
            "Epoch 141/500\n",
            "270/270 [==============================] - 65s 240ms/step - loss: 0.0039 - accuracy: 0.9985 - val_loss: 3.0301 - val_accuracy: 0.6151 - lr: 1.3303e-05\n",
            "Epoch 142/500\n",
            "270/270 [==============================] - 62s 229ms/step - loss: 0.0043 - accuracy: 0.9983 - val_loss: 3.0011 - val_accuracy: 0.6178 - lr: 1.1973e-05\n",
            "Epoch 143/500\n",
            "270/270 [==============================] - 62s 228ms/step - loss: 0.0035 - accuracy: 0.9986 - val_loss: 3.0234 - val_accuracy: 0.6108 - lr: 1.1973e-05\n",
            "Epoch 144/500\n",
            "270/270 [==============================] - 65s 240ms/step - loss: 0.0039 - accuracy: 0.9984 - val_loss: 3.0440 - val_accuracy: 0.6173 - lr: 1.1973e-05\n",
            "Epoch 145/500\n",
            "270/270 [==============================] - 62s 228ms/step - loss: 0.0041 - accuracy: 0.9985 - val_loss: 3.0386 - val_accuracy: 0.6153 - lr: 1.0775e-05\n",
            "Epoch 146/500\n",
            "270/270 [==============================] - 65s 240ms/step - loss: 0.0042 - accuracy: 0.9986 - val_loss: 3.0228 - val_accuracy: 0.6192 - lr: 1.0775e-05\n",
            "Epoch 147/500\n",
            "270/270 [==============================] - 65s 240ms/step - loss: 0.0038 - accuracy: 0.9984 - val_loss: 3.0338 - val_accuracy: 0.6117 - lr: 1.0775e-05\n",
            "Epoch 148/500\n",
            "270/270 [==============================] - 62s 229ms/step - loss: 0.0043 - accuracy: 0.9983 - val_loss: 3.0167 - val_accuracy: 0.6193 - lr: 9.6977e-06\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8fb0607b50>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
        "# early_stopper = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=30, mode='auto')\n",
        "\n",
        "# model.fit(\n",
        "#     x=train_data,\n",
        "#     y=train_labels,\n",
        "#     epochs=500,\n",
        "#     verbose=1,\n",
        "#     validation_split=0.3,\n",
        "#     batch_size=64,\n",
        "#     callbacks=[lr_reducer, early_stopper]\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uY9o0L1zlu7j"
      },
      "outputs": [],
      "source": [
        "# model.save('/content/drive/MyDrive/파이썬스터디 프로젝트/labeling_fer_resnet.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TD8ppILhFUrz"
      },
      "source": [
        "## 모델 결과"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7Ljx2EWA2v2n"
      },
      "outputs": [],
      "source": [
        "def model_eval():\n",
        "  pred_mu = model.predict(test_data)\n",
        "  for i in range(1, 30):\n",
        "    pred_mu += model.predict(test_data)\n",
        "  pred_mu = pred_mu/30\n",
        "  predicted_test_labels = np.argmax(pred_mu, axis=1)\n",
        "  return(accuracy_score(np.argmax(test_labels, axis=1), predicted_test_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OzhQw9NOWjl3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9093fd65-0b77-4044-d693-81c9cb829eca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6912789077737531"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# model = keras.models.load_model('/content/drive/MyDrive/파이썬스터디 프로젝트/labeling_fer_resnet.h5')\n",
        "model = create_res_net()\n",
        "model.load_weights('/content/drive/MyDrive/파이썬스터디 프로젝트/labeling_fer_resnet.h5')\n",
        "model_eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcbs04885lTu"
      },
      "source": [
        "# 라벨링"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAtfoQKLZDJC"
      },
      "source": [
        "## 라벨링; 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/파이썬스터디 프로젝트/face_array.pkl', 'rb') as f:\n",
        "\tunlab_data = pickle.load(f)"
      ],
      "metadata": {
        "id": "_6SiKXN6jfgO"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "GKJgmFGWhlfl"
      },
      "outputs": [],
      "source": [
        "# model = keras.models.load_model('/content/drive/MyDrive/파이썬스터디 프로젝트/labeling_fer_resnet.h5')\n",
        "model = create_res_net()\n",
        "model.load_weights('/content/drive/MyDrive/파이썬스터디 프로젝트/labeling_fer_resnet.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_Jb5VX9ZC3y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e0e76ac-d76a-4b6e-dff9-b109e2968675"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 6/20 [07:57<18:25, 78.94s/it]"
          ]
        }
      ],
      "source": [
        "# Vars : 분산들\n",
        "# Outs : 결과 값들\n",
        "\n",
        "Vars = []\n",
        "Outs = []\n",
        "\n",
        "temp1 = []\n",
        "\n",
        "for i in tqdm(range(20)):\n",
        "  temp1.append(model.predict(unlab_data))\n",
        "\n",
        "for j in tqdm(range(unlab_data.shape[0])):\n",
        "\n",
        "  temp2 = np.array([0,0,0,0,0,0,0], 'float32')\n",
        "  \n",
        "  for i in range(20):\n",
        "    temp2 += temp1[i][j]\n",
        "  Outs.append(temp2/20)\n",
        "  \n",
        "  outputs=[]\n",
        "\n",
        "  for i in range(20):\n",
        "    outputs.append(temp1[i][j][np.argmax(temp2)])\n",
        "  Vars.append(np.var(outputs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NnAVIXTXlAa"
      },
      "outputs": [],
      "source": [
        "Outs = np.array(Outs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxIFHtGqAF9B"
      },
      "source": [
        "class마다 균등하게 선택"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGGGKCzvAiZb"
      },
      "outputs": [],
      "source": [
        "Counter(np.argmax(unlab_labels, axis=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXY6ufSkE_rP"
      },
      "source": [
        "### 불확정성\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEPgF4EWEdGx"
      },
      "outputs": [],
      "source": [
        "# upper bound of variance?\n",
        "\n",
        "UB25 = []\n",
        "UB50 = []\n",
        "UB75 = []\n",
        "\n",
        "\n",
        "for h in range(7):\n",
        "  classvars = []\n",
        "  for i in tqdm(range(67224)):\n",
        "    if np.argmax(unlab_labels, axis=1).tolist()[i]==h:\n",
        "      classvars.append(Vars[i])\n",
        "  UB25.append(np.percentile(classvars, 25))\n",
        "  UB50.append(np.percentile(classvars, 50))\n",
        "  UB75.append(np.percentile(classvars, 75))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2m-BBSNFRO6"
      },
      "outputs": [],
      "source": [
        "# UB25 < UB50 < UB75"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOq_mV-uFGKX"
      },
      "outputs": [],
      "source": [
        "vars25 = []\n",
        "vars50 = []\n",
        "vars75 = []\n",
        "vars100 = []\n",
        "\n",
        "ind = 0 \n",
        "\n",
        "\n",
        "for i in Vars:\n",
        "  if i <= UB25[np.argmax(unlab_labels, axis=1)[ind]]:\n",
        "    vars25.append(ind)\n",
        "  elif i <= UB50[np.argmax(unlab_labels, axis=1)[ind]]:\n",
        "    vars50.append(ind)\n",
        "  elif i <= UB75[np.argmax(unlab_labels, axis=1)[ind]]:\n",
        "    vars75.append(ind)\n",
        "  else:\n",
        "    vars100.append(ind)\n",
        "  ind += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kx2MFUxKh6d"
      },
      "outputs": [],
      "source": [
        "k1 = random.sample(range(len(vars25)), len(vars25))\n",
        "k2 = random.sample(range(len(vars50)), len(vars50))\n",
        "k3 = random.sample(range(len(vars75)), len(vars75))\n",
        "k4 = random.sample(range(len(vars100)), len(vars100))\n",
        "\n",
        "lowvars = k1[0:len(k1)]+k2[0:len(k2)]\n",
        "highvars = k3[0:len(k3)]+k4[0:len(k4)]\n",
        "\n",
        "clstvars1 = k1[0:np.int(len(k1)/2)] + k2[0:np.int(len(k2)/2)] + k3[0:np.int(len(k3)/2)] + k4[0:np.int(len(k4)/2)]\n",
        "clstvars2 = k1[np.int(len(k1)/2):len(k1)] + k2[np.int(len(k2)/2):len(k2)] + k3[np.int(len(k3)/2):len(k3)] + k4[np.int(len(k4)/2):len(k4)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ka9xFulFGKY"
      },
      "outputs": [],
      "source": [
        "train_data_1 = unlab_data[lowvars]\n",
        "train_labels_1 = Outs[lowvars]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLqnYj4ZFGKZ"
      },
      "outputs": [],
      "source": [
        "shufindx = random.sample(range(train_data_1.shape[0]),train_data_1.shape[0])\n",
        "train_data_1 = train_data_1[shufindx]\n",
        "train_labels_1 = train_labels_1[shufindx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUv8cAJwQ-VX"
      },
      "outputs": [],
      "source": [
        "train_data_2 = unlab_data[clstvars1]\n",
        "train_labels_2 = Outs[clstvars1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUodIRYhQ-VX"
      },
      "outputs": [],
      "source": [
        "shufindx = random.sample(range(train_data_2.shape[0]),train_data_2.shape[0])\n",
        "train_data_2 = train_data_2[shufindx]\n",
        "train_labels_2 = train_labels_2[shufindx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvb7FhRaFGKZ"
      },
      "outputs": [],
      "source": [
        "train_data_3 = unlab_data[randomindx]\n",
        "train_labels_3 = Outs[randomindx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsSWcAFHFGKZ"
      },
      "outputs": [],
      "source": [
        "shufindx = random.sample(range(train_data_3.shape[0]),train_data_3.shape[0])\n",
        "train_data_3 = train_data_3[shufindx]\n",
        "train_labels_3 = train_labels_3[shufindx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F39Oc7V8FGKZ"
      },
      "outputs": [],
      "source": [
        "unlab_data_1 = unlab_data[highvars]\n",
        "unlab_labels_1 = unlab_labels[highvars]\n",
        "unlab_data_2 = unlab_data[clstvars2]\n",
        "unlab_labels_2 = unlab_labels[clstvars2]\n",
        "unlab_data_3 = unlab_data[randomindx2]\n",
        "unlab_labels_3 = unlab_labels[randomindx2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMw4VfmaFGKZ"
      },
      "outputs": [],
      "source": [
        "Counter(np.argmax(train_labels_1, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5ZZKXxBFGKZ"
      },
      "outputs": [],
      "source": [
        "Counter(np.argmax(train_labels_2, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKgTMCL8Q-VZ"
      },
      "outputs": [],
      "source": [
        "Counter(np.argmax(train_labels_3, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y05Wb3HZFGKa"
      },
      "outputs": [],
      "source": [
        "Counter(np.argmax(unlab_labels_1, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHWvlgD4FGKa"
      },
      "outputs": [],
      "source": [
        "Counter(np.argmax(unlab_labels_2, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4WYCa4aQ-VZ"
      },
      "outputs": [],
      "source": [
        "Counter(np.argmax(unlab_labels_3, axis=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOtFA9fr5oDF"
      },
      "source": [
        "## 모델링\n",
        "\n",
        "간단한 모델에서 받은 지식를 복잡한 모델에서 학습 \n",
        "\n",
        "uncertainty aware data vs random sampling data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9u3DDkIzmpzz"
      },
      "source": [
        "#### Resnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBBsEBDGQ-Va"
      },
      "outputs": [],
      "source": [
        "def relu_bn(inputs: Tensor) -> Tensor:\n",
        "    relu = ReLU()(inputs)\n",
        "    bn = BatchNormalization()(relu)\n",
        "    return bn\n",
        "\n",
        "def create_simple_net():\n",
        "    \n",
        "    inputs = Input(shape=(48, 48, 1))\n",
        "    num_filters = 256\n",
        "    \n",
        "    t = BatchNormalization()(inputs)\n",
        "    t = Conv2D(kernel_size=3,\n",
        "               strides=1,\n",
        "               filters=num_filters,\n",
        "               padding=\"same\")(t)\n",
        "    t = relu_bn(t)\n",
        "    t = Conv2D(kernel_size=3,\n",
        "               strides=1,\n",
        "               filters=num_filters,\n",
        "               padding=\"same\")(t)\n",
        "    t = relu_bn(t)\n",
        "    t = Dropout(0.5)(t)\n",
        "    t = Flatten()(t)\n",
        "    outputs = Dense(7, activation='softmax')(t)\n",
        "    \n",
        "    model = Model(inputs, outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def model_eval():\n",
        "  pred_mu = model.predict(test_data)\n",
        "  for i in range(1, 30):\n",
        "    pred_mu += model.predict(test_data)\n",
        "  pred_mu = pred_mu/30\n",
        "  predicted_test_labels = np.argmax(pred_mu, axis=1)\n",
        "  return(accuracy_score(np.argmax(test_labels, axis=1), predicted_test_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HmHMKXKQ-Va",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8e263c9-502f-46bf-b5a2-478bd35f6a08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "219/219 [==============================] - 15s 56ms/step - loss: 8.2907 - accuracy: 0.3673 - val_loss: 4.2329 - val_accuracy: 0.2795 - lr: 0.0010\n",
            "Epoch 2/500\n",
            "219/219 [==============================] - 11s 48ms/step - loss: 5.4086 - accuracy: 0.5383 - val_loss: 4.9902 - val_accuracy: 0.2181 - lr: 0.0010\n",
            "Epoch 3/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 3.5815 - accuracy: 0.6332 - val_loss: 2.6574 - val_accuracy: 0.5352 - lr: 0.0010\n",
            "Epoch 4/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 2.5584 - accuracy: 0.7035 - val_loss: 2.6286 - val_accuracy: 0.6562 - lr: 0.0010\n",
            "Epoch 5/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 1.8950 - accuracy: 0.7544 - val_loss: 2.1456 - val_accuracy: 0.6914 - lr: 0.0010\n",
            "Epoch 6/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 1.4230 - accuracy: 0.7897 - val_loss: 1.6314 - val_accuracy: 0.7292 - lr: 0.0010\n",
            "Epoch 7/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 1.0636 - accuracy: 0.8175 - val_loss: 1.3242 - val_accuracy: 0.7439 - lr: 0.0010\n",
            "Epoch 8/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.8117 - accuracy: 0.8412 - val_loss: 1.0996 - val_accuracy: 0.7615 - lr: 0.0010\n",
            "Epoch 9/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.6640 - accuracy: 0.8661 - val_loss: 0.9447 - val_accuracy: 0.7697 - lr: 0.0010\n",
            "Epoch 10/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.5790 - accuracy: 0.8822 - val_loss: 0.9016 - val_accuracy: 0.7712 - lr: 0.0010\n",
            "Epoch 11/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.5466 - accuracy: 0.8922 - val_loss: 0.8700 - val_accuracy: 0.7765 - lr: 0.0010\n",
            "Epoch 12/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.5241 - accuracy: 0.8975 - val_loss: 0.8458 - val_accuracy: 0.7834 - lr: 0.0010\n",
            "Epoch 13/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.5131 - accuracy: 0.9051 - val_loss: 0.8451 - val_accuracy: 0.7842 - lr: 0.0010\n",
            "Epoch 14/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.5022 - accuracy: 0.9037 - val_loss: 0.8221 - val_accuracy: 0.7925 - lr: 0.0010\n",
            "Epoch 15/500\n",
            "219/219 [==============================] - 10s 48ms/step - loss: 0.4948 - accuracy: 0.9077 - val_loss: 0.8246 - val_accuracy: 0.7894 - lr: 0.0010\n",
            "Epoch 16/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.4940 - accuracy: 0.9038 - val_loss: 0.8142 - val_accuracy: 0.7902 - lr: 0.0010\n",
            "Epoch 17/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.4873 - accuracy: 0.9098 - val_loss: 0.8116 - val_accuracy: 0.7810 - lr: 0.0010\n",
            "Epoch 18/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.4781 - accuracy: 0.9114 - val_loss: 0.8199 - val_accuracy: 0.7824 - lr: 0.0010\n",
            "Epoch 19/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.4812 - accuracy: 0.9082 - val_loss: 0.8013 - val_accuracy: 0.7867 - lr: 0.0010\n",
            "Epoch 20/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.4776 - accuracy: 0.9114 - val_loss: 0.8048 - val_accuracy: 0.7880 - lr: 0.0010\n",
            "Epoch 21/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.4758 - accuracy: 0.9142 - val_loss: 0.8054 - val_accuracy: 0.7842 - lr: 0.0010\n",
            "Epoch 22/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.4746 - accuracy: 0.9123 - val_loss: 0.7984 - val_accuracy: 0.7815 - lr: 0.0010\n",
            "Epoch 23/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.4724 - accuracy: 0.9127 - val_loss: 0.8053 - val_accuracy: 0.7837 - lr: 0.0010\n",
            "Epoch 24/500\n",
            "219/219 [==============================] - 11s 49ms/step - loss: 0.4787 - accuracy: 0.9092 - val_loss: 0.7996 - val_accuracy: 0.7817 - lr: 0.0010\n",
            "Epoch 25/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.4716 - accuracy: 0.9134 - val_loss: 0.8000 - val_accuracy: 0.7752 - lr: 0.0010\n",
            "Epoch 26/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.4665 - accuracy: 0.9121 - val_loss: 0.7860 - val_accuracy: 0.7927 - lr: 9.0000e-04\n",
            "Epoch 27/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.4626 - accuracy: 0.9187 - val_loss: 0.7823 - val_accuracy: 0.7880 - lr: 9.0000e-04\n",
            "Epoch 28/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.4620 - accuracy: 0.9154 - val_loss: 0.7816 - val_accuracy: 0.7849 - lr: 9.0000e-04\n",
            "Epoch 29/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.4613 - accuracy: 0.9157 - val_loss: 0.7917 - val_accuracy: 0.7862 - lr: 9.0000e-04\n",
            "Epoch 30/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.4631 - accuracy: 0.9138 - val_loss: 0.7808 - val_accuracy: 0.7845 - lr: 9.0000e-04\n",
            "Epoch 31/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.4633 - accuracy: 0.9114 - val_loss: 0.7949 - val_accuracy: 0.7844 - lr: 9.0000e-04\n",
            "Epoch 32/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.4626 - accuracy: 0.9136 - val_loss: 0.7864 - val_accuracy: 0.7819 - lr: 9.0000e-04\n",
            "Epoch 33/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.4618 - accuracy: 0.9128 - val_loss: 0.7782 - val_accuracy: 0.7829 - lr: 9.0000e-04\n",
            "Epoch 34/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.4622 - accuracy: 0.9139 - val_loss: 0.7768 - val_accuracy: 0.7789 - lr: 9.0000e-04\n",
            "Epoch 35/500\n",
            "219/219 [==============================] - 10s 48ms/step - loss: 0.4567 - accuracy: 0.9155 - val_loss: 0.8005 - val_accuracy: 0.7755 - lr: 9.0000e-04\n",
            "Epoch 36/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.4579 - accuracy: 0.9136 - val_loss: 0.7678 - val_accuracy: 0.7899 - lr: 9.0000e-04\n",
            "Epoch 37/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.4600 - accuracy: 0.9131 - val_loss: 0.7905 - val_accuracy: 0.7799 - lr: 9.0000e-04\n",
            "Epoch 38/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.4605 - accuracy: 0.9115 - val_loss: 0.7763 - val_accuracy: 0.7860 - lr: 9.0000e-04\n",
            "Epoch 39/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.4560 - accuracy: 0.9140 - val_loss: 0.7660 - val_accuracy: 0.7860 - lr: 9.0000e-04\n",
            "Epoch 40/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.4521 - accuracy: 0.9173 - val_loss: 0.7685 - val_accuracy: 0.7817 - lr: 9.0000e-04\n",
            "Epoch 41/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.4544 - accuracy: 0.9149 - val_loss: 0.7698 - val_accuracy: 0.7794 - lr: 9.0000e-04\n",
            "Epoch 42/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.4512 - accuracy: 0.9177 - val_loss: 0.7699 - val_accuracy: 0.7804 - lr: 9.0000e-04\n",
            "Epoch 43/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.4467 - accuracy: 0.9197 - val_loss: 0.7602 - val_accuracy: 0.7847 - lr: 8.1000e-04\n",
            "Epoch 44/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.4423 - accuracy: 0.9165 - val_loss: 0.7525 - val_accuracy: 0.7910 - lr: 8.1000e-04\n",
            "Epoch 45/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.4401 - accuracy: 0.9184 - val_loss: 0.7531 - val_accuracy: 0.7882 - lr: 8.1000e-04\n",
            "Epoch 46/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.4418 - accuracy: 0.9175 - val_loss: 0.7476 - val_accuracy: 0.7899 - lr: 8.1000e-04\n",
            "Epoch 47/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.4386 - accuracy: 0.9184 - val_loss: 0.7506 - val_accuracy: 0.7890 - lr: 8.1000e-04\n",
            "Epoch 48/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.4382 - accuracy: 0.9187 - val_loss: 0.7604 - val_accuracy: 0.7910 - lr: 8.1000e-04\n",
            "Epoch 49/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.4402 - accuracy: 0.9173 - val_loss: 0.7545 - val_accuracy: 0.7927 - lr: 8.1000e-04\n",
            "Epoch 50/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.4344 - accuracy: 0.9199 - val_loss: 0.7490 - val_accuracy: 0.7942 - lr: 7.2900e-04\n",
            "Epoch 51/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.4290 - accuracy: 0.9206 - val_loss: 0.7428 - val_accuracy: 0.7939 - lr: 7.2900e-04\n",
            "Epoch 52/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.4294 - accuracy: 0.9199 - val_loss: 0.7348 - val_accuracy: 0.7959 - lr: 7.2900e-04\n",
            "Epoch 53/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.4280 - accuracy: 0.9168 - val_loss: 0.7373 - val_accuracy: 0.7960 - lr: 7.2900e-04\n",
            "Epoch 54/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.4288 - accuracy: 0.9231 - val_loss: 0.7379 - val_accuracy: 0.7969 - lr: 7.2900e-04\n",
            "Epoch 55/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.4285 - accuracy: 0.9190 - val_loss: 0.7398 - val_accuracy: 0.7914 - lr: 7.2900e-04\n",
            "Epoch 56/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.4227 - accuracy: 0.9239 - val_loss: 0.7267 - val_accuracy: 0.7934 - lr: 6.5610e-04\n",
            "Epoch 57/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.4198 - accuracy: 0.9241 - val_loss: 0.7345 - val_accuracy: 0.7959 - lr: 6.5610e-04\n",
            "Epoch 58/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.4220 - accuracy: 0.9228 - val_loss: 0.7369 - val_accuracy: 0.7912 - lr: 6.5610e-04\n",
            "Epoch 59/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.4200 - accuracy: 0.9216 - val_loss: 0.7370 - val_accuracy: 0.7964 - lr: 6.5610e-04\n",
            "Epoch 60/500\n",
            "219/219 [==============================] - 10s 48ms/step - loss: 0.4166 - accuracy: 0.9274 - val_loss: 0.7293 - val_accuracy: 0.8015 - lr: 5.9049e-04\n",
            "Epoch 61/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.4136 - accuracy: 0.9239 - val_loss: 0.7305 - val_accuracy: 0.7999 - lr: 5.9049e-04\n",
            "Epoch 62/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.4126 - accuracy: 0.9264 - val_loss: 0.7197 - val_accuracy: 0.7979 - lr: 5.9049e-04\n",
            "Epoch 63/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.4127 - accuracy: 0.9270 - val_loss: 0.7277 - val_accuracy: 0.8049 - lr: 5.9049e-04\n",
            "Epoch 64/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.4130 - accuracy: 0.9241 - val_loss: 0.7340 - val_accuracy: 0.7977 - lr: 5.9049e-04\n",
            "Epoch 65/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.4120 - accuracy: 0.9274 - val_loss: 0.7310 - val_accuracy: 0.7982 - lr: 5.9049e-04\n",
            "Epoch 66/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.4086 - accuracy: 0.9284 - val_loss: 0.7209 - val_accuracy: 0.8035 - lr: 5.3144e-04\n",
            "Epoch 67/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.4054 - accuracy: 0.9269 - val_loss: 0.7252 - val_accuracy: 0.8002 - lr: 5.3144e-04\n",
            "Epoch 68/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.4083 - accuracy: 0.9239 - val_loss: 0.7237 - val_accuracy: 0.7979 - lr: 5.3144e-04\n",
            "Epoch 69/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.4061 - accuracy: 0.9270 - val_loss: 0.7215 - val_accuracy: 0.8064 - lr: 4.7830e-04\n",
            "Epoch 70/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.4028 - accuracy: 0.9298 - val_loss: 0.7140 - val_accuracy: 0.8030 - lr: 4.7830e-04\n",
            "Epoch 71/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.4042 - accuracy: 0.9269 - val_loss: 0.7130 - val_accuracy: 0.8034 - lr: 4.7830e-04\n",
            "Epoch 72/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.4019 - accuracy: 0.9304 - val_loss: 0.7200 - val_accuracy: 0.8057 - lr: 4.7830e-04\n",
            "Epoch 73/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.4041 - accuracy: 0.9249 - val_loss: 0.7177 - val_accuracy: 0.8042 - lr: 4.7830e-04\n",
            "Epoch 74/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.4016 - accuracy: 0.9294 - val_loss: 0.7141 - val_accuracy: 0.8037 - lr: 4.7830e-04\n",
            "Epoch 75/500\n",
            "219/219 [==============================] - 10s 48ms/step - loss: 0.3995 - accuracy: 0.9324 - val_loss: 0.7157 - val_accuracy: 0.8039 - lr: 4.3047e-04\n",
            "Epoch 76/500\n",
            "219/219 [==============================] - 10s 48ms/step - loss: 0.3992 - accuracy: 0.9279 - val_loss: 0.7147 - val_accuracy: 0.8007 - lr: 4.3047e-04\n",
            "Epoch 77/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3989 - accuracy: 0.9298 - val_loss: 0.7134 - val_accuracy: 0.8047 - lr: 4.3047e-04\n",
            "Epoch 78/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3964 - accuracy: 0.9321 - val_loss: 0.7120 - val_accuracy: 0.8020 - lr: 3.8742e-04\n",
            "Epoch 79/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3963 - accuracy: 0.9279 - val_loss: 0.7061 - val_accuracy: 0.8064 - lr: 3.8742e-04\n",
            "Epoch 80/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3952 - accuracy: 0.9297 - val_loss: 0.7048 - val_accuracy: 0.8089 - lr: 3.8742e-04\n",
            "Epoch 81/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3960 - accuracy: 0.9291 - val_loss: 0.7100 - val_accuracy: 0.8077 - lr: 3.8742e-04\n",
            "Epoch 82/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3934 - accuracy: 0.9309 - val_loss: 0.7119 - val_accuracy: 0.8004 - lr: 3.8742e-04\n",
            "Epoch 83/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3961 - accuracy: 0.9314 - val_loss: 0.7032 - val_accuracy: 0.8087 - lr: 3.8742e-04\n",
            "Epoch 84/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3933 - accuracy: 0.9312 - val_loss: 0.7126 - val_accuracy: 0.8084 - lr: 3.8742e-04\n",
            "Epoch 85/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3944 - accuracy: 0.9299 - val_loss: 0.7107 - val_accuracy: 0.8059 - lr: 3.8742e-04\n",
            "Epoch 86/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3945 - accuracy: 0.9307 - val_loss: 0.7059 - val_accuracy: 0.8085 - lr: 3.8742e-04\n",
            "Epoch 87/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3927 - accuracy: 0.9297 - val_loss: 0.7133 - val_accuracy: 0.8070 - lr: 3.4868e-04\n",
            "Epoch 88/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3920 - accuracy: 0.9294 - val_loss: 0.7121 - val_accuracy: 0.8057 - lr: 3.4868e-04\n",
            "Epoch 89/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3914 - accuracy: 0.9304 - val_loss: 0.7090 - val_accuracy: 0.8050 - lr: 3.4868e-04\n",
            "Epoch 90/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3904 - accuracy: 0.9344 - val_loss: 0.7109 - val_accuracy: 0.8100 - lr: 3.1381e-04\n",
            "Epoch 91/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3899 - accuracy: 0.9354 - val_loss: 0.7061 - val_accuracy: 0.8067 - lr: 3.1381e-04\n",
            "Epoch 92/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3889 - accuracy: 0.9356 - val_loss: 0.7079 - val_accuracy: 0.8062 - lr: 3.1381e-04\n",
            "Epoch 93/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3879 - accuracy: 0.9321 - val_loss: 0.7023 - val_accuracy: 0.8105 - lr: 2.8243e-04\n",
            "Epoch 94/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3873 - accuracy: 0.9347 - val_loss: 0.7059 - val_accuracy: 0.8119 - lr: 2.8243e-04\n",
            "Epoch 95/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3873 - accuracy: 0.9365 - val_loss: 0.7044 - val_accuracy: 0.8122 - lr: 2.8243e-04\n",
            "Epoch 96/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3868 - accuracy: 0.9366 - val_loss: 0.7057 - val_accuracy: 0.8074 - lr: 2.8243e-04\n",
            "Epoch 97/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3867 - accuracy: 0.9326 - val_loss: 0.7024 - val_accuracy: 0.8117 - lr: 2.5419e-04\n",
            "Epoch 98/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3847 - accuracy: 0.9359 - val_loss: 0.7052 - val_accuracy: 0.8157 - lr: 2.5419e-04\n",
            "Epoch 99/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3852 - accuracy: 0.9369 - val_loss: 0.6990 - val_accuracy: 0.8117 - lr: 2.5419e-04\n",
            "Epoch 100/500\n",
            "219/219 [==============================] - 10s 45ms/step - loss: 0.3853 - accuracy: 0.9321 - val_loss: 0.7019 - val_accuracy: 0.8114 - lr: 2.5419e-04\n",
            "Epoch 101/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3848 - accuracy: 0.9366 - val_loss: 0.7005 - val_accuracy: 0.8110 - lr: 2.5419e-04\n",
            "Epoch 102/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3850 - accuracy: 0.9339 - val_loss: 0.7023 - val_accuracy: 0.8134 - lr: 2.5419e-04\n",
            "Epoch 103/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3841 - accuracy: 0.9334 - val_loss: 0.7076 - val_accuracy: 0.8114 - lr: 2.2877e-04\n",
            "Epoch 104/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3836 - accuracy: 0.9351 - val_loss: 0.7014 - val_accuracy: 0.8094 - lr: 2.2877e-04\n",
            "Epoch 105/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3832 - accuracy: 0.9356 - val_loss: 0.7030 - val_accuracy: 0.8129 - lr: 2.2877e-04\n",
            "Epoch 106/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3822 - accuracy: 0.9353 - val_loss: 0.7004 - val_accuracy: 0.8129 - lr: 2.0589e-04\n",
            "Epoch 107/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3830 - accuracy: 0.9360 - val_loss: 0.7005 - val_accuracy: 0.8124 - lr: 2.0589e-04\n",
            "Epoch 108/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3824 - accuracy: 0.9362 - val_loss: 0.6993 - val_accuracy: 0.8119 - lr: 2.0589e-04\n",
            "Epoch 109/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3817 - accuracy: 0.9361 - val_loss: 0.7016 - val_accuracy: 0.8115 - lr: 1.8530e-04\n",
            "Epoch 110/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3814 - accuracy: 0.9361 - val_loss: 0.7014 - val_accuracy: 0.8099 - lr: 1.8530e-04\n",
            "Epoch 111/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3812 - accuracy: 0.9376 - val_loss: 0.6972 - val_accuracy: 0.8172 - lr: 1.8530e-04\n",
            "Epoch 112/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3814 - accuracy: 0.9376 - val_loss: 0.6981 - val_accuracy: 0.8129 - lr: 1.8530e-04\n",
            "Epoch 113/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3810 - accuracy: 0.9387 - val_loss: 0.7006 - val_accuracy: 0.8157 - lr: 1.8530e-04\n",
            "Epoch 114/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3806 - accuracy: 0.9351 - val_loss: 0.6980 - val_accuracy: 0.8130 - lr: 1.8530e-04\n",
            "Epoch 115/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3796 - accuracy: 0.9365 - val_loss: 0.6972 - val_accuracy: 0.8129 - lr: 1.6677e-04\n",
            "Epoch 116/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3797 - accuracy: 0.9346 - val_loss: 0.6995 - val_accuracy: 0.8142 - lr: 1.6677e-04\n",
            "Epoch 117/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3798 - accuracy: 0.9383 - val_loss: 0.6985 - val_accuracy: 0.8129 - lr: 1.6677e-04\n",
            "Epoch 118/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3789 - accuracy: 0.9381 - val_loss: 0.6974 - val_accuracy: 0.8177 - lr: 1.5009e-04\n",
            "Epoch 119/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3786 - accuracy: 0.9372 - val_loss: 0.6977 - val_accuracy: 0.8167 - lr: 1.5009e-04\n",
            "Epoch 120/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3788 - accuracy: 0.9376 - val_loss: 0.6946 - val_accuracy: 0.8184 - lr: 1.5009e-04\n",
            "Epoch 121/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3793 - accuracy: 0.9371 - val_loss: 0.6959 - val_accuracy: 0.8160 - lr: 1.5009e-04\n",
            "Epoch 122/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3797 - accuracy: 0.9389 - val_loss: 0.6994 - val_accuracy: 0.8152 - lr: 1.5009e-04\n",
            "Epoch 123/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3781 - accuracy: 0.9401 - val_loss: 0.6952 - val_accuracy: 0.8164 - lr: 1.5009e-04\n",
            "Epoch 124/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3780 - accuracy: 0.9363 - val_loss: 0.6970 - val_accuracy: 0.8217 - lr: 1.3509e-04\n",
            "Epoch 125/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3784 - accuracy: 0.9389 - val_loss: 0.6946 - val_accuracy: 0.8187 - lr: 1.3509e-04\n",
            "Epoch 126/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3777 - accuracy: 0.9387 - val_loss: 0.6963 - val_accuracy: 0.8210 - lr: 1.3509e-04\n",
            "Epoch 127/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3766 - accuracy: 0.9421 - val_loss: 0.6952 - val_accuracy: 0.8187 - lr: 1.2158e-04\n",
            "Epoch 128/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3765 - accuracy: 0.9377 - val_loss: 0.6964 - val_accuracy: 0.8154 - lr: 1.2158e-04\n",
            "Epoch 129/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3774 - accuracy: 0.9348 - val_loss: 0.6998 - val_accuracy: 0.8195 - lr: 1.2158e-04\n",
            "Epoch 130/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3758 - accuracy: 0.9401 - val_loss: 0.6963 - val_accuracy: 0.8209 - lr: 1.0942e-04\n",
            "Epoch 131/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3771 - accuracy: 0.9397 - val_loss: 0.6987 - val_accuracy: 0.8200 - lr: 1.0942e-04\n",
            "Epoch 132/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3761 - accuracy: 0.9404 - val_loss: 0.6966 - val_accuracy: 0.8197 - lr: 1.0942e-04\n",
            "Epoch 133/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3755 - accuracy: 0.9395 - val_loss: 0.6958 - val_accuracy: 0.8187 - lr: 9.8477e-05\n",
            "Epoch 134/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3754 - accuracy: 0.9395 - val_loss: 0.6961 - val_accuracy: 0.8209 - lr: 9.8477e-05\n",
            "Epoch 135/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3751 - accuracy: 0.9409 - val_loss: 0.6947 - val_accuracy: 0.8207 - lr: 9.8477e-05\n",
            "Epoch 136/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3758 - accuracy: 0.9374 - val_loss: 0.6947 - val_accuracy: 0.8224 - lr: 8.8629e-05\n",
            "Epoch 137/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3751 - accuracy: 0.9421 - val_loss: 0.6940 - val_accuracy: 0.8209 - lr: 8.8629e-05\n",
            "Epoch 138/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3753 - accuracy: 0.9389 - val_loss: 0.6965 - val_accuracy: 0.8205 - lr: 8.8629e-05\n",
            "Epoch 139/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3751 - accuracy: 0.9394 - val_loss: 0.6955 - val_accuracy: 0.8219 - lr: 8.8629e-05\n",
            "Epoch 140/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3763 - accuracy: 0.9369 - val_loss: 0.6962 - val_accuracy: 0.8209 - lr: 8.8629e-05\n",
            "Epoch 141/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3746 - accuracy: 0.9400 - val_loss: 0.6966 - val_accuracy: 0.8207 - lr: 7.9766e-05\n",
            "Epoch 142/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3745 - accuracy: 0.9369 - val_loss: 0.6949 - val_accuracy: 0.8252 - lr: 7.9766e-05\n",
            "Epoch 143/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3742 - accuracy: 0.9404 - val_loss: 0.6946 - val_accuracy: 0.8244 - lr: 7.9766e-05\n",
            "Epoch 144/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3743 - accuracy: 0.9386 - val_loss: 0.6944 - val_accuracy: 0.8252 - lr: 7.1790e-05\n",
            "Epoch 145/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3742 - accuracy: 0.9398 - val_loss: 0.6933 - val_accuracy: 0.8242 - lr: 7.1790e-05\n",
            "Epoch 146/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3738 - accuracy: 0.9411 - val_loss: 0.6959 - val_accuracy: 0.8239 - lr: 7.1790e-05\n",
            "Epoch 147/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3729 - accuracy: 0.9430 - val_loss: 0.6955 - val_accuracy: 0.8244 - lr: 7.1790e-05\n",
            "Epoch 148/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3738 - accuracy: 0.9412 - val_loss: 0.6935 - val_accuracy: 0.8240 - lr: 7.1790e-05\n",
            "Epoch 149/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3745 - accuracy: 0.9409 - val_loss: 0.6923 - val_accuracy: 0.8239 - lr: 6.4611e-05\n",
            "Epoch 150/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3741 - accuracy: 0.9389 - val_loss: 0.6933 - val_accuracy: 0.8244 - lr: 6.4611e-05\n",
            "Epoch 151/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3744 - accuracy: 0.9406 - val_loss: 0.6932 - val_accuracy: 0.8255 - lr: 6.4611e-05\n",
            "Epoch 152/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3738 - accuracy: 0.9417 - val_loss: 0.6946 - val_accuracy: 0.8257 - lr: 6.4611e-05\n",
            "Epoch 153/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3734 - accuracy: 0.9414 - val_loss: 0.6936 - val_accuracy: 0.8269 - lr: 5.8150e-05\n",
            "Epoch 154/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3742 - accuracy: 0.9393 - val_loss: 0.6932 - val_accuracy: 0.8285 - lr: 5.8150e-05\n",
            "Epoch 155/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3736 - accuracy: 0.9391 - val_loss: 0.6943 - val_accuracy: 0.8290 - lr: 5.8150e-05\n",
            "Epoch 156/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3739 - accuracy: 0.9408 - val_loss: 0.6922 - val_accuracy: 0.8307 - lr: 5.2335e-05\n",
            "Epoch 157/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3736 - accuracy: 0.9438 - val_loss: 0.6935 - val_accuracy: 0.8302 - lr: 5.2335e-05\n",
            "Epoch 158/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3734 - accuracy: 0.9427 - val_loss: 0.6918 - val_accuracy: 0.8295 - lr: 5.2335e-05\n",
            "Epoch 159/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3733 - accuracy: 0.9406 - val_loss: 0.6908 - val_accuracy: 0.8284 - lr: 5.2335e-05\n",
            "Epoch 160/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3740 - accuracy: 0.9424 - val_loss: 0.6927 - val_accuracy: 0.8285 - lr: 5.2335e-05\n",
            "Epoch 161/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3730 - accuracy: 0.9406 - val_loss: 0.6930 - val_accuracy: 0.8272 - lr: 5.2335e-05\n",
            "Epoch 162/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3728 - accuracy: 0.9408 - val_loss: 0.6915 - val_accuracy: 0.8292 - lr: 5.2335e-05\n",
            "Epoch 163/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3728 - accuracy: 0.9418 - val_loss: 0.6920 - val_accuracy: 0.8280 - lr: 4.7101e-05\n",
            "Epoch 164/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3731 - accuracy: 0.9393 - val_loss: 0.6925 - val_accuracy: 0.8280 - lr: 4.7101e-05\n",
            "Epoch 165/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3723 - accuracy: 0.9407 - val_loss: 0.6911 - val_accuracy: 0.8264 - lr: 4.7101e-05\n",
            "Epoch 166/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3732 - accuracy: 0.9418 - val_loss: 0.6914 - val_accuracy: 0.8280 - lr: 4.2391e-05\n",
            "Epoch 167/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3732 - accuracy: 0.9391 - val_loss: 0.6934 - val_accuracy: 0.8277 - lr: 4.2391e-05\n",
            "Epoch 168/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3736 - accuracy: 0.9393 - val_loss: 0.6910 - val_accuracy: 0.8255 - lr: 4.2391e-05\n",
            "Epoch 169/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3719 - accuracy: 0.9435 - val_loss: 0.6914 - val_accuracy: 0.8289 - lr: 3.8152e-05\n",
            "Epoch 170/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3724 - accuracy: 0.9416 - val_loss: 0.6915 - val_accuracy: 0.8304 - lr: 3.8152e-05\n",
            "Epoch 171/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3721 - accuracy: 0.9446 - val_loss: 0.6922 - val_accuracy: 0.8287 - lr: 3.8152e-05\n",
            "Epoch 172/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3723 - accuracy: 0.9389 - val_loss: 0.6933 - val_accuracy: 0.8272 - lr: 3.4337e-05\n",
            "Epoch 173/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3720 - accuracy: 0.9390 - val_loss: 0.6911 - val_accuracy: 0.8289 - lr: 3.4337e-05\n",
            "Epoch 174/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3726 - accuracy: 0.9399 - val_loss: 0.6917 - val_accuracy: 0.8289 - lr: 3.4337e-05\n",
            "Epoch 175/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3724 - accuracy: 0.9419 - val_loss: 0.6909 - val_accuracy: 0.8307 - lr: 3.0903e-05\n",
            "Epoch 176/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3718 - accuracy: 0.9389 - val_loss: 0.6919 - val_accuracy: 0.8309 - lr: 3.0903e-05\n",
            "Epoch 177/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3714 - accuracy: 0.9417 - val_loss: 0.6924 - val_accuracy: 0.8290 - lr: 3.0903e-05\n",
            "Epoch 178/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3728 - accuracy: 0.9435 - val_loss: 0.6918 - val_accuracy: 0.8295 - lr: 2.7813e-05\n",
            "Epoch 179/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3720 - accuracy: 0.9416 - val_loss: 0.6921 - val_accuracy: 0.8279 - lr: 2.7813e-05\n",
            "Epoch 180/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3724 - accuracy: 0.9428 - val_loss: 0.6920 - val_accuracy: 0.8295 - lr: 2.7813e-05\n",
            "Epoch 181/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3714 - accuracy: 0.9424 - val_loss: 0.6916 - val_accuracy: 0.8292 - lr: 2.5032e-05\n",
            "Epoch 182/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3712 - accuracy: 0.9425 - val_loss: 0.6919 - val_accuracy: 0.8284 - lr: 2.5032e-05\n",
            "Epoch 183/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3725 - accuracy: 0.9408 - val_loss: 0.6916 - val_accuracy: 0.8287 - lr: 2.5032e-05\n",
            "Epoch 184/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3716 - accuracy: 0.9394 - val_loss: 0.6920 - val_accuracy: 0.8300 - lr: 2.2528e-05\n",
            "Epoch 185/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3717 - accuracy: 0.9397 - val_loss: 0.6913 - val_accuracy: 0.8282 - lr: 2.2528e-05\n",
            "Epoch 186/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3718 - accuracy: 0.9394 - val_loss: 0.6918 - val_accuracy: 0.8304 - lr: 2.2528e-05\n",
            "Epoch 187/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3711 - accuracy: 0.9404 - val_loss: 0.6923 - val_accuracy: 0.8299 - lr: 2.0276e-05\n",
            "Epoch 188/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3716 - accuracy: 0.9421 - val_loss: 0.6915 - val_accuracy: 0.8325 - lr: 2.0276e-05\n",
            "Epoch 189/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3712 - accuracy: 0.9437 - val_loss: 0.6917 - val_accuracy: 0.8320 - lr: 2.0276e-05\n",
            "Epoch 190/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3722 - accuracy: 0.9408 - val_loss: 0.6913 - val_accuracy: 0.8314 - lr: 1.8248e-05\n",
            "Epoch 191/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3711 - accuracy: 0.9377 - val_loss: 0.6917 - val_accuracy: 0.8307 - lr: 1.8248e-05\n",
            "Epoch 192/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3709 - accuracy: 0.9427 - val_loss: 0.6915 - val_accuracy: 0.8310 - lr: 1.8248e-05\n",
            "Epoch 193/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3710 - accuracy: 0.9431 - val_loss: 0.6913 - val_accuracy: 0.8305 - lr: 1.6423e-05\n",
            "Epoch 194/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3712 - accuracy: 0.9419 - val_loss: 0.6908 - val_accuracy: 0.8294 - lr: 1.6423e-05\n",
            "Epoch 195/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3710 - accuracy: 0.9404 - val_loss: 0.6916 - val_accuracy: 0.8290 - lr: 1.6423e-05\n",
            "Epoch 196/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3712 - accuracy: 0.9432 - val_loss: 0.6916 - val_accuracy: 0.8284 - lr: 1.4781e-05\n",
            "Epoch 197/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3717 - accuracy: 0.9431 - val_loss: 0.6909 - val_accuracy: 0.8289 - lr: 1.4781e-05\n",
            "Epoch 198/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3723 - accuracy: 0.9418 - val_loss: 0.6910 - val_accuracy: 0.8300 - lr: 1.4781e-05\n",
            "Epoch 199/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3713 - accuracy: 0.9413 - val_loss: 0.6907 - val_accuracy: 0.8294 - lr: 1.3303e-05\n",
            "Epoch 200/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3712 - accuracy: 0.9411 - val_loss: 0.6914 - val_accuracy: 0.8302 - lr: 1.3303e-05\n",
            "Epoch 201/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3712 - accuracy: 0.9398 - val_loss: 0.6915 - val_accuracy: 0.8292 - lr: 1.3303e-05\n",
            "Epoch 202/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3707 - accuracy: 0.9441 - val_loss: 0.6918 - val_accuracy: 0.8295 - lr: 1.1973e-05\n",
            "Epoch 203/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3715 - accuracy: 0.9424 - val_loss: 0.6912 - val_accuracy: 0.8300 - lr: 1.1973e-05\n",
            "Epoch 204/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3705 - accuracy: 0.9427 - val_loss: 0.6909 - val_accuracy: 0.8300 - lr: 1.1973e-05\n",
            "Epoch 205/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3711 - accuracy: 0.9422 - val_loss: 0.6908 - val_accuracy: 0.8300 - lr: 1.0775e-05\n",
            "Epoch 206/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3710 - accuracy: 0.9411 - val_loss: 0.6910 - val_accuracy: 0.8302 - lr: 1.0775e-05\n",
            "Epoch 207/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3709 - accuracy: 0.9393 - val_loss: 0.6909 - val_accuracy: 0.8302 - lr: 1.0775e-05\n",
            "Epoch 208/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3715 - accuracy: 0.9433 - val_loss: 0.6906 - val_accuracy: 0.8299 - lr: 9.6977e-06\n",
            "Epoch 209/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3719 - accuracy: 0.9401 - val_loss: 0.6913 - val_accuracy: 0.8294 - lr: 9.6977e-06\n",
            "Epoch 210/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3721 - accuracy: 0.9423 - val_loss: 0.6906 - val_accuracy: 0.8302 - lr: 9.6977e-06\n",
            "Epoch 211/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3710 - accuracy: 0.9411 - val_loss: 0.6909 - val_accuracy: 0.8305 - lr: 9.6977e-06\n",
            "Epoch 212/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3713 - accuracy: 0.9408 - val_loss: 0.6906 - val_accuracy: 0.8300 - lr: 8.7280e-06\n",
            "Epoch 213/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.3716 - accuracy: 0.9412 - val_loss: 0.6907 - val_accuracy: 0.8315 - lr: 8.7280e-06\n",
            "Epoch 214/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3709 - accuracy: 0.9449 - val_loss: 0.6912 - val_accuracy: 0.8304 - lr: 8.7280e-06\n",
            "Epoch 215/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3709 - accuracy: 0.9427 - val_loss: 0.6911 - val_accuracy: 0.8314 - lr: 7.8552e-06\n",
            "Epoch 216/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3711 - accuracy: 0.9427 - val_loss: 0.6911 - val_accuracy: 0.8309 - lr: 7.8552e-06\n",
            "Epoch 217/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3715 - accuracy: 0.9420 - val_loss: 0.6910 - val_accuracy: 0.8310 - lr: 7.8552e-06\n",
            "Epoch 218/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.3706 - accuracy: 0.9437 - val_loss: 0.6910 - val_accuracy: 0.8309 - lr: 7.0697e-06\n",
            "Epoch 1/500\n",
            "270/270 [==============================] - 7s 25ms/step - loss: 1.5626 - accuracy: 0.4850 - val_loss: 1.5223 - val_accuracy: 0.4818 - lr: 1.0000e-05\n",
            "Epoch 2/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.5498 - accuracy: 0.4845 - val_loss: 1.5174 - val_accuracy: 0.4812 - lr: 1.0000e-05\n",
            "Epoch 3/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.5398 - accuracy: 0.4867 - val_loss: 1.5132 - val_accuracy: 0.4815 - lr: 1.0000e-05\n",
            "Epoch 4/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.5312 - accuracy: 0.4839 - val_loss: 1.5094 - val_accuracy: 0.4808 - lr: 1.0000e-05\n",
            "Epoch 5/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.5230 - accuracy: 0.4855 - val_loss: 1.5057 - val_accuracy: 0.4811 - lr: 1.0000e-05\n",
            "Epoch 6/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.5184 - accuracy: 0.4854 - val_loss: 1.5024 - val_accuracy: 0.4811 - lr: 1.0000e-05\n",
            "Epoch 7/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.5094 - accuracy: 0.4884 - val_loss: 1.4993 - val_accuracy: 0.4819 - lr: 1.0000e-05\n",
            "Epoch 8/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.5047 - accuracy: 0.4875 - val_loss: 1.4963 - val_accuracy: 0.4819 - lr: 1.0000e-05\n",
            "Epoch 9/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.4877 - accuracy: 0.4899 - val_loss: 1.4935 - val_accuracy: 0.4825 - lr: 1.0000e-05\n",
            "Epoch 10/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.4861 - accuracy: 0.4931 - val_loss: 1.4909 - val_accuracy: 0.4827 - lr: 1.0000e-05\n",
            "Epoch 11/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.4703 - accuracy: 0.4949 - val_loss: 1.4882 - val_accuracy: 0.4834 - lr: 1.0000e-05\n",
            "Epoch 12/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.4683 - accuracy: 0.4952 - val_loss: 1.4858 - val_accuracy: 0.4829 - lr: 1.0000e-05\n",
            "Epoch 13/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.4544 - accuracy: 0.4978 - val_loss: 1.4834 - val_accuracy: 0.4826 - lr: 1.0000e-05\n",
            "Epoch 14/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.4563 - accuracy: 0.4985 - val_loss: 1.4812 - val_accuracy: 0.4829 - lr: 1.0000e-05\n",
            "Epoch 15/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.4504 - accuracy: 0.4974 - val_loss: 1.4791 - val_accuracy: 0.4837 - lr: 1.0000e-05\n",
            "Epoch 16/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.4393 - accuracy: 0.4990 - val_loss: 1.4771 - val_accuracy: 0.4842 - lr: 1.0000e-05\n",
            "Epoch 17/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.4318 - accuracy: 0.5028 - val_loss: 1.4752 - val_accuracy: 0.4843 - lr: 1.0000e-05\n",
            "Epoch 18/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.4295 - accuracy: 0.4988 - val_loss: 1.4733 - val_accuracy: 0.4853 - lr: 1.0000e-05\n",
            "Epoch 19/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.4138 - accuracy: 0.5085 - val_loss: 1.4716 - val_accuracy: 0.4857 - lr: 1.0000e-05\n",
            "Epoch 20/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.4116 - accuracy: 0.5086 - val_loss: 1.4699 - val_accuracy: 0.4862 - lr: 1.0000e-05\n",
            "Epoch 21/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.4011 - accuracy: 0.5085 - val_loss: 1.4682 - val_accuracy: 0.4861 - lr: 1.0000e-05\n",
            "Epoch 22/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.3988 - accuracy: 0.5090 - val_loss: 1.4667 - val_accuracy: 0.4861 - lr: 1.0000e-05\n",
            "Epoch 23/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.3878 - accuracy: 0.5115 - val_loss: 1.4652 - val_accuracy: 0.4867 - lr: 1.0000e-05\n",
            "Epoch 24/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.3899 - accuracy: 0.5078 - val_loss: 1.4637 - val_accuracy: 0.4864 - lr: 1.0000e-05\n",
            "Epoch 25/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.3776 - accuracy: 0.5117 - val_loss: 1.4623 - val_accuracy: 0.4872 - lr: 1.0000e-05\n",
            "Epoch 26/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.3748 - accuracy: 0.5179 - val_loss: 1.4609 - val_accuracy: 0.4871 - lr: 1.0000e-05\n",
            "Epoch 27/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.3602 - accuracy: 0.5163 - val_loss: 1.4595 - val_accuracy: 0.4872 - lr: 1.0000e-05\n",
            "Epoch 28/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.3590 - accuracy: 0.5208 - val_loss: 1.4582 - val_accuracy: 0.4876 - lr: 1.0000e-05\n",
            "Epoch 29/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.3584 - accuracy: 0.5197 - val_loss: 1.4568 - val_accuracy: 0.4879 - lr: 1.0000e-05\n",
            "Epoch 30/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.3501 - accuracy: 0.5254 - val_loss: 1.4556 - val_accuracy: 0.4883 - lr: 1.0000e-05\n",
            "Epoch 31/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.3418 - accuracy: 0.5244 - val_loss: 1.4543 - val_accuracy: 0.4885 - lr: 1.0000e-05\n",
            "Epoch 32/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.3347 - accuracy: 0.5260 - val_loss: 1.4530 - val_accuracy: 0.4887 - lr: 1.0000e-05\n",
            "Epoch 33/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.3276 - accuracy: 0.5256 - val_loss: 1.4518 - val_accuracy: 0.4892 - lr: 1.0000e-05\n",
            "Epoch 34/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.3295 - accuracy: 0.5258 - val_loss: 1.4506 - val_accuracy: 0.4896 - lr: 1.0000e-05\n",
            "Epoch 35/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.3175 - accuracy: 0.5300 - val_loss: 1.4494 - val_accuracy: 0.4898 - lr: 1.0000e-05\n",
            "Epoch 36/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.3182 - accuracy: 0.5273 - val_loss: 1.4482 - val_accuracy: 0.4895 - lr: 1.0000e-05\n",
            "Epoch 37/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.3129 - accuracy: 0.5302 - val_loss: 1.4471 - val_accuracy: 0.4899 - lr: 1.0000e-05\n",
            "Epoch 38/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.3041 - accuracy: 0.5319 - val_loss: 1.4460 - val_accuracy: 0.4903 - lr: 1.0000e-05\n",
            "Epoch 39/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.2943 - accuracy: 0.5362 - val_loss: 1.4449 - val_accuracy: 0.4900 - lr: 1.0000e-05\n",
            "Epoch 40/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.2938 - accuracy: 0.5343 - val_loss: 1.4438 - val_accuracy: 0.4902 - lr: 1.0000e-05\n",
            "Epoch 41/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.2851 - accuracy: 0.5364 - val_loss: 1.4427 - val_accuracy: 0.4904 - lr: 1.0000e-05\n",
            "Epoch 42/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.2744 - accuracy: 0.5408 - val_loss: 1.4416 - val_accuracy: 0.4911 - lr: 1.0000e-05\n",
            "Epoch 43/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.2775 - accuracy: 0.5435 - val_loss: 1.4405 - val_accuracy: 0.4911 - lr: 1.0000e-05\n",
            "Epoch 44/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.2754 - accuracy: 0.5412 - val_loss: 1.4394 - val_accuracy: 0.4914 - lr: 1.0000e-05\n",
            "Epoch 45/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.2643 - accuracy: 0.5453 - val_loss: 1.4383 - val_accuracy: 0.4919 - lr: 1.0000e-05\n",
            "Epoch 46/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.2568 - accuracy: 0.5429 - val_loss: 1.4373 - val_accuracy: 0.4921 - lr: 1.0000e-05\n",
            "Epoch 47/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.2576 - accuracy: 0.5449 - val_loss: 1.4363 - val_accuracy: 0.4921 - lr: 1.0000e-05\n",
            "Epoch 48/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.2501 - accuracy: 0.5477 - val_loss: 1.4353 - val_accuracy: 0.4921 - lr: 1.0000e-05\n",
            "Epoch 49/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.2422 - accuracy: 0.5539 - val_loss: 1.4343 - val_accuracy: 0.4926 - lr: 1.0000e-05\n",
            "Epoch 50/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.2304 - accuracy: 0.5556 - val_loss: 1.4334 - val_accuracy: 0.4923 - lr: 1.0000e-05\n",
            "Epoch 51/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.2291 - accuracy: 0.5539 - val_loss: 1.4323 - val_accuracy: 0.4923 - lr: 1.0000e-05\n",
            "Epoch 52/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.2277 - accuracy: 0.5525 - val_loss: 1.4314 - val_accuracy: 0.4918 - lr: 1.0000e-05\n",
            "Epoch 53/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.2183 - accuracy: 0.5541 - val_loss: 1.4304 - val_accuracy: 0.4921 - lr: 1.0000e-05\n",
            "Epoch 54/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.2171 - accuracy: 0.5607 - val_loss: 1.4295 - val_accuracy: 0.4917 - lr: 1.0000e-05\n",
            "Epoch 55/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.2129 - accuracy: 0.5583 - val_loss: 1.4285 - val_accuracy: 0.4921 - lr: 1.0000e-05\n",
            "Epoch 56/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.2083 - accuracy: 0.5619 - val_loss: 1.4276 - val_accuracy: 0.4925 - lr: 1.0000e-05\n",
            "Epoch 57/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.2026 - accuracy: 0.5613 - val_loss: 1.4267 - val_accuracy: 0.4922 - lr: 1.0000e-05\n",
            "Epoch 58/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.1954 - accuracy: 0.5653 - val_loss: 1.4258 - val_accuracy: 0.4922 - lr: 1.0000e-05\n",
            "Epoch 59/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.1950 - accuracy: 0.5673 - val_loss: 1.4249 - val_accuracy: 0.4925 - lr: 1.0000e-05\n",
            "Epoch 60/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.1852 - accuracy: 0.5683 - val_loss: 1.4239 - val_accuracy: 0.4921 - lr: 1.0000e-05\n",
            "Epoch 61/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.1778 - accuracy: 0.5705 - val_loss: 1.4231 - val_accuracy: 0.4922 - lr: 1.0000e-05\n",
            "Epoch 62/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.1739 - accuracy: 0.5733 - val_loss: 1.4222 - val_accuracy: 0.4925 - lr: 1.0000e-05\n",
            "Epoch 63/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.1677 - accuracy: 0.5750 - val_loss: 1.4213 - val_accuracy: 0.4927 - lr: 1.0000e-05\n",
            "Epoch 64/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.1609 - accuracy: 0.5754 - val_loss: 1.4205 - val_accuracy: 0.4929 - lr: 1.0000e-05\n",
            "Epoch 65/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.1618 - accuracy: 0.5751 - val_loss: 1.4197 - val_accuracy: 0.4934 - lr: 1.0000e-05\n",
            "Epoch 66/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.1524 - accuracy: 0.5807 - val_loss: 1.4189 - val_accuracy: 0.4934 - lr: 1.0000e-05\n",
            "Epoch 67/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.1480 - accuracy: 0.5790 - val_loss: 1.4181 - val_accuracy: 0.4932 - lr: 1.0000e-05\n",
            "Epoch 68/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.1417 - accuracy: 0.5825 - val_loss: 1.4172 - val_accuracy: 0.4927 - lr: 1.0000e-05\n",
            "Epoch 69/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.1401 - accuracy: 0.5831 - val_loss: 1.4164 - val_accuracy: 0.4933 - lr: 1.0000e-05\n",
            "Epoch 70/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.1325 - accuracy: 0.5838 - val_loss: 1.4156 - val_accuracy: 0.4932 - lr: 1.0000e-05\n",
            "Epoch 71/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.1349 - accuracy: 0.5833 - val_loss: 1.4148 - val_accuracy: 0.4936 - lr: 1.0000e-05\n",
            "Epoch 72/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.1279 - accuracy: 0.5867 - val_loss: 1.4140 - val_accuracy: 0.4937 - lr: 1.0000e-05\n",
            "Epoch 73/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.1189 - accuracy: 0.5894 - val_loss: 1.4133 - val_accuracy: 0.4940 - lr: 1.0000e-05\n",
            "Epoch 74/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.1205 - accuracy: 0.5884 - val_loss: 1.4125 - val_accuracy: 0.4946 - lr: 1.0000e-05\n",
            "Epoch 75/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.1134 - accuracy: 0.5894 - val_loss: 1.4117 - val_accuracy: 0.4942 - lr: 1.0000e-05\n",
            "Epoch 76/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.1109 - accuracy: 0.5934 - val_loss: 1.4111 - val_accuracy: 0.4944 - lr: 1.0000e-05\n",
            "Epoch 77/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.1021 - accuracy: 0.5938 - val_loss: 1.4103 - val_accuracy: 0.4944 - lr: 1.0000e-05\n",
            "Epoch 78/500\n",
            "270/270 [==============================] - 6s 24ms/step - loss: 1.1033 - accuracy: 0.5953 - val_loss: 1.4095 - val_accuracy: 0.4937 - lr: 1.0000e-05\n",
            "Epoch 79/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.0972 - accuracy: 0.5975 - val_loss: 1.4088 - val_accuracy: 0.4937 - lr: 1.0000e-05\n",
            "Epoch 80/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.0804 - accuracy: 0.6012 - val_loss: 1.4081 - val_accuracy: 0.4936 - lr: 1.0000e-05\n",
            "Epoch 81/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.0814 - accuracy: 0.5992 - val_loss: 1.4074 - val_accuracy: 0.4938 - lr: 1.0000e-05\n",
            "Epoch 82/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.0712 - accuracy: 0.6069 - val_loss: 1.4067 - val_accuracy: 0.4941 - lr: 1.0000e-05\n",
            "Epoch 83/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.0746 - accuracy: 0.6070 - val_loss: 1.4060 - val_accuracy: 0.4946 - lr: 1.0000e-05\n",
            "Epoch 84/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.0746 - accuracy: 0.6079 - val_loss: 1.4053 - val_accuracy: 0.4948 - lr: 1.0000e-05\n",
            "Epoch 85/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.0613 - accuracy: 0.6096 - val_loss: 1.4046 - val_accuracy: 0.4952 - lr: 1.0000e-05\n",
            "Epoch 86/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.0617 - accuracy: 0.6104 - val_loss: 1.4040 - val_accuracy: 0.4948 - lr: 1.0000e-05\n",
            "Epoch 87/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.0562 - accuracy: 0.6129 - val_loss: 1.4034 - val_accuracy: 0.4948 - lr: 1.0000e-05\n",
            "Epoch 88/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.0514 - accuracy: 0.6142 - val_loss: 1.4027 - val_accuracy: 0.4946 - lr: 1.0000e-05\n",
            "Epoch 89/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.0458 - accuracy: 0.6156 - val_loss: 1.4021 - val_accuracy: 0.4948 - lr: 1.0000e-05\n",
            "Epoch 90/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.0407 - accuracy: 0.6133 - val_loss: 1.4014 - val_accuracy: 0.4949 - lr: 1.0000e-05\n",
            "Epoch 91/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.0383 - accuracy: 0.6199 - val_loss: 1.4008 - val_accuracy: 0.4948 - lr: 1.0000e-05\n",
            "Epoch 92/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.0377 - accuracy: 0.6208 - val_loss: 1.4001 - val_accuracy: 0.4951 - lr: 1.0000e-05\n",
            "Epoch 93/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.0336 - accuracy: 0.6223 - val_loss: 1.3995 - val_accuracy: 0.4957 - lr: 1.0000e-05\n",
            "Epoch 94/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.0302 - accuracy: 0.6194 - val_loss: 1.3988 - val_accuracy: 0.4961 - lr: 1.0000e-05\n",
            "Epoch 95/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.0200 - accuracy: 0.6244 - val_loss: 1.3982 - val_accuracy: 0.4968 - lr: 1.0000e-05\n",
            "Epoch 96/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.0191 - accuracy: 0.6291 - val_loss: 1.3976 - val_accuracy: 0.4963 - lr: 1.0000e-05\n",
            "Epoch 97/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.0123 - accuracy: 0.6265 - val_loss: 1.3970 - val_accuracy: 0.4972 - lr: 1.0000e-05\n",
            "Epoch 98/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.0155 - accuracy: 0.6292 - val_loss: 1.3963 - val_accuracy: 0.4974 - lr: 1.0000e-05\n",
            "Epoch 99/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 1.0046 - accuracy: 0.6308 - val_loss: 1.3957 - val_accuracy: 0.4975 - lr: 1.0000e-05\n",
            "Epoch 100/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.9976 - accuracy: 0.6373 - val_loss: 1.3951 - val_accuracy: 0.4980 - lr: 1.0000e-05\n",
            "Epoch 101/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.9962 - accuracy: 0.6369 - val_loss: 1.3946 - val_accuracy: 0.4983 - lr: 1.0000e-05\n",
            "Epoch 102/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.9929 - accuracy: 0.6379 - val_loss: 1.3940 - val_accuracy: 0.4986 - lr: 1.0000e-05\n",
            "Epoch 103/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.9877 - accuracy: 0.6389 - val_loss: 1.3934 - val_accuracy: 0.4980 - lr: 1.0000e-05\n",
            "Epoch 104/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.9870 - accuracy: 0.6421 - val_loss: 1.3929 - val_accuracy: 0.4991 - lr: 1.0000e-05\n",
            "Epoch 105/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.9761 - accuracy: 0.6431 - val_loss: 1.3923 - val_accuracy: 0.4991 - lr: 1.0000e-05\n",
            "Epoch 106/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.9751 - accuracy: 0.6438 - val_loss: 1.3918 - val_accuracy: 0.4988 - lr: 1.0000e-05\n",
            "Epoch 107/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.9745 - accuracy: 0.6452 - val_loss: 1.3912 - val_accuracy: 0.4990 - lr: 1.0000e-05\n",
            "Epoch 108/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.9689 - accuracy: 0.6440 - val_loss: 1.3907 - val_accuracy: 0.4998 - lr: 1.0000e-05\n",
            "Epoch 109/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.9674 - accuracy: 0.6463 - val_loss: 1.3901 - val_accuracy: 0.4999 - lr: 1.0000e-05\n",
            "Epoch 110/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.9575 - accuracy: 0.6498 - val_loss: 1.3896 - val_accuracy: 0.5001 - lr: 1.0000e-05\n",
            "Epoch 111/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.9602 - accuracy: 0.6487 - val_loss: 1.3891 - val_accuracy: 0.4995 - lr: 1.0000e-05\n",
            "Epoch 112/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.9545 - accuracy: 0.6516 - val_loss: 1.3886 - val_accuracy: 0.4998 - lr: 1.0000e-05\n",
            "Epoch 113/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.9479 - accuracy: 0.6592 - val_loss: 1.3881 - val_accuracy: 0.5006 - lr: 1.0000e-05\n",
            "Epoch 114/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.9397 - accuracy: 0.6571 - val_loss: 1.3875 - val_accuracy: 0.5002 - lr: 1.0000e-05\n",
            "Epoch 115/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.9366 - accuracy: 0.6593 - val_loss: 1.3870 - val_accuracy: 0.5006 - lr: 1.0000e-05\n",
            "Epoch 116/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.9407 - accuracy: 0.6551 - val_loss: 1.3864 - val_accuracy: 0.5002 - lr: 1.0000e-05\n",
            "Epoch 117/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.9329 - accuracy: 0.6607 - val_loss: 1.3860 - val_accuracy: 0.5005 - lr: 1.0000e-05\n",
            "Epoch 118/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.9283 - accuracy: 0.6628 - val_loss: 1.3854 - val_accuracy: 0.5006 - lr: 1.0000e-05\n",
            "Epoch 119/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.9302 - accuracy: 0.6628 - val_loss: 1.3850 - val_accuracy: 0.5006 - lr: 1.0000e-05\n",
            "Epoch 120/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.9204 - accuracy: 0.6684 - val_loss: 1.3845 - val_accuracy: 0.5012 - lr: 1.0000e-05\n",
            "Epoch 121/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.9155 - accuracy: 0.6682 - val_loss: 1.3841 - val_accuracy: 0.5009 - lr: 1.0000e-05\n",
            "Epoch 122/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.9172 - accuracy: 0.6672 - val_loss: 1.3835 - val_accuracy: 0.5012 - lr: 1.0000e-05\n",
            "Epoch 123/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.9045 - accuracy: 0.6736 - val_loss: 1.3830 - val_accuracy: 0.5012 - lr: 1.0000e-05\n",
            "Epoch 124/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.9072 - accuracy: 0.6732 - val_loss: 1.3826 - val_accuracy: 0.5010 - lr: 1.0000e-05\n",
            "Epoch 125/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.9051 - accuracy: 0.6721 - val_loss: 1.3822 - val_accuracy: 0.5013 - lr: 1.0000e-05\n",
            "Epoch 126/500\n",
            "270/270 [==============================] - 6s 24ms/step - loss: 0.8970 - accuracy: 0.6779 - val_loss: 1.3816 - val_accuracy: 0.5017 - lr: 1.0000e-05\n",
            "Epoch 127/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.8928 - accuracy: 0.6771 - val_loss: 1.3813 - val_accuracy: 0.5014 - lr: 1.0000e-05\n",
            "Epoch 128/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.8908 - accuracy: 0.6784 - val_loss: 1.3809 - val_accuracy: 0.5014 - lr: 1.0000e-05\n",
            "Epoch 129/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.8870 - accuracy: 0.6825 - val_loss: 1.3805 - val_accuracy: 0.5016 - lr: 1.0000e-05\n",
            "Epoch 130/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.8769 - accuracy: 0.6832 - val_loss: 1.3801 - val_accuracy: 0.5014 - lr: 1.0000e-05\n",
            "Epoch 131/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.8791 - accuracy: 0.6829 - val_loss: 1.3796 - val_accuracy: 0.5017 - lr: 1.0000e-05\n",
            "Epoch 132/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.8765 - accuracy: 0.6846 - val_loss: 1.3792 - val_accuracy: 0.5017 - lr: 1.0000e-05\n",
            "Epoch 133/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.8741 - accuracy: 0.6853 - val_loss: 1.3788 - val_accuracy: 0.5013 - lr: 1.0000e-05\n",
            "Epoch 134/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.8722 - accuracy: 0.6837 - val_loss: 1.3784 - val_accuracy: 0.5014 - lr: 1.0000e-05\n",
            "Epoch 135/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.8617 - accuracy: 0.6900 - val_loss: 1.3780 - val_accuracy: 0.5020 - lr: 1.0000e-05\n",
            "Epoch 136/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.8671 - accuracy: 0.6877 - val_loss: 1.3776 - val_accuracy: 0.5018 - lr: 1.0000e-05\n",
            "Epoch 137/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.8568 - accuracy: 0.6932 - val_loss: 1.3772 - val_accuracy: 0.5021 - lr: 1.0000e-05\n",
            "Epoch 138/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.8545 - accuracy: 0.6959 - val_loss: 1.3769 - val_accuracy: 0.5021 - lr: 1.0000e-05\n",
            "Epoch 139/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.8517 - accuracy: 0.6932 - val_loss: 1.3765 - val_accuracy: 0.5024 - lr: 1.0000e-05\n",
            "Epoch 140/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.8468 - accuracy: 0.6985 - val_loss: 1.3762 - val_accuracy: 0.5025 - lr: 1.0000e-05\n",
            "Epoch 141/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.8465 - accuracy: 0.6977 - val_loss: 1.3757 - val_accuracy: 0.5025 - lr: 1.0000e-05\n",
            "Epoch 142/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.8427 - accuracy: 0.7011 - val_loss: 1.3753 - val_accuracy: 0.5024 - lr: 1.0000e-05\n",
            "Epoch 143/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.8376 - accuracy: 0.7033 - val_loss: 1.3749 - val_accuracy: 0.5030 - lr: 1.0000e-05\n",
            "Epoch 144/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.8378 - accuracy: 0.7026 - val_loss: 1.3745 - val_accuracy: 0.5032 - lr: 1.0000e-05\n",
            "Epoch 145/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.8322 - accuracy: 0.7049 - val_loss: 1.3742 - val_accuracy: 0.5033 - lr: 1.0000e-05\n",
            "Epoch 146/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.8291 - accuracy: 0.7088 - val_loss: 1.3739 - val_accuracy: 0.5032 - lr: 1.0000e-05\n",
            "Epoch 147/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.8229 - accuracy: 0.7079 - val_loss: 1.3735 - val_accuracy: 0.5036 - lr: 1.0000e-05\n",
            "Epoch 148/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.8205 - accuracy: 0.7088 - val_loss: 1.3732 - val_accuracy: 0.5030 - lr: 1.0000e-05\n",
            "Epoch 149/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.8145 - accuracy: 0.7131 - val_loss: 1.3729 - val_accuracy: 0.5036 - lr: 1.0000e-05\n",
            "Epoch 150/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.8149 - accuracy: 0.7146 - val_loss: 1.3726 - val_accuracy: 0.5036 - lr: 1.0000e-05\n",
            "Epoch 151/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.8032 - accuracy: 0.7199 - val_loss: 1.3722 - val_accuracy: 0.5045 - lr: 1.0000e-05\n",
            "Epoch 152/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.8070 - accuracy: 0.7168 - val_loss: 1.3719 - val_accuracy: 0.5045 - lr: 1.0000e-05\n",
            "Epoch 153/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.8016 - accuracy: 0.7206 - val_loss: 1.3716 - val_accuracy: 0.5043 - lr: 1.0000e-05\n",
            "Epoch 154/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.8027 - accuracy: 0.7182 - val_loss: 1.3712 - val_accuracy: 0.5040 - lr: 1.0000e-05\n",
            "Epoch 155/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.7953 - accuracy: 0.7202 - val_loss: 1.3710 - val_accuracy: 0.5039 - lr: 1.0000e-05\n",
            "Epoch 156/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.7928 - accuracy: 0.7239 - val_loss: 1.3707 - val_accuracy: 0.5033 - lr: 1.0000e-05\n",
            "Epoch 157/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.7869 - accuracy: 0.7245 - val_loss: 1.3703 - val_accuracy: 0.5044 - lr: 1.0000e-05\n",
            "Epoch 158/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.7851 - accuracy: 0.7292 - val_loss: 1.3700 - val_accuracy: 0.5051 - lr: 1.0000e-05\n",
            "Epoch 159/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.7809 - accuracy: 0.7271 - val_loss: 1.3697 - val_accuracy: 0.5049 - lr: 1.0000e-05\n",
            "Epoch 160/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.7857 - accuracy: 0.7319 - val_loss: 1.3694 - val_accuracy: 0.5047 - lr: 1.0000e-05\n",
            "Epoch 161/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.7779 - accuracy: 0.7308 - val_loss: 1.3691 - val_accuracy: 0.5051 - lr: 1.0000e-05\n",
            "Epoch 162/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.7745 - accuracy: 0.7321 - val_loss: 1.3689 - val_accuracy: 0.5051 - lr: 1.0000e-05\n",
            "Epoch 163/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.7723 - accuracy: 0.7311 - val_loss: 1.3687 - val_accuracy: 0.5054 - lr: 1.0000e-05\n",
            "Epoch 164/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.7697 - accuracy: 0.7351 - val_loss: 1.3684 - val_accuracy: 0.5054 - lr: 1.0000e-05\n",
            "Epoch 165/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.7686 - accuracy: 0.7314 - val_loss: 1.3681 - val_accuracy: 0.5054 - lr: 1.0000e-05\n",
            "Epoch 166/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.7589 - accuracy: 0.7400 - val_loss: 1.3678 - val_accuracy: 0.5058 - lr: 1.0000e-05\n",
            "Epoch 167/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.7550 - accuracy: 0.7461 - val_loss: 1.3676 - val_accuracy: 0.5052 - lr: 1.0000e-05\n",
            "Epoch 168/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.7559 - accuracy: 0.7407 - val_loss: 1.3673 - val_accuracy: 0.5054 - lr: 1.0000e-05\n",
            "Epoch 169/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.7486 - accuracy: 0.7423 - val_loss: 1.3670 - val_accuracy: 0.5048 - lr: 1.0000e-05\n",
            "Epoch 170/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.7474 - accuracy: 0.7441 - val_loss: 1.3667 - val_accuracy: 0.5052 - lr: 1.0000e-05\n",
            "Epoch 171/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.7439 - accuracy: 0.7474 - val_loss: 1.3665 - val_accuracy: 0.5054 - lr: 1.0000e-05\n",
            "Epoch 172/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.7420 - accuracy: 0.7481 - val_loss: 1.3662 - val_accuracy: 0.5056 - lr: 1.0000e-05\n",
            "Epoch 173/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.7367 - accuracy: 0.7483 - val_loss: 1.3659 - val_accuracy: 0.5051 - lr: 1.0000e-05\n",
            "Epoch 174/500\n",
            "270/270 [==============================] - 6s 24ms/step - loss: 0.7394 - accuracy: 0.7450 - val_loss: 1.3657 - val_accuracy: 0.5054 - lr: 1.0000e-05\n",
            "Epoch 175/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.7341 - accuracy: 0.7508 - val_loss: 1.3654 - val_accuracy: 0.5056 - lr: 1.0000e-05\n",
            "Epoch 176/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.7294 - accuracy: 0.7500 - val_loss: 1.3652 - val_accuracy: 0.5059 - lr: 1.0000e-05\n",
            "Epoch 177/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.7327 - accuracy: 0.7484 - val_loss: 1.3650 - val_accuracy: 0.5058 - lr: 1.0000e-05\n",
            "Epoch 178/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.7224 - accuracy: 0.7591 - val_loss: 1.3649 - val_accuracy: 0.5055 - lr: 1.0000e-05\n",
            "Epoch 179/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.7211 - accuracy: 0.7573 - val_loss: 1.3646 - val_accuracy: 0.5058 - lr: 1.0000e-05\n",
            "Epoch 180/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.7183 - accuracy: 0.7573 - val_loss: 1.3643 - val_accuracy: 0.5060 - lr: 1.0000e-05\n",
            "Epoch 181/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.7193 - accuracy: 0.7610 - val_loss: 1.3642 - val_accuracy: 0.5063 - lr: 1.0000e-05\n",
            "Epoch 182/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.7146 - accuracy: 0.7610 - val_loss: 1.3640 - val_accuracy: 0.5059 - lr: 1.0000e-05\n",
            "Epoch 183/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.7127 - accuracy: 0.7650 - val_loss: 1.3637 - val_accuracy: 0.5056 - lr: 1.0000e-05\n",
            "Epoch 184/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.7093 - accuracy: 0.7623 - val_loss: 1.3634 - val_accuracy: 0.5059 - lr: 1.0000e-05\n",
            "Epoch 185/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.7075 - accuracy: 0.7635 - val_loss: 1.3632 - val_accuracy: 0.5064 - lr: 1.0000e-05\n",
            "Epoch 186/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.6986 - accuracy: 0.7706 - val_loss: 1.3630 - val_accuracy: 0.5066 - lr: 1.0000e-05\n",
            "Epoch 187/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.6969 - accuracy: 0.7692 - val_loss: 1.3627 - val_accuracy: 0.5066 - lr: 1.0000e-05\n",
            "Epoch 188/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.6961 - accuracy: 0.7729 - val_loss: 1.3626 - val_accuracy: 0.5064 - lr: 1.0000e-05\n",
            "Epoch 189/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.6896 - accuracy: 0.7709 - val_loss: 1.3624 - val_accuracy: 0.5063 - lr: 1.0000e-05\n",
            "Epoch 190/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.6922 - accuracy: 0.7728 - val_loss: 1.3622 - val_accuracy: 0.5066 - lr: 1.0000e-05\n",
            "Epoch 191/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.6863 - accuracy: 0.7761 - val_loss: 1.3619 - val_accuracy: 0.5068 - lr: 1.0000e-05\n",
            "Epoch 192/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.6846 - accuracy: 0.7748 - val_loss: 1.3618 - val_accuracy: 0.5070 - lr: 1.0000e-05\n",
            "Epoch 193/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.6838 - accuracy: 0.7753 - val_loss: 1.3616 - val_accuracy: 0.5073 - lr: 1.0000e-05\n",
            "Epoch 194/500\n",
            "270/270 [==============================] - 8s 28ms/step - loss: 0.6803 - accuracy: 0.7755 - val_loss: 1.3614 - val_accuracy: 0.5075 - lr: 1.0000e-05\n",
            "Epoch 195/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.6752 - accuracy: 0.7799 - val_loss: 1.3612 - val_accuracy: 0.5074 - lr: 1.0000e-05\n",
            "Epoch 196/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.6734 - accuracy: 0.7799 - val_loss: 1.3611 - val_accuracy: 0.5077 - lr: 1.0000e-05\n",
            "Epoch 197/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.6740 - accuracy: 0.7814 - val_loss: 1.3610 - val_accuracy: 0.5078 - lr: 1.0000e-05\n",
            "Epoch 198/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.6653 - accuracy: 0.7875 - val_loss: 1.3608 - val_accuracy: 0.5079 - lr: 1.0000e-05\n",
            "Epoch 199/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.6678 - accuracy: 0.7833 - val_loss: 1.3606 - val_accuracy: 0.5078 - lr: 1.0000e-05\n",
            "Epoch 200/500\n",
            "270/270 [==============================] - 6s 24ms/step - loss: 0.6612 - accuracy: 0.7863 - val_loss: 1.3605 - val_accuracy: 0.5079 - lr: 1.0000e-05\n",
            "Epoch 201/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.6592 - accuracy: 0.7848 - val_loss: 1.3603 - val_accuracy: 0.5085 - lr: 1.0000e-05\n",
            "Epoch 202/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.6567 - accuracy: 0.7870 - val_loss: 1.3601 - val_accuracy: 0.5085 - lr: 1.0000e-05\n",
            "Epoch 203/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.6572 - accuracy: 0.7866 - val_loss: 1.3599 - val_accuracy: 0.5087 - lr: 1.0000e-05\n",
            "Epoch 204/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.6536 - accuracy: 0.7895 - val_loss: 1.3597 - val_accuracy: 0.5089 - lr: 1.0000e-05\n",
            "Epoch 205/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.6478 - accuracy: 0.7937 - val_loss: 1.3596 - val_accuracy: 0.5090 - lr: 1.0000e-05\n",
            "Epoch 206/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.6459 - accuracy: 0.7945 - val_loss: 1.3595 - val_accuracy: 0.5094 - lr: 1.0000e-05\n",
            "Epoch 207/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.6477 - accuracy: 0.7936 - val_loss: 1.3594 - val_accuracy: 0.5091 - lr: 1.0000e-05\n",
            "Epoch 208/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.6473 - accuracy: 0.7949 - val_loss: 1.3592 - val_accuracy: 0.5093 - lr: 1.0000e-05\n",
            "Epoch 209/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.6397 - accuracy: 0.7989 - val_loss: 1.3591 - val_accuracy: 0.5090 - lr: 1.0000e-05\n",
            "Epoch 210/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.6361 - accuracy: 0.7980 - val_loss: 1.3590 - val_accuracy: 0.5091 - lr: 1.0000e-05\n",
            "Epoch 211/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.6327 - accuracy: 0.8032 - val_loss: 1.3589 - val_accuracy: 0.5086 - lr: 1.0000e-05\n",
            "Epoch 212/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.6338 - accuracy: 0.7986 - val_loss: 1.3588 - val_accuracy: 0.5091 - lr: 1.0000e-05\n",
            "Epoch 213/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.6260 - accuracy: 0.8069 - val_loss: 1.3587 - val_accuracy: 0.5094 - lr: 1.0000e-05\n",
            "Epoch 214/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.6210 - accuracy: 0.8084 - val_loss: 1.3586 - val_accuracy: 0.5097 - lr: 1.0000e-05\n",
            "Epoch 215/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.6244 - accuracy: 0.8062 - val_loss: 1.3585 - val_accuracy: 0.5100 - lr: 1.0000e-05\n",
            "Epoch 216/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.6245 - accuracy: 0.8058 - val_loss: 1.3583 - val_accuracy: 0.5102 - lr: 1.0000e-05\n",
            "Epoch 217/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.6201 - accuracy: 0.8098 - val_loss: 1.3582 - val_accuracy: 0.5104 - lr: 1.0000e-05\n",
            "Epoch 218/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.6182 - accuracy: 0.8080 - val_loss: 1.3581 - val_accuracy: 0.5116 - lr: 1.0000e-05\n",
            "Epoch 219/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.6168 - accuracy: 0.8048 - val_loss: 1.3579 - val_accuracy: 0.5120 - lr: 1.0000e-05\n",
            "Epoch 220/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.6131 - accuracy: 0.8123 - val_loss: 1.3578 - val_accuracy: 0.5116 - lr: 1.0000e-05\n",
            "Epoch 221/500\n",
            "270/270 [==============================] - 6s 24ms/step - loss: 0.6060 - accuracy: 0.8180 - val_loss: 1.3577 - val_accuracy: 0.5117 - lr: 1.0000e-05\n",
            "Epoch 222/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.6077 - accuracy: 0.8129 - val_loss: 1.3575 - val_accuracy: 0.5123 - lr: 1.0000e-05\n",
            "Epoch 223/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.6062 - accuracy: 0.8141 - val_loss: 1.3574 - val_accuracy: 0.5119 - lr: 1.0000e-05\n",
            "Epoch 224/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.6018 - accuracy: 0.8183 - val_loss: 1.3573 - val_accuracy: 0.5120 - lr: 1.0000e-05\n",
            "Epoch 225/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5994 - accuracy: 0.8220 - val_loss: 1.3572 - val_accuracy: 0.5123 - lr: 1.0000e-05\n",
            "Epoch 226/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5962 - accuracy: 0.8204 - val_loss: 1.3572 - val_accuracy: 0.5127 - lr: 1.0000e-05\n",
            "Epoch 227/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5949 - accuracy: 0.8199 - val_loss: 1.3571 - val_accuracy: 0.5131 - lr: 1.0000e-05\n",
            "Epoch 228/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5939 - accuracy: 0.8187 - val_loss: 1.3571 - val_accuracy: 0.5132 - lr: 1.0000e-05\n",
            "Epoch 229/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5901 - accuracy: 0.8235 - val_loss: 1.3570 - val_accuracy: 0.5129 - lr: 1.0000e-05\n",
            "Epoch 230/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5869 - accuracy: 0.8269 - val_loss: 1.3569 - val_accuracy: 0.5133 - lr: 1.0000e-05\n",
            "Epoch 231/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5858 - accuracy: 0.8244 - val_loss: 1.3568 - val_accuracy: 0.5131 - lr: 1.0000e-05\n",
            "Epoch 232/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5847 - accuracy: 0.8268 - val_loss: 1.3567 - val_accuracy: 0.5132 - lr: 1.0000e-05\n",
            "Epoch 233/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5789 - accuracy: 0.8273 - val_loss: 1.3567 - val_accuracy: 0.5133 - lr: 1.0000e-05\n",
            "Epoch 234/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5800 - accuracy: 0.8292 - val_loss: 1.3566 - val_accuracy: 0.5135 - lr: 1.0000e-05\n",
            "Epoch 235/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5758 - accuracy: 0.8281 - val_loss: 1.3565 - val_accuracy: 0.5136 - lr: 1.0000e-05\n",
            "Epoch 236/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5727 - accuracy: 0.8331 - val_loss: 1.3565 - val_accuracy: 0.5135 - lr: 1.0000e-05\n",
            "Epoch 237/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5744 - accuracy: 0.8323 - val_loss: 1.3565 - val_accuracy: 0.5133 - lr: 1.0000e-05\n",
            "Epoch 238/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5677 - accuracy: 0.8340 - val_loss: 1.3565 - val_accuracy: 0.5135 - lr: 1.0000e-05\n",
            "Epoch 239/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5653 - accuracy: 0.8368 - val_loss: 1.3564 - val_accuracy: 0.5136 - lr: 9.0000e-06\n",
            "Epoch 240/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5645 - accuracy: 0.8363 - val_loss: 1.3563 - val_accuracy: 0.5138 - lr: 9.0000e-06\n",
            "Epoch 241/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5629 - accuracy: 0.8380 - val_loss: 1.3564 - val_accuracy: 0.5135 - lr: 9.0000e-06\n",
            "Epoch 242/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5602 - accuracy: 0.8384 - val_loss: 1.3563 - val_accuracy: 0.5135 - lr: 9.0000e-06\n",
            "Epoch 243/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5600 - accuracy: 0.8392 - val_loss: 1.3563 - val_accuracy: 0.5133 - lr: 9.0000e-06\n",
            "Epoch 244/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5586 - accuracy: 0.8402 - val_loss: 1.3562 - val_accuracy: 0.5133 - lr: 8.1000e-06\n",
            "Epoch 245/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5546 - accuracy: 0.8415 - val_loss: 1.3562 - val_accuracy: 0.5129 - lr: 8.1000e-06\n",
            "Epoch 246/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5525 - accuracy: 0.8411 - val_loss: 1.3562 - val_accuracy: 0.5128 - lr: 8.1000e-06\n",
            "Epoch 247/500\n",
            "270/270 [==============================] - 8s 28ms/step - loss: 0.5509 - accuracy: 0.8429 - val_loss: 1.3562 - val_accuracy: 0.5133 - lr: 8.1000e-06\n",
            "Epoch 248/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5537 - accuracy: 0.8416 - val_loss: 1.3561 - val_accuracy: 0.5125 - lr: 7.2900e-06\n",
            "Epoch 249/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5532 - accuracy: 0.8442 - val_loss: 1.3561 - val_accuracy: 0.5132 - lr: 7.2900e-06\n",
            "Epoch 250/500\n",
            "270/270 [==============================] - 6s 24ms/step - loss: 0.5460 - accuracy: 0.8442 - val_loss: 1.3560 - val_accuracy: 0.5132 - lr: 7.2900e-06\n",
            "Epoch 251/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5503 - accuracy: 0.8420 - val_loss: 1.3559 - val_accuracy: 0.5132 - lr: 7.2900e-06\n",
            "Epoch 252/500\n",
            "270/270 [==============================] - 6s 24ms/step - loss: 0.5415 - accuracy: 0.8459 - val_loss: 1.3559 - val_accuracy: 0.5131 - lr: 7.2900e-06\n",
            "Epoch 253/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5442 - accuracy: 0.8464 - val_loss: 1.3559 - val_accuracy: 0.5131 - lr: 7.2900e-06\n",
            "Epoch 254/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5387 - accuracy: 0.8501 - val_loss: 1.3558 - val_accuracy: 0.5131 - lr: 7.2900e-06\n",
            "Epoch 255/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5425 - accuracy: 0.8449 - val_loss: 1.3557 - val_accuracy: 0.5132 - lr: 7.2900e-06\n",
            "Epoch 256/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5387 - accuracy: 0.8468 - val_loss: 1.3557 - val_accuracy: 0.5135 - lr: 7.2900e-06\n",
            "Epoch 257/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5382 - accuracy: 0.8497 - val_loss: 1.3557 - val_accuracy: 0.5133 - lr: 7.2900e-06\n",
            "Epoch 258/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5367 - accuracy: 0.8496 - val_loss: 1.3557 - val_accuracy: 0.5135 - lr: 7.2900e-06\n",
            "Epoch 259/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5355 - accuracy: 0.8513 - val_loss: 1.3556 - val_accuracy: 0.5136 - lr: 7.2900e-06\n",
            "Epoch 260/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5333 - accuracy: 0.8490 - val_loss: 1.3556 - val_accuracy: 0.5138 - lr: 6.5610e-06\n",
            "Epoch 261/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5304 - accuracy: 0.8504 - val_loss: 1.3556 - val_accuracy: 0.5139 - lr: 6.5610e-06\n",
            "Epoch 262/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5274 - accuracy: 0.8539 - val_loss: 1.3555 - val_accuracy: 0.5142 - lr: 6.5610e-06\n",
            "Epoch 263/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5274 - accuracy: 0.8533 - val_loss: 1.3556 - val_accuracy: 0.5142 - lr: 6.5610e-06\n",
            "Epoch 264/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5289 - accuracy: 0.8515 - val_loss: 1.3556 - val_accuracy: 0.5143 - lr: 6.5610e-06\n",
            "Epoch 265/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5246 - accuracy: 0.8562 - val_loss: 1.3555 - val_accuracy: 0.5146 - lr: 5.9049e-06\n",
            "Epoch 266/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5232 - accuracy: 0.8567 - val_loss: 1.3556 - val_accuracy: 0.5147 - lr: 5.9049e-06\n",
            "Epoch 267/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5275 - accuracy: 0.8521 - val_loss: 1.3555 - val_accuracy: 0.5148 - lr: 5.9049e-06\n",
            "Epoch 268/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5230 - accuracy: 0.8565 - val_loss: 1.3555 - val_accuracy: 0.5148 - lr: 5.3144e-06\n",
            "Epoch 269/500\n",
            "270/270 [==============================] - 6s 24ms/step - loss: 0.5221 - accuracy: 0.8544 - val_loss: 1.3555 - val_accuracy: 0.5151 - lr: 5.3144e-06\n",
            "Epoch 270/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5212 - accuracy: 0.8584 - val_loss: 1.3555 - val_accuracy: 0.5148 - lr: 5.3144e-06\n",
            "Epoch 271/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5171 - accuracy: 0.8610 - val_loss: 1.3555 - val_accuracy: 0.5150 - lr: 4.7830e-06\n",
            "Epoch 272/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5164 - accuracy: 0.8601 - val_loss: 1.3555 - val_accuracy: 0.5152 - lr: 4.7830e-06\n",
            "Epoch 273/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5139 - accuracy: 0.8600 - val_loss: 1.3555 - val_accuracy: 0.5151 - lr: 4.7830e-06\n",
            "Epoch 274/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5149 - accuracy: 0.8616 - val_loss: 1.3555 - val_accuracy: 0.5155 - lr: 4.3047e-06\n",
            "Epoch 275/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5145 - accuracy: 0.8596 - val_loss: 1.3555 - val_accuracy: 0.5159 - lr: 4.3047e-06\n",
            "Epoch 276/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5149 - accuracy: 0.8592 - val_loss: 1.3555 - val_accuracy: 0.5158 - lr: 4.3047e-06\n",
            "Epoch 277/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5116 - accuracy: 0.8636 - val_loss: 1.3555 - val_accuracy: 0.5154 - lr: 3.8742e-06\n",
            "Epoch 278/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5131 - accuracy: 0.8608 - val_loss: 1.3555 - val_accuracy: 0.5155 - lr: 3.8742e-06\n",
            "Epoch 279/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5132 - accuracy: 0.8605 - val_loss: 1.3555 - val_accuracy: 0.5154 - lr: 3.8742e-06\n",
            "Epoch 280/500\n",
            "270/270 [==============================] - 6s 24ms/step - loss: 0.5127 - accuracy: 0.8577 - val_loss: 1.3555 - val_accuracy: 0.5155 - lr: 3.4868e-06\n",
            "Epoch 281/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5072 - accuracy: 0.8606 - val_loss: 1.3555 - val_accuracy: 0.5155 - lr: 3.4868e-06\n",
            "Epoch 282/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5091 - accuracy: 0.8650 - val_loss: 1.3555 - val_accuracy: 0.5152 - lr: 3.4868e-06\n",
            "Epoch 283/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5078 - accuracy: 0.8629 - val_loss: 1.3555 - val_accuracy: 0.5151 - lr: 3.1381e-06\n",
            "Epoch 284/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5091 - accuracy: 0.8633 - val_loss: 1.3555 - val_accuracy: 0.5150 - lr: 3.1381e-06\n",
            "Epoch 285/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5056 - accuracy: 0.8651 - val_loss: 1.3555 - val_accuracy: 0.5151 - lr: 3.1381e-06\n",
            "Epoch 286/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5052 - accuracy: 0.8644 - val_loss: 1.3555 - val_accuracy: 0.5151 - lr: 2.8243e-06\n",
            "Epoch 287/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5077 - accuracy: 0.8622 - val_loss: 1.3555 - val_accuracy: 0.5154 - lr: 2.8243e-06\n",
            "Epoch 288/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5074 - accuracy: 0.8623 - val_loss: 1.3555 - val_accuracy: 0.5154 - lr: 2.8243e-06\n",
            "Epoch 289/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5038 - accuracy: 0.8645 - val_loss: 1.3555 - val_accuracy: 0.5152 - lr: 2.5419e-06\n",
            "Epoch 290/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5046 - accuracy: 0.8666 - val_loss: 1.3555 - val_accuracy: 0.5154 - lr: 2.5419e-06\n",
            "Epoch 291/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5026 - accuracy: 0.8690 - val_loss: 1.3555 - val_accuracy: 0.5154 - lr: 2.5419e-06\n",
            "Epoch 292/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5070 - accuracy: 0.8626 - val_loss: 1.3555 - val_accuracy: 0.5152 - lr: 2.2877e-06\n",
            "Epoch 293/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5029 - accuracy: 0.8686 - val_loss: 1.3555 - val_accuracy: 0.5154 - lr: 2.2877e-06\n",
            "Epoch 294/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5017 - accuracy: 0.8658 - val_loss: 1.3555 - val_accuracy: 0.5151 - lr: 2.2877e-06\n",
            "Epoch 295/500\n",
            "270/270 [==============================] - 6s 24ms/step - loss: 0.4999 - accuracy: 0.8653 - val_loss: 1.3555 - val_accuracy: 0.5155 - lr: 2.0589e-06\n",
            "Epoch 296/500\n",
            "270/270 [==============================] - 6s 24ms/step - loss: 0.5028 - accuracy: 0.8678 - val_loss: 1.3555 - val_accuracy: 0.5152 - lr: 2.0589e-06\n",
            "Epoch 297/500\n",
            "270/270 [==============================] - 6s 24ms/step - loss: 0.5005 - accuracy: 0.8677 - val_loss: 1.3555 - val_accuracy: 0.5151 - lr: 2.0589e-06\n",
            "Epoch 298/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5026 - accuracy: 0.8645 - val_loss: 1.3555 - val_accuracy: 0.5152 - lr: 1.8530e-06\n",
            "Epoch 299/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5005 - accuracy: 0.8663 - val_loss: 1.3555 - val_accuracy: 0.5152 - lr: 1.8530e-06\n",
            "Epoch 300/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.4999 - accuracy: 0.8689 - val_loss: 1.3555 - val_accuracy: 0.5152 - lr: 1.8530e-06\n",
            "Epoch 301/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5030 - accuracy: 0.8670 - val_loss: 1.3555 - val_accuracy: 0.5152 - lr: 1.6677e-06\n",
            "Epoch 302/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.4986 - accuracy: 0.8682 - val_loss: 1.3555 - val_accuracy: 0.5152 - lr: 1.6677e-06\n",
            "Epoch 303/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5038 - accuracy: 0.8661 - val_loss: 1.3555 - val_accuracy: 0.5152 - lr: 1.6677e-06\n",
            "Epoch 304/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.4976 - accuracy: 0.8691 - val_loss: 1.3556 - val_accuracy: 0.5155 - lr: 1.5009e-06\n",
            "Epoch 305/500\n",
            "270/270 [==============================] - 6s 23ms/step - loss: 0.5020 - accuracy: 0.8682 - val_loss: 1.3556 - val_accuracy: 0.5151 - lr: 1.5009e-06\n"
          ]
        }
      ],
      "source": [
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
        "early_stopper = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=30, mode='auto')\n",
        "model = create_simple_net()\n",
        "\n",
        "model.fit(\n",
        "    x=train_data_1,\n",
        "    y=train_labels_1,\n",
        "    epochs=500,\n",
        "    verbose=1,\n",
        "    validation_split=0.3,\n",
        "    batch_size=64,\n",
        "    callbacks=[lr_reducer, early_stopper]\n",
        ")\n",
        "\n",
        "for i in range(len(model.layers)-1):\n",
        "    model.layers[i].trainable = False\n",
        "    \n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.00001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
        "early_stopper = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=30, mode='auto')\n",
        "\n",
        "model.fit(\n",
        "    x=train_data,\n",
        "    y=train_labels,\n",
        "    epochs=500,\n",
        "    verbose=1,\n",
        "    validation_split=0.3,\n",
        "    batch_size=64,\n",
        "    callbacks=[lr_reducer, early_stopper]\n",
        ")\n",
        "\n",
        "model.save('/content/drive/MyDrive/파이썬스터디 프로젝트/labeling_fer_resnet_a1.h5')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_eval()"
      ],
      "metadata": {
        "id": "RdSjd0GuVS8k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cc4d8b3-cd77-405a-ad39-53b797c351aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5207578712733352"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7R1x771YQ-Vb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "83bc91af-6730-4057-cb08-a71400378ff4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "219/219 [==============================] - 11s 49ms/step - loss: 8.1805 - accuracy: 0.3767 - val_loss: 12.1101 - val_accuracy: 0.0095 - lr: 0.0010\n",
            "Epoch 2/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 5.2286 - accuracy: 0.5603 - val_loss: 3.2864 - val_accuracy: 0.2985 - lr: 0.0010\n",
            "Epoch 3/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 3.5063 - accuracy: 0.6630 - val_loss: 2.0741 - val_accuracy: 0.6173 - lr: 0.0010\n",
            "Epoch 4/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 2.6133 - accuracy: 0.7116 - val_loss: 2.5500 - val_accuracy: 0.6628 - lr: 0.0010\n",
            "Epoch 5/500\n",
            "219/219 [==============================] - 10s 45ms/step - loss: 1.9451 - accuracy: 0.7569 - val_loss: 2.0711 - val_accuracy: 0.7178 - lr: 0.0010\n",
            "Epoch 6/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 1.4978 - accuracy: 0.7910 - val_loss: 1.6800 - val_accuracy: 0.7432 - lr: 0.0010\n",
            "Epoch 7/500\n",
            "219/219 [==============================] - 10s 48ms/step - loss: 1.1147 - accuracy: 0.8262 - val_loss: 1.3787 - val_accuracy: 0.7523 - lr: 0.0010\n",
            "Epoch 8/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.8453 - accuracy: 0.8446 - val_loss: 1.0735 - val_accuracy: 0.7820 - lr: 0.0010\n",
            "Epoch 9/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.6722 - accuracy: 0.8681 - val_loss: 0.9078 - val_accuracy: 0.7928 - lr: 0.0010\n",
            "Epoch 10/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.5769 - accuracy: 0.8866 - val_loss: 0.8391 - val_accuracy: 0.8022 - lr: 0.0010\n",
            "Epoch 11/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.5403 - accuracy: 0.8988 - val_loss: 0.8094 - val_accuracy: 0.8072 - lr: 0.0010\n",
            "Epoch 12/500\n",
            "219/219 [==============================] - 10s 45ms/step - loss: 0.5224 - accuracy: 0.9002 - val_loss: 0.7928 - val_accuracy: 0.8058 - lr: 0.0010\n",
            "Epoch 13/500\n",
            "219/219 [==============================] - 10s 45ms/step - loss: 0.5068 - accuracy: 0.9065 - val_loss: 0.7907 - val_accuracy: 0.8035 - lr: 0.0010\n",
            "Epoch 14/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.5014 - accuracy: 0.9100 - val_loss: 0.7797 - val_accuracy: 0.8025 - lr: 0.0010\n",
            "Epoch 15/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.4958 - accuracy: 0.9076 - val_loss: 0.7770 - val_accuracy: 0.8097 - lr: 0.0010\n",
            "Epoch 16/500\n",
            "219/219 [==============================] - 10s 45ms/step - loss: 0.4889 - accuracy: 0.9084 - val_loss: 0.7715 - val_accuracy: 0.8033 - lr: 0.0010\n",
            "Epoch 17/500\n",
            "219/219 [==============================] - 10s 45ms/step - loss: 0.4835 - accuracy: 0.9133 - val_loss: 0.7673 - val_accuracy: 0.8070 - lr: 0.0010\n",
            "Epoch 18/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.4776 - accuracy: 0.9134 - val_loss: 0.7642 - val_accuracy: 0.8050 - lr: 0.0010\n",
            "Epoch 19/500\n",
            "219/219 [==============================] - 10s 45ms/step - loss: 0.4761 - accuracy: 0.9114 - val_loss: 0.7682 - val_accuracy: 0.8082 - lr: 0.0010\n",
            "Epoch 20/500\n",
            "219/219 [==============================] - 10s 47ms/step - loss: 0.4743 - accuracy: 0.9132 - val_loss: 0.7576 - val_accuracy: 0.8060 - lr: 0.0010\n",
            "Epoch 21/500\n",
            "219/219 [==============================] - 10s 45ms/step - loss: 0.4736 - accuracy: 0.9111 - val_loss: 0.7550 - val_accuracy: 0.8017 - lr: 0.0010\n",
            "Epoch 22/500\n",
            "219/219 [==============================] - 10s 45ms/step - loss: 0.4674 - accuracy: 0.9136 - val_loss: 0.7585 - val_accuracy: 0.8067 - lr: 0.0010\n",
            "Epoch 23/500\n",
            "219/219 [==============================] - 10s 45ms/step - loss: 0.4677 - accuracy: 0.9127 - val_loss: 0.7502 - val_accuracy: 0.8115 - lr: 0.0010\n",
            "Epoch 24/500\n",
            "219/219 [==============================] - 11s 49ms/step - loss: 0.4640 - accuracy: 0.9187 - val_loss: 0.7620 - val_accuracy: 0.8013 - lr: 0.0010\n",
            "Epoch 25/500\n",
            "219/219 [==============================] - 11s 48ms/step - loss: 0.4658 - accuracy: 0.9126 - val_loss: 0.7532 - val_accuracy: 0.8035 - lr: 0.0010\n",
            "Epoch 26/500\n",
            "219/219 [==============================] - 10s 46ms/step - loss: 0.4645 - accuracy: 0.9129 - val_loss: 0.7440 - val_accuracy: 0.8090 - lr: 0.0010\n",
            "Epoch 27/500\n",
            " 37/219 [====>.........................] - ETA: 7s - loss: 0.4685 - accuracy: 0.9160"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-a8a5dbfa7e9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlr_reducer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopper\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
        "early_stopper = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=30, mode='auto')\n",
        "model = create_simple_net()\n",
        "\n",
        "model.fit(\n",
        "    x=train_data_2,\n",
        "    y=train_labels_2,\n",
        "    epochs=500,\n",
        "    verbose=1,\n",
        "    validation_split=0.3,\n",
        "    batch_size=64,\n",
        "    callbacks=[lr_reducer, early_stopper]\n",
        ")\n",
        "\n",
        "for i in range(len(model.layers)-1):\n",
        "    model.layers[i].trainable = False\n",
        "    \n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.00001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
        "early_stopper = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=30, mode='auto')\n",
        "\n",
        "model.fit(\n",
        "    x=train_data,\n",
        "    y=train_labels,\n",
        "    epochs=500,\n",
        "    verbose=1,\n",
        "    validation_split=0.3,\n",
        "    batch_size=64,\n",
        "    callbacks=[lr_reducer, early_stopper]\n",
        ")\n",
        "\n",
        "model.save('/content/drive/MyDrive/파이썬스터디 프로젝트/labeling_fer_resnet_b1.h5')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_eval()"
      ],
      "metadata": {
        "id": "wMWqorK5RE5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpTO55j8Q-Vb"
      },
      "outputs": [],
      "source": [
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
        "early_stopper = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=30, mode='auto')\n",
        "model = create_simple_net()\n",
        "\n",
        "model.fit(\n",
        "    x=train_data_3,\n",
        "    y=train_labels_3,\n",
        "    epochs=500,\n",
        "    verbose=1,\n",
        "    validation_split=0.3,\n",
        "    batch_size=64,\n",
        "    callbacks=[lr_reducer, early_stopper]\n",
        ")\n",
        "\n",
        "for i in range(len(model.layers)-1):\n",
        "    model.layers[i].trainable = False\n",
        "    \n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.00001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
        "early_stopper = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=30, mode='auto')\n",
        "\n",
        "model.fit(\n",
        "    x=train_data,\n",
        "    y=train_labels,\n",
        "    epochs=500,\n",
        "    verbose=1,\n",
        "    validation_split=0.3,\n",
        "    batch_size=64,\n",
        "    callbacks=[lr_reducer, early_stopper]\n",
        ")\n",
        "\n",
        "model.save('/content/drive/MyDrive/파이썬스터디 프로젝트/labeling_fer_resnet_c1.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPJMkp-jQ-Vb"
      },
      "outputs": [],
      "source": [
        "model_eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Up2OMM3lm7RM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "uncertaintylabeling_fer2013+resnet.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}