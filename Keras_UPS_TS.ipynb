{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YianKim/2022_uncertainty_aware_semisupervise/blob/main/Keras_UPS_TS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ik7Qx5iO8lQ_"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import clone_model\n",
        "\n",
        "import PIL\n",
        "from PIL import Image\n",
        "\n",
        "import pickle\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "from keras.layers.core import Lambda\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import MaxPooling2D, AveragePooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras import optimizers\n",
        "from keras.callbacks import *\n",
        "from sklearn.metrics import *\n",
        "from keras.models import load_model\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as backend\n",
        "import math\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A3-ednCG7ZL"
      },
      "source": [
        "# cifar10 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAzGxvve8pgp"
      },
      "outputs": [],
      "source": [
        "cifar10 = keras.datasets.cifar10 \n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "train_images = train_images/255\n",
        "test_images = test_images/255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqUm92WTGW3p"
      },
      "outputs": [],
      "source": [
        "def dummy_labels(labels):\n",
        "  zero_labels = np.zeros([labels.shape[0], 10], np.int8)  \n",
        "  for i in range(labels.shape[0]):\n",
        "    zero_labels[i][labels[i]] = 1\n",
        "  return(zero_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFVXfHQNGtmq"
      },
      "outputs": [],
      "source": [
        "train_labels = dummy_labels(train_labels)\n",
        "test_labels = dummy_labels(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOqDJtXCG2ov"
      },
      "outputs": [],
      "source": [
        "# 1000 labeled, 49000 unlabeled\n",
        "indx = random.sample(range(train_labels.shape[0]),train_labels.shape[0])\n",
        "\n",
        "lbl_train_images = train_images[indx[:1000]]\n",
        "ubl_train_images = train_images[indx[1000:]]\n",
        "\n",
        "lbl_train_labels = train_labels[indx[:1000]]\n",
        "ubl_train_labels = train_labels[indx[1000:]]\n",
        "\n",
        "# valids1 =  train_images[indx[800:1000]]\n",
        "# valids2 =  train_labels[indx[800:1000]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqLLHcZLuZmP"
      },
      "source": [
        "# pseudo labeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hwy4DFT_BI1i"
      },
      "source": [
        "### Augmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KXwvnpUNxl9"
      },
      "outputs": [],
      "source": [
        "def basic_augmentation(imagearray):\n",
        "  image = Image.fromarray(imagearray)\n",
        "  tr1 = transforms.RandomHorizontalFlip()\n",
        "  tr2 = transforms.RandomRotation(10)\n",
        "  tr3 = transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2)\n",
        "  tr4 = transforms.RandomAffine(0, shear=10, scale=(0.8, 1.2))\n",
        "  image = tr1(tr2(tr3(tr4(image))))\n",
        "  return(np.array(image))\n",
        "\n",
        "def makeaugs(n, input):\n",
        "  augs = []\n",
        "  for j in range(n):\n",
        "    for i in input:\n",
        "      augs.append(basic_augmentation(np.array(i*255, np.uint8)))\n",
        "  return(np.array(augs)/255)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfW7b6sHNxl9"
      },
      "source": [
        "### Mixup Augmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpmSPXnnBLyT"
      },
      "outputs": [],
      "source": [
        "def sample_beta_distribution(size, concentration_0=0.3, concentration_1=0.3):\n",
        "    gamma_1_sample = tf.random.gamma(shape=[size], alpha=concentration_1)\n",
        "    gamma_2_sample = tf.random.gamma(shape=[size], alpha=concentration_0)\n",
        "    return gamma_1_sample / (gamma_1_sample + gamma_2_sample)\n",
        "\n",
        "def mixup (size, data, alpha = 0.2):\n",
        "  image, label = data\n",
        "  L = sample_beta_distribution(size, alpha, alpha)\n",
        "  XL = tf.reshape(L, (size, 1, 1, 1))\n",
        "  YL = tf.reshape(L, (size, 1))\n",
        "  IND1 = np.random.choice(len(label), size)\n",
        "  IND2 = np.random.choice(len(label), size)\n",
        "  newimage = XL*image[IND1] + (1-XL)*image[IND2]\n",
        "  newlabel = YL*label[IND1] + (1-YL)*label[IND2]\n",
        "  return (newimage, newlabel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewpemDXc8nEY"
      },
      "source": [
        "### 스케줄러"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKyn6Njs7vqa"
      },
      "outputs": [],
      "source": [
        "class SGDR(Callback):\n",
        "\n",
        "    def __init__(self, min_lr=0.0, max_lr=0.03, base_epochs=20, mul_epochs=2):\n",
        "        super(SGDR, self).__init__()\n",
        "\n",
        "        self.min_lr = min_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.base_epochs = base_epochs\n",
        "        self.mul_epochs = mul_epochs\n",
        "\n",
        "        self.cycles = 0.\n",
        "        self.cycle_iterations = 0.\n",
        "        self.trn_iterations = 0.\n",
        "\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self, new_min_lr=None, new_max_lr=None,\n",
        "               new_base_epochs=None, new_mul_epochs=None):\n",
        "        \"\"\"Resets cycle iterations.\"\"\"\n",
        "        \n",
        "        if new_min_lr != None:\n",
        "            self.min_lr = new_min_lr\n",
        "        if new_max_lr != None:\n",
        "            self.max_lr = new_max_lr\n",
        "        if new_base_epochs != None:\n",
        "            self.base_epochs = new_base_epochs\n",
        "        if new_mul_epochs != None:\n",
        "            self.mul_epochs = new_mul_epochs\n",
        "        self.cycles = 0.\n",
        "        self.cycle_iterations = 0.\n",
        "        \n",
        "    def sgdr(self):\n",
        "        \n",
        "        cycle_epochs = self.base_epochs * (self.mul_epochs ** self.cycles)\n",
        "        tide = ((self.cycles == 0) * 1) * (self.cycle_iterations*self.max_lr + (self.base_epochs - self.cycle_iterations)*self.min_lr) / self.base_epochs + ((self.cycles != 0) * 1)*(self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(np.pi * (self.cycle_iterations + 1) / cycle_epochs)))\n",
        "        return tide\n",
        "        \n",
        "    def on_train_begin(self, logs=None):\n",
        "        \n",
        "        if self.cycle_iterations == 0:\n",
        "            K.set_value(self.model.optimizer.lr, self.max_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.sgdr())\n",
        "            \n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        \n",
        "        logs = logs or {}\n",
        "        logs['lr'] = K.get_value(self.model.optimizer.lr)\n",
        "        \n",
        "        self.trn_iterations += 1\n",
        "        self.cycle_iterations += 1\n",
        "        if self.cycle_iterations >= self.base_epochs * (self.mul_epochs ** self.cycles):\n",
        "            self.cycles += 1\n",
        "            self.cycle_iterations = 0\n",
        "            K.set_value(self.model.optimizer.lr, self.max_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.sgdr())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjpoH_dl8qPt"
      },
      "source": [
        "### main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocpqRGhONxl_"
      },
      "outputs": [],
      "source": [
        "def PermaDropout(rate):\n",
        "    return Lambda(lambda x: K.dropout(x, level=rate))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iB5NxKqNxl_"
      },
      "outputs": [],
      "source": [
        "def create_cnn_1(n):\n",
        "  inputlayer = keras.Input(shape=(32, 32, 3))\n",
        "\n",
        "  conv1a = Conv2D(128*n, (3,3), padding = 'same')\n",
        "  bn1a = BatchNormalization()\n",
        "  conv1b = Conv2D(128*n, (3,3), padding = 'same')\n",
        "  bn1b = BatchNormalization()\n",
        "  conv1c = Conv2D(128*n, (3,3), padding = 'same')\n",
        "  bn1c = BatchNormalization()\n",
        "  pl1 = MaxPooling2D(2, 2)\n",
        "  MCdrop1 = PermaDropout(0.3)\n",
        "\n",
        "  conv2a = Conv2D(256*n, (3,3), padding = 'same')\n",
        "  bn2a = BatchNormalization()\n",
        "  conv2b = Conv2D(256*n, (3,3), padding = 'same')\n",
        "  bn2b = BatchNormalization()\n",
        "  conv2c = Conv2D(256*n, (3,3), padding = 'same')\n",
        "  bn2c = BatchNormalization()\n",
        "  pl2 = MaxPooling2D(2, 2)\n",
        "  MCdrop2 = PermaDropout(0.3)\n",
        "\n",
        "  conv3a = Conv2D(512*n, (3,3))\n",
        "  bn3a = BatchNormalization()\n",
        "  conv3b = Conv2D(256*n, (1,1))\n",
        "  bn3b = BatchNormalization()\n",
        "  conv3c = Conv2D(128*n, (1,1))\n",
        "  bn3c = BatchNormalization()\n",
        "  pl3 = AveragePooling2D(6, 2)\n",
        "\n",
        "  fc = Dense(10)\n",
        "  activ = keras.layers.LeakyReLU(0.1)\n",
        "\n",
        "  model = Sequential([\n",
        "                  inputlayer, \n",
        "                  conv1a, bn1a, activ,\n",
        "                  conv1b, bn1b, activ,\n",
        "                  conv1c, bn1c, activ,\n",
        "                  pl1, MCdrop1,\n",
        "\n",
        "                  conv2a, bn2a, activ,\n",
        "                  conv2b, bn2b, activ,\n",
        "                  conv2c, bn2c, activ,\n",
        "                  pl2, MCdrop2,\n",
        "\n",
        "                  conv3a, bn3a, activ,\n",
        "                  conv3b, bn3b, activ,\n",
        "                  conv3c, bn3c, activ,\n",
        "                  pl3, Flatten(),\n",
        "\n",
        "                  fc\n",
        "                  ])\n",
        "\n",
        "  opt = keras.optimizers.SGD(0.03, momentum = 0.9)\n",
        "\n",
        "  model.compile(\n",
        "    optimizer = opt,\n",
        "    loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        "  )\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NawxOOQQNxmA"
      },
      "outputs": [],
      "source": [
        "def create_cnn_2(n):\n",
        "  inputlayer = keras.Input(shape=(32, 32, 3))\n",
        "\n",
        "  conv1a = Conv2D(128*n, (3,3), padding = 'same')\n",
        "  bn1a = BatchNormalization()\n",
        "  conv1b = Conv2D(128*n, (3,3), padding = 'same')\n",
        "  bn1b = BatchNormalization()\n",
        "  pl1 = MaxPooling2D(2, 2)\n",
        "  MCdrop1 = PermaDropout(0.3)\n",
        "\n",
        "  conv2a = Conv2D(256*n, (3,3), padding = 'same')\n",
        "  bn2a = BatchNormalization()\n",
        "  conv2b = Conv2D(256*n, (3,3), padding = 'same')\n",
        "  bn2b = BatchNormalization()\n",
        "  pl2 = MaxPooling2D(2, 2)\n",
        "  MCdrop2 = PermaDropout(0.3)\n",
        "\n",
        "  conv3a = Conv2D(512*n, (3,3))\n",
        "  bn3a = BatchNormalization()\n",
        "  conv3b = Conv2D(256*n, (1,1))\n",
        "  bn3b = BatchNormalization()\n",
        "  pl3 = AveragePooling2D(6, 2)\n",
        "\n",
        "  fc = Dense(10)\n",
        "  activ = keras.layers.LeakyReLU(0.1)\n",
        "\n",
        "  model = Sequential([\n",
        "                  inputlayer, \n",
        "                  conv1a, bn1a, activ,\n",
        "                  conv1b, bn1b, activ,\n",
        "                  pl1, MCdrop1,\n",
        "\n",
        "                  conv2a, bn2a, activ,\n",
        "                  conv2b, bn2b, activ,\n",
        "                  pl2, MCdrop2,\n",
        "\n",
        "                  conv3a, bn3a, activ,\n",
        "                  conv3b, bn3b, activ,\n",
        "                  pl3, Flatten(),\n",
        "\n",
        "                  fc\n",
        "                  ])\n",
        "\n",
        "  opt = keras.optimizers.SGD(0.03, momentum = 0.9)\n",
        "\n",
        "  model.compile(\n",
        "    optimizer = opt,\n",
        "    loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        "  )\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eA0GY7KcNxmA"
      },
      "outputs": [],
      "source": [
        "def create_cnn_3(n):\n",
        "  inputlayer = keras.Input(shape=(32, 32, 3))\n",
        "\n",
        "  conv1a = Conv2D(128*n, (3,3), padding = 'same')\n",
        "  bn1a = BatchNormalization()\n",
        "  pl1 = MaxPooling2D(2, 2)\n",
        "  MCdrop1 = PermaDropout(0.3)\n",
        "\n",
        "  conv2a = Conv2D(256*n, (3,3), padding = 'same')\n",
        "  bn2a = BatchNormalization()\n",
        "  pl2 = MaxPooling2D(2, 2)\n",
        "  MCdrop2 = PermaDropout(0.3)\n",
        "\n",
        "  conv3a = Conv2D(512*n, (3,3))\n",
        "  bn3a = BatchNormalization()\n",
        "  pl3 = AveragePooling2D(6, 2)\n",
        "\n",
        "  fc = Dense(10)\n",
        "  activ = keras.layers.LeakyReLU(0.1)\n",
        "\n",
        "  model = Sequential([\n",
        "                  inputlayer, \n",
        "                  conv1a, bn1a, activ,\n",
        "                  pl1, MCdrop1,\n",
        "\n",
        "                  conv2a, bn2a, activ,\n",
        "                  pl2, MCdrop2,\n",
        "\n",
        "                  conv3a, bn3a, activ,\n",
        "                  pl3, Flatten(),\n",
        "\n",
        "                  fc\n",
        "                  ])\n",
        "\n",
        "  opt = keras.optimizers.SGD(0.03, momentum = 0.9)\n",
        "\n",
        "  model.compile(\n",
        "    optimizer = opt,\n",
        "    loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        "  )\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRCDNliwfudJ"
      },
      "outputs": [],
      "source": [
        "def fit_and_labeling_cnn_13(Epoch, Batch):\n",
        "\n",
        "  X = lbl_train_images\n",
        "  y = lbl_train_labels\n",
        "\n",
        "  lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
        "  early_stopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=30, mode='auto')\n",
        "  sgdr = SGDR(min_lr=0.0, max_lr=0.03/(1+itr*0.05), base_epochs=20) #스케줄러\n",
        "  \n",
        "  size = len(y) * 7\n",
        "  newimage, newlabel = mixup(size, (X, y))\n",
        "  augimage, auglabel = makeaugs(7, X), np.concatenate((y,y,y,y,y,y,y))\n",
        "  X = np.concatenate((X, newimage, augimage))\n",
        "  y = np.concatenate((y, newlabel, auglabel))\n",
        "  del newimage, newlabel\n",
        "\n",
        "  model.fit(\n",
        "      x=X,\n",
        "      y=y,\n",
        "      epochs=Epoch,\n",
        "      verbose=0,\n",
        "#       validation_data = (valids1, valids2),\n",
        "      batch_size=Batch,\n",
        "#       callbacks=[sgdr, early_stopper]\n",
        "      callbacks=[sgdr]\n",
        "  )\n",
        "  \n",
        "  model_test_eval(model, test_images, test_labels)\n",
        "  T = 1\n",
        "\n",
        "  for predsamples in range(10):\n",
        "    if predsamples == 0 :\n",
        "      predictions = np.array(tf.nn.softmax(model.predict(ubl_train_images)/T))\n",
        "      predictions = predictions.reshape((1,) + predictions.shape)\n",
        "    else:\n",
        "      pred = np.array(tf.nn.softmax(model.predict(ubl_train_images)/T))\n",
        "      pred = pred.reshape((1,) + pred.shape)\n",
        "      predictions = np.concatenate((predictions, pred))\n",
        "\n",
        "  return predictions\n",
        "\n",
        "def model_test_eval(model, test_images, test_labels):\n",
        "  T = 1\n",
        "  pred = np.array(tf.nn.softmax(model.predict(test_images)/T))\n",
        "  for i in range(1,10):\n",
        "    pred += np.array(tf.nn.softmax(model.predict(test_images)))\n",
        "  acc = (np.argmax(pred,axis=1) == np.argmax(test_labels,axis=1))*1\n",
        "  acc = sum(acc)/len(acc)\n",
        "  print(\"test set 성능 : \" + str(acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEBsyUAAg-W2"
      },
      "outputs": [],
      "source": [
        "def label_selecting():\n",
        "  K_conf = 0.8\n",
        "  K_uncert = 0.05\n",
        "\n",
        "  pseudo = np.argmax(np.mean(predictions, axis=0), axis=1)\n",
        "  conf = np.max(np.mean(predictions, axis=0), axis=1)\n",
        "  uncert = np.std(predictions, axis=0)\n",
        "  uncert = np.array([uncert[i][pseudo[i]] for i in range(len(pseudo))])\n",
        "\n",
        "  select_pseudo = (1*(conf > K_conf)) * (1*(uncert < K_uncert))\n",
        "\n",
        "  labels = []\n",
        "  for i in pseudo:\n",
        "    temp = [0,0,0,0,0,0,0,0,0,0]\n",
        "    temp[i] = 1\n",
        "    labels.append(temp)\n",
        "  pseudo = np.array(labels)\n",
        "#   pseudo = np.mean(predictions, axis=0)\n",
        "\n",
        "  lbl_idx = []\n",
        "  ubl_idx = []\n",
        "  k = 0\n",
        "  for i in select_pseudo:\n",
        "    if i == 1:\n",
        "      lbl_idx.append(k)\n",
        "    if i == 0:\n",
        "      ubl_idx.append(k)\n",
        "    k += 1\n",
        "\n",
        "    \n",
        "  ubl_append = ubl_train_images[lbl_idx]\n",
        "  pseudo_append = pseudo[lbl_idx]\n",
        "    \n",
        "  if itr < 5:\n",
        "      try: \n",
        "        numsamples = np.min(list(Counter(np.argmax(pseudo_append, axis=1)).values()))\n",
        "      except:\n",
        "        numsamples = 0\n",
        "      multlabel = np.argmax(pseudo_append, axis=1)\n",
        "      sufindx = random.sample(range(len(multlabel)), len(multlabel))\n",
        "\n",
        "      idxcounter = [0,0,0,0,0,0,0,0,0,0]\n",
        "      idxsample = []\n",
        "\n",
        "      for i in sufindx:\n",
        "        if idxcounter[multlabel[i]] < numsamples+25:\n",
        "          idxcounter[multlabel[i]] += 1\n",
        "          idxsample.append(i)\n",
        "      \n",
        "      image1 = np.concatenate((lbl_train_images, ubl_append[idxsample]))\n",
        "      label1 = np.concatenate((lbl_train_labels, pseudo_append[idxsample]))\n",
        "      image2 = np.concatenate((ubl_train_images[ubl_idx], np.delete(ubl_append, idxsample)))\n",
        "  \n",
        "  else:\n",
        "      image1 = np.concatenate((lbl_train_images, ubl_append))\n",
        "      label1 = np.concatenate((lbl_train_labels, pseudo_append))\n",
        "      image2 = ubl_train_images[ubl_idx]\n",
        "\n",
        "  return image1, label1, image2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7CPu64FflCu"
      },
      "source": [
        "### Knowledge Distillation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VCAjFi0fkt5"
      },
      "outputs": [],
      "source": [
        "class Distiller(keras.Model):\n",
        "    def __init__(self, student, teacher):\n",
        "        super(Distiller, self).__init__()\n",
        "        self.teacher = teacher\n",
        "        self.student = student\n",
        "\n",
        "    def compile(\n",
        "        self,\n",
        "        optimizer,\n",
        "        metrics,\n",
        "        student_loss_fn,\n",
        "        distillation_loss_fn,\n",
        "        alpha=0.1,\n",
        "        temperature=1,\n",
        "    ):\n",
        "\n",
        "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n",
        "        self.student_loss_fn = student_loss_fn\n",
        "        self.distillation_loss_fn = distillation_loss_fn\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # Unpack data\n",
        "        x, y = data\n",
        "\n",
        "        # Forward pass of teacher\n",
        "        teacher_predictions = self.teacher(x, training=False)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Forward pass of student\n",
        "            student_predictions = self.student(x, training=True)\n",
        "\n",
        "            # Compute losses\n",
        "            student_loss = self.student_loss_fn(y, student_predictions)\n",
        "            distillation_loss = self.distillation_loss_fn(\n",
        "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
        "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
        "            )\n",
        "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.student.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # Update the metrics configured in `compile()`.\n",
        "        self.compiled_metrics.update_state(y, student_predictions)\n",
        "\n",
        "        # Return a dict of performance\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update(\n",
        "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
        "        )\n",
        "        return results\n",
        "\n",
        "    def test_step(self, data):\n",
        "        # Unpack the data\n",
        "        x, y = data\n",
        "\n",
        "        # Compute predictions\n",
        "        y_prediction = self.student(x, training=False)\n",
        "\n",
        "        # Calculate the loss\n",
        "        student_loss = self.student_loss_fn(y, y_prediction)\n",
        "\n",
        "        # Update the metrics.\n",
        "        self.compiled_metrics.update_state(y, y_prediction)\n",
        "\n",
        "        # Return a dict of performance\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update({\"student_loss\": student_loss})\n",
        "        return results\n",
        "    \n",
        "    def call(self, inputs, *args, **kwargs):\n",
        "        return self.model(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSqcF7gpflfj"
      },
      "source": [
        "### 실행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7whKvK8czWv",
        "outputId": "a3813ac3-537f-483f-fe25-16b6398e6878"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counter({4: 115, 9: 112, 5: 106, 1: 103, 7: 102, 6: 101, 8: 97, 0: 93, 3: 92, 2: 79})\n",
            "**********itr 1 teacher evaluation**********\n",
            "layer 개수 : 26\n",
            "test set 성능 : 0.6094\n",
            "**********itr 1 student evaluation (before & after)**********\n",
            "test set 성능 : 0.0793\n",
            "test set 성능 : 0.619\n",
            "Counter({4: 485, 9: 482, 5: 476, 1: 473, 7: 472, 6: 471, 8: 467, 0: 463, 3: 462, 2: 424})\n",
            "**********itr 2 teacher evaluation**********\n",
            "layer 개수 : 26\n",
            "test set 성능 : 0.662\n",
            "**********itr 2 student evaluation (before & after)**********\n",
            "test set 성능 : 0.1\n",
            "test set 성능 : 0.6829\n",
            "Counter({9: 1015, 5: 1009, 1: 1006, 6: 1004, 4: 1003, 8: 1000, 0: 996, 3: 995, 7: 980, 2: 957})\n",
            "**********itr 3 teacher evaluation**********\n",
            "layer 개수 : 26\n",
            "test set 성능 : 0.7068\n",
            "**********itr 3 student evaluation (before & after)**********\n",
            "test set 성능 : 0.0997\n",
            "test set 성능 : 0.7159\n",
            "Counter({9: 1370, 5: 1364, 1: 1361, 4: 1358, 8: 1355, 0: 1351, 3: 1350, 7: 1335, 6: 1334, 2: 1312})\n",
            "**********itr 4 teacher evaluation**********\n",
            "layer 개수 : 20\n",
            "test set 성능 : 0.7145\n",
            "**********itr 4 student evaluation (before & after)**********\n",
            "test set 성능 : 0.0906\n",
            "test set 성능 : 0.7156\n",
            "Counter({9: 1527, 5: 1521, 8: 1512, 0: 1508, 3: 1507, 1: 1499, 7: 1492, 6: 1491, 4: 1490, 2: 1469})\n",
            "**********itr 5 teacher evaluation**********\n",
            "layer 개수 : 20\n",
            "test set 성능 : 0.7244\n",
            "**********itr 5 student evaluation (before & after)**********\n",
            "test set 성능 : 0.0999\n",
            "test set 성능 : 0.7145\n",
            "Counter({9: 1665, 5: 1655, 0: 1646, 3: 1645, 8: 1629, 4: 1628, 6: 1621, 7: 1616, 1: 1612, 2: 1585})\n",
            "**********itr 6 teacher evaluation**********\n",
            "layer 개수 : 20\n",
            "test set 성능 : 0.7281\n",
            "**********itr 6 student evaluation (before & after)**********\n",
            "test set 성능 : 0.1\n",
            "test set 성능 : 0.7252\n",
            "Counter({6: 1777, 3: 1776, 0: 1771, 9: 1769, 5: 1766, 7: 1754, 8: 1731, 1: 1726, 4: 1724, 2: 1715})\n",
            "**********itr 7 teacher evaluation**********\n",
            "layer 개수 : 20\n",
            "test set 성능 : 0.7314\n",
            "**********itr 7 student evaluation (before & after)**********\n",
            "test set 성능 : 0.1012\n",
            "test set 성능 : 0.7187\n",
            "Counter({5: 1947, 3: 1918, 0: 1867, 6: 1862, 7: 1859, 2: 1855, 1: 1834, 9: 1834, 8: 1820, 4: 1812})\n",
            "**********itr 8 teacher evaluation**********\n",
            "layer 개수 : 20\n",
            "test set 성능 : 0.7366\n",
            "**********itr 8 student evaluation (before & after)**********\n",
            "test set 성능 : 0.1\n",
            "test set 성능 : 0.7261\n",
            "Counter({3: 2100, 5: 2026, 0: 2014, 7: 1961, 2: 1922, 6: 1912, 1: 1911, 9: 1897, 4: 1890, 8: 1882})\n",
            "**********itr 9 teacher evaluation**********\n",
            "layer 개수 : 20\n",
            "test set 성능 : 0.7355\n",
            "**********itr 9 student evaluation (before & after)**********\n",
            "test set 성능 : 0.1\n",
            "test set 성능 : 0.7301\n",
            "Counter({3: 2231, 5: 2111, 0: 2074, 7: 2012, 2: 2001, 1: 1963, 6: 1962, 4: 1958, 8: 1943, 9: 1941})\n",
            "**********itr 10 teacher evaluation**********\n",
            "layer 개수 : 20\n",
            "test set 성능 : 0.7407\n",
            "**********itr 10 student evaluation (before & after)**********\n",
            "test set 성능 : 0.1\n",
            "test set 성능 : 0.7303\n",
            "Counter({3: 2300, 5: 2257, 0: 2132, 7: 2085, 2: 2069, 6: 2020, 4: 2010, 8: 1999, 1: 1998, 9: 1976})\n",
            "**********itr 11 teacher evaluation**********\n",
            "layer 개수 : 20\n",
            "test set 성능 : 0.7456\n",
            "**********itr 11 student evaluation (before & after)**********\n",
            "test set 성능 : 0.1\n",
            "test set 성능 : 0.7284\n",
            "Counter({3: 2358, 5: 2342, 0: 2206, 7: 2145, 2: 2143, 4: 2099, 6: 2054, 1: 2044, 9: 2030, 8: 2022})\n",
            "**********itr 12 teacher evaluation**********\n",
            "layer 개수 : 20\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "\n",
        "for itr in range(20):\n",
        "  if itr == 0:\n",
        "    model = create_cnn_1(1)\n",
        "  else:\n",
        "    if itr <= 2:\n",
        "        student = create_cnn_1(1/2)\n",
        "    elif itr > 2 & itr <= 5:\n",
        "        student = create_cnn_2(1/2)\n",
        "    elif itr > 5 & itr <= 9:\n",
        "        student = create_cnn_2(1/4)\n",
        "    elif itr > 9 & itr <= 13:\n",
        "        student = create_cnn_2(1/8)\n",
        "    elif itr > 13 & itr <= 16:\n",
        "        student = create_cnn_3(1/8)\n",
        "    elif itr > 16 & itr <= 19:\n",
        "        student = create_cnn_3(1/16)\n",
        "    model = student\n",
        "    print(\"**********itr \" + str(itr) + \" student evaluation (before & after)**********\")\n",
        "    model_test_eval(model, test_images, test_labels)\n",
        "    distiller = Distiller(student=student, teacher=teacher)\n",
        "    distiller.compile(\n",
        "        optimizer=keras.optimizers.Adam(),\n",
        "        metrics=[keras.metrics.CategoricalAccuracy()],\n",
        "        student_loss_fn=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "        distillation_loss_fn=keras.losses.KLDivergence(),\n",
        "        alpha=0.1,\n",
        "        temperature=1,\n",
        "    )\n",
        "\n",
        "    # Distill teacher to student\n",
        "    distiller.fit(lbl_train_images, lbl_train_labels, epochs=200, batch_size=128, verbose=0)\n",
        "    model = student\n",
        "    model_test_eval(model, test_images, test_labels)\n",
        "\n",
        "  print(Counter(np.argmax(lbl_train_labels, axis=1)))\n",
        "  print(\"**********itr \" + str(itr+1) + \" teacher evaluation**********\")\n",
        "  print(\"layer 개수 : \" + str(len(model.layers)))\n",
        "  predictions = fit_and_labeling_cnn_13(200, 128)\n",
        "  lbl_train_images, lbl_train_labels, ubl_train_images = label_selecting()\n",
        "  del predictions\n",
        "  teacher = model\n",
        "  gc.collect()\n",
        "\n",
        "    \n",
        "print(\"time :\", time.time() - start)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Keras_UPS_TS.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}