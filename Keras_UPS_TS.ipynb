{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YianKim/2022_uncertainty_aware_semisupervise/blob/main/Keras_UPS_TS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ik7Qx5iO8lQ_"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import clone_model\n",
        "\n",
        "import PIL\n",
        "from PIL import Image\n",
        "\n",
        "import pickle\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "from keras.layers.core import Lambda\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import MaxPooling2D, AveragePooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras import optimizers\n",
        "from keras.callbacks import *\n",
        "from sklearn.metrics import *\n",
        "from keras.models import load_model\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as backend\n",
        "import math\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A3-ednCG7ZL"
      },
      "source": [
        "# cifar10 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAzGxvve8pgp"
      },
      "outputs": [],
      "source": [
        "cifar10 = keras.datasets.cifar10 \n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "train_images = train_images/255\n",
        "test_images = test_images/255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqUm92WTGW3p"
      },
      "outputs": [],
      "source": [
        "def dummy_labels(labels):\n",
        "  zero_labels = np.zeros([labels.shape[0], 10], np.int8)  \n",
        "  for i in range(labels.shape[0]):\n",
        "    zero_labels[i][labels[i]] = 1\n",
        "  return(zero_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFVXfHQNGtmq"
      },
      "outputs": [],
      "source": [
        "train_labels = dummy_labels(train_labels)\n",
        "test_labels = dummy_labels(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOqDJtXCG2ov"
      },
      "outputs": [],
      "source": [
        "# 1000 labeled, 49000 unlabeled\n",
        "random.seed(10)\n",
        "indx = random.sample(range(train_labels.shape[0]),train_labels.shape[0])\n",
        "\n",
        "lbl_train_images = train_images[indx[:1000]]\n",
        "ubl_train_images = train_images[indx[1000:]]\n",
        "\n",
        "lbl_train_labels = train_labels[indx[:1000]]\n",
        "ubl_train_labels = train_labels[indx[1000:]]\n",
        "\n",
        "# valids1 =  train_images[indx[800:1000]]\n",
        "# valids2 =  train_labels[indx[800:1000]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqLLHcZLuZmP"
      },
      "source": [
        "# pseudo labeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hwy4DFT_BI1i"
      },
      "source": [
        "### Augmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnzb6BfzTZee"
      },
      "outputs": [],
      "source": [
        "def basic_augmentation(imagearray):\n",
        "  image = Image.fromarray(imagearray)\n",
        "  tr1 = transforms.RandomHorizontalFlip()\n",
        "  tr2 = transforms.RandomRotation(10)\n",
        "  tr3 = transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2)\n",
        "  tr4 = transforms.RandomAffine(0, shear=10, scale=(0.8, 1.2))\n",
        "  image = tr1(tr2(tr3(tr4(image))))\n",
        "  return(np.array(image))\n",
        "\n",
        "def makeaugs(n, input):\n",
        "  augs = []\n",
        "  for j in range(n):\n",
        "    for i in input:\n",
        "      augs.append(basic_augmentation(np.array(i*255, np.uint8)))\n",
        "  return(np.array(augs)/255)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bDTe4mDTZef"
      },
      "source": [
        "### Mixup Augmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpmSPXnnBLyT"
      },
      "outputs": [],
      "source": [
        "def sample_beta_distribution(size, concentration_0=0.3, concentration_1=0.3):\n",
        "    gamma_1_sample = tf.random.gamma(shape=[size], alpha=concentration_1)\n",
        "    gamma_2_sample = tf.random.gamma(shape=[size], alpha=concentration_0)\n",
        "    return gamma_1_sample / (gamma_1_sample + gamma_2_sample)\n",
        "\n",
        "def mixup (size, data, alpha = 0.2):\n",
        "  image, label = data\n",
        "  L = sample_beta_distribution(size, alpha, alpha)\n",
        "  XL = tf.reshape(L, (size, 1, 1, 1))\n",
        "  YL = tf.reshape(L, (size, 1))\n",
        "  IND1 = np.random.choice(len(label), size)\n",
        "  IND2 = np.random.choice(len(label), size)\n",
        "  newimage = XL*image[IND1] + (1-XL)*image[IND2]\n",
        "  newlabel = YL*label[IND1] + (1-YL)*label[IND2]\n",
        "  return (newimage, newlabel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewpemDXc8nEY"
      },
      "source": [
        "### 스케줄러"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKyn6Njs7vqa"
      },
      "outputs": [],
      "source": [
        "class SGDR(Callback):\n",
        "\n",
        "    def __init__(self, min_lr=0.0, max_lr=0.03, base_epochs=20, mul_epochs=2):\n",
        "        super(SGDR, self).__init__()\n",
        "\n",
        "        self.min_lr = min_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.base_epochs = base_epochs\n",
        "        self.mul_epochs = mul_epochs\n",
        "\n",
        "        self.cycles = 0.\n",
        "        self.cycle_iterations = 0.\n",
        "        self.trn_iterations = 0.\n",
        "\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self, new_min_lr=None, new_max_lr=None,\n",
        "               new_base_epochs=None, new_mul_epochs=None):\n",
        "        \"\"\"Resets cycle iterations.\"\"\"\n",
        "        \n",
        "        if new_min_lr != None:\n",
        "            self.min_lr = new_min_lr\n",
        "        if new_max_lr != None:\n",
        "            self.max_lr = new_max_lr\n",
        "        if new_base_epochs != None:\n",
        "            self.base_epochs = new_base_epochs\n",
        "        if new_mul_epochs != None:\n",
        "            self.mul_epochs = new_mul_epochs\n",
        "        self.cycles = 0.\n",
        "        self.cycle_iterations = 0.\n",
        "        \n",
        "    def sgdr(self):\n",
        "        \n",
        "        cycle_epochs = self.base_epochs * (self.mul_epochs ** self.cycles)\n",
        "        tide = ((self.cycles == 0) * 1) * (self.cycle_iterations*self.max_lr + (self.base_epochs - self.cycle_iterations)*self.min_lr) / self.base_epochs + ((self.cycles != 0) * 1)*(self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(np.pi * (self.cycle_iterations + 1) / cycle_epochs)))\n",
        "#         tide = (self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(np.pi * (self.cycle_iterations + 1) / cycle_epochs)))\n",
        "        return tide\n",
        "        \n",
        "    def on_train_begin(self, logs=None):\n",
        "        \n",
        "        if self.cycle_iterations == 0:\n",
        "            K.set_value(self.model.optimizer.lr, self.max_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.sgdr())\n",
        "            \n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        \n",
        "        logs = logs or {}\n",
        "        logs['lr'] = K.get_value(self.model.optimizer.lr)\n",
        "        \n",
        "        self.trn_iterations += 1\n",
        "        self.cycle_iterations += 1\n",
        "        if self.cycle_iterations >= self.base_epochs * (self.mul_epochs ** self.cycles):\n",
        "            self.cycles += 1\n",
        "            self.cycle_iterations = 0\n",
        "            K.set_value(self.model.optimizer.lr, self.max_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.sgdr())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjpoH_dl8qPt"
      },
      "source": [
        "### main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_glNNaEQTZeg"
      },
      "outputs": [],
      "source": [
        "def PermaDropout(rate):\n",
        "    return Lambda(lambda x: K.dropout(x, level=rate))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4WAOLCuTZeg"
      },
      "outputs": [],
      "source": [
        "def create_cnn_1(n):\n",
        "  inputlayer = keras.Input(shape=(32, 32, 3))\n",
        "\n",
        "  conv1a = Conv2D(128*n, (3,3), padding = 'same')\n",
        "  bn1a = BatchNormalization()\n",
        "  conv1b = Conv2D(128*n, (3,3), padding = 'same')\n",
        "  bn1b = BatchNormalization()\n",
        "  conv1c = Conv2D(128*n, (3,3), padding = 'same')\n",
        "  bn1c = BatchNormalization()\n",
        "  pl1 = MaxPooling2D(2, 2)\n",
        "  MCdrop1 = PermaDropout(0.5)\n",
        "\n",
        "  conv2a = Conv2D(256*n, (3,3), padding = 'same')\n",
        "  bn2a = BatchNormalization()\n",
        "  conv2b = Conv2D(256*n, (3,3), padding = 'same')\n",
        "  bn2b = BatchNormalization()\n",
        "  conv2c = Conv2D(256*n, (3,3), padding = 'same')\n",
        "  bn2c = BatchNormalization()\n",
        "  pl2 = MaxPooling2D(2, 2)\n",
        "  MCdrop2 = PermaDropout(0.5)\n",
        "\n",
        "  conv3a = Conv2D(512*n, (3,3))\n",
        "  bn3a = BatchNormalization()\n",
        "  conv3b = Conv2D(256*n, (1,1))\n",
        "  bn3b = BatchNormalization()\n",
        "  conv3c = Conv2D(128*n, (1,1))\n",
        "  bn3c = BatchNormalization()\n",
        "  pl3 = AveragePooling2D(6, 2)\n",
        "\n",
        "  fc = Dense(10)\n",
        "  activ = keras.layers.LeakyReLU(0.1)\n",
        "\n",
        "  model = Sequential([\n",
        "                  inputlayer, \n",
        "                  tfa.layers.WeightNormalization(conv1a), bn1a, activ,\n",
        "                  tfa.layers.WeightNormalization(conv1b), bn1b, activ,\n",
        "                  tfa.layers.WeightNormalization(conv1c), bn1c, activ,\n",
        "                  pl1, MCdrop1,\n",
        "\n",
        "                  tfa.layers.WeightNormalization(conv2a), bn2a, activ,\n",
        "                  tfa.layers.WeightNormalization(conv2b), bn2b, activ,\n",
        "                  tfa.layers.WeightNormalization(conv2c), bn2c, activ,\n",
        "                  pl2, MCdrop2,\n",
        "\n",
        "                  tfa.layers.WeightNormalization(conv3a), bn3a, activ,\n",
        "                  tfa.layers.WeightNormalization(conv3b), bn3b, activ,\n",
        "                  tfa.layers.WeightNormalization(conv3c), bn3c, activ,\n",
        "                  pl3, Flatten(),\n",
        "\n",
        "                  fc\n",
        "                  ])\n",
        "\n",
        "  opt = keras.optimizers.SGD(0.03, momentum = 0.9)\n",
        "\n",
        "  model.compile(\n",
        "    optimizer = opt,\n",
        "    loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        "  )\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVmxcptcTZeh"
      },
      "outputs": [],
      "source": [
        "def create_cnn_2(n):\n",
        "  inputlayer = keras.Input(shape=(32, 32, 3))\n",
        "\n",
        "  conv1a = Conv2D(128*n, (3,3), padding = 'same')\n",
        "  bn1a = BatchNormalization()\n",
        "  conv1b = Conv2D(128*n, (3,3), padding = 'same')\n",
        "  bn1b = BatchNormalization()\n",
        "  pl1 = MaxPooling2D(2, 2)\n",
        "  MCdrop1 = PermaDropout(0.5)\n",
        "\n",
        "  conv2a = Conv2D(256*n, (3,3), padding = 'same')\n",
        "  bn2a = BatchNormalization()\n",
        "  conv2b = Conv2D(256*n, (3,3), padding = 'same')\n",
        "  bn2b = BatchNormalization()\n",
        "  pl2 = MaxPooling2D(2, 2)\n",
        "  MCdrop2 = PermaDropout(0.5)\n",
        "\n",
        "  conv3a = Conv2D(512*n, (3,3))\n",
        "  bn3a = BatchNormalization()\n",
        "  conv3b = Conv2D(256*n, (1,1))\n",
        "  bn3b = BatchNormalization()\n",
        "  pl3 = AveragePooling2D(6, 2)\n",
        "\n",
        "  fc = Dense(10)\n",
        "  activ = keras.layers.LeakyReLU(0.1)\n",
        "\n",
        "  model = Sequential([\n",
        "                  inputlayer, \n",
        "                  tfa.layers.WeightNormalization(conv1a), bn1a, activ,\n",
        "                  tfa.layers.WeightNormalization(conv1b), bn1b, activ,\n",
        "                  pl1, MCdrop1,\n",
        "\n",
        "                  tfa.layers.WeightNormalization(conv2a), bn2a, activ,\n",
        "                  tfa.layers.WeightNormalization(conv2b), bn2b, activ,\n",
        "                  pl2, MCdrop2,\n",
        "\n",
        "                  tfa.layers.WeightNormalization(conv3a), bn3a, activ,\n",
        "                  tfa.layers.WeightNormalization(conv3b), bn3b, activ,\n",
        "                  pl3, Flatten(),\n",
        "\n",
        "                  fc\n",
        "                  ])\n",
        "  opt = keras.optimizers.SGD(0.03, momentum = 0.9)\n",
        "\n",
        "  model.compile(\n",
        "    optimizer = opt,\n",
        "    loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        "  )\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wenY76daTZeh"
      },
      "outputs": [],
      "source": [
        "def create_cnn_3(n):\n",
        "  inputlayer = keras.Input(shape=(32, 32, 3))\n",
        "\n",
        "  conv1a = Conv2D(128*n, (3,3), padding = 'same')\n",
        "  bn1a = BatchNormalization()\n",
        "  pl1 = MaxPooling2D(2, 2)\n",
        "  MCdrop1 = PermaDropout(0.5)\n",
        "\n",
        "  conv2a = Conv2D(256*n, (3,3), padding = 'same')\n",
        "  bn2a = BatchNormalization()\n",
        "  pl2 = MaxPooling2D(2, 2)\n",
        "  MCdrop2 = PermaDropout(0.5)\n",
        "\n",
        "  conv3a = Conv2D(512*n, (3,3))\n",
        "  bn3a = BatchNormalization()\n",
        "  pl3 = AveragePooling2D(6, 2)\n",
        "\n",
        "  fc = Dense(10)\n",
        "  activ = keras.layers.LeakyReLU(0.1)\n",
        "\n",
        "  model = Sequential([\n",
        "                  inputlayer, \n",
        "                  tfa.layers.WeightNormalization(conv1a), bn1a, activ,\n",
        "                  pl1, MCdrop1,\n",
        "\n",
        "                  tfa.layers.WeightNormalization(conv2a), bn2a, activ,\n",
        "                  pl2, MCdrop2,\n",
        "\n",
        "                  tfa.layers.WeightNormalization(conv3a), bn3a, activ,\n",
        "                  pl3, Flatten(),\n",
        "\n",
        "                  fc\n",
        "                  ])\n",
        "\n",
        "  opt = keras.optimizers.SGD(0.03, momentum = 0.9)\n",
        "\n",
        "  model.compile(\n",
        "    optimizer = opt,\n",
        "    loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        "  )\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRCDNliwfudJ"
      },
      "outputs": [],
      "source": [
        "def fit_and_labeling_cnn_13(Epoch, Batch):\n",
        "\n",
        "  X = lbl_train_images\n",
        "  y = lbl_train_labels\n",
        "\n",
        "  lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
        "  early_stopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=30, mode='auto')\n",
        "  sgdr = SGDR(min_lr=0.0, max_lr=0.03, base_epochs=20) #스케줄러\n",
        "  \n",
        "#   size = len(y) * 7\n",
        "#   newimage, newlabel = mixup(size, (X, y))\n",
        "#   augimage, auglabel = makeaugs(7, X), np.concatenate((y,y,y,y,y,y,y))\n",
        "#   X = np.concatenate((X, newimage, augimage))\n",
        "#   y = np.concatenate((y, newlabel, auglabel))\n",
        "#   del newimage, newlabel\n",
        "\n",
        "  model.fit(\n",
        "      x=X,\n",
        "      y=y,\n",
        "      epochs=Epoch,\n",
        "      verbose=0,\n",
        "#       validation_data = (valids1, valids2),\n",
        "      batch_size=Batch,\n",
        "#       callbacks=[sgdr, early_stopper]\n",
        "      callbacks=[sgdr]\n",
        "  )\n",
        "  \n",
        "  model_test_eval(model, test_images, test_labels)\n",
        "  T = 1\n",
        "\n",
        "  for predsamples in range(10):\n",
        "    if predsamples == 0 :\n",
        "      predictions = np.array(tf.nn.softmax(model.predict(ubl_train_images)/T))\n",
        "      predictions = predictions.reshape((1,) + predictions.shape)\n",
        "    else:\n",
        "      pred = np.array(tf.nn.softmax(model.predict(ubl_train_images)/T))\n",
        "      pred = pred.reshape((1,) + pred.shape)\n",
        "      predictions = np.concatenate((predictions, pred))\n",
        "\n",
        "  return predictions\n",
        "\n",
        "def model_test_eval(model, test_images, test_labels):\n",
        "  T = 1\n",
        "  pred = np.array(tf.nn.softmax(model.predict(test_images)/T))\n",
        "  for i in range(1,10):\n",
        "    pred += np.array(tf.nn.softmax(model.predict(test_images)))\n",
        "  acc = (np.argmax(pred,axis=1) == np.argmax(test_labels,axis=1))*1\n",
        "  acc = sum(acc)/len(acc)\n",
        "  print(\"test set 성능 : \" + str(acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEBsyUAAg-W2"
      },
      "outputs": [],
      "source": [
        "def label_selecting():\n",
        "  K_conf = 0.9\n",
        "  K_uncert = 0.05\n",
        "\n",
        "  pseudo = np.argmax(np.mean(predictions, axis=0), axis=1)\n",
        "  conf = np.max(np.mean(predictions, axis=0), axis=1)\n",
        "  uncert = np.std(predictions, axis=0)\n",
        "  uncert = np.array([uncert[i][pseudo[i]] for i in range(len(pseudo))])\n",
        "\n",
        "  select_pseudo = (1*(conf > K_conf)) * (1*(uncert < K_uncert))\n",
        "\n",
        "  labels = []\n",
        "  for i in pseudo:\n",
        "    temp = [0,0,0,0,0,0,0,0,0,0]\n",
        "    temp[i] = 1\n",
        "    labels.append(temp)\n",
        "  pseudo = np.array(labels)\n",
        "#   pseudo = np.mean(predictions, axis=0)\n",
        "\n",
        "  lbl_idx = []\n",
        "  ubl_idx = []\n",
        "  k = 0\n",
        "  for i in select_pseudo:\n",
        "    if i == 1:\n",
        "      lbl_idx.append(k)\n",
        "    if i == 0:\n",
        "      ubl_idx.append(k)\n",
        "    k += 1\n",
        "\n",
        "    \n",
        "  ubl_append = ubl_train_images[lbl_idx]\n",
        "  pseudo_append = pseudo[lbl_idx]\n",
        "    \n",
        "  if itr < 20: # 일시적 수정\n",
        "      try: \n",
        "        numsamples = np.min(list(Counter(np.argmax(pseudo_append, axis=1)).values()))\n",
        "      except:\n",
        "        numsamples = 0\n",
        "      multlabel = np.argmax(pseudo_append, axis=1)\n",
        "      sufindx = random.sample(range(len(multlabel)), len(multlabel))\n",
        "\n",
        "      idxcounter = [0,0,0,0,0,0,0,0,0,0]\n",
        "      idxsample = []\n",
        "\n",
        "      for i in sufindx:\n",
        "#         if idxcounter[multlabel[i]] < numsamples+25:\n",
        "        if idxcounter[multlabel[i]] < 250:\n",
        "          idxcounter[multlabel[i]] += 1\n",
        "          idxsample.append(i)\n",
        "      \n",
        "      image1 = np.concatenate((lbl_train_images, ubl_append[idxsample]))\n",
        "      label1 = np.concatenate((lbl_train_labels, pseudo_append[idxsample]))\n",
        "      image2 = np.concatenate((ubl_train_images[ubl_idx], ubl_append[np.delete(list(range(len(ubl_append))), idxsample)]))\n",
        "  \n",
        "  else:\n",
        "      image1 = np.concatenate((lbl_train_images, ubl_append))\n",
        "      label1 = np.concatenate((lbl_train_labels, pseudo_append))\n",
        "      image2 = ubl_train_images[ubl_idx]\n",
        "\n",
        "  return image1, label1, image2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7CPu64FflCu"
      },
      "source": [
        "### Knowledge Distillation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VCAjFi0fkt5"
      },
      "outputs": [],
      "source": [
        "class Distiller(keras.Model):\n",
        "    def __init__(self, student, teacher):\n",
        "        super(Distiller, self).__init__()\n",
        "        self.teacher = teacher\n",
        "        self.student = student\n",
        "\n",
        "    def compile(\n",
        "        self,\n",
        "        optimizer,\n",
        "        metrics,\n",
        "        student_loss_fn,\n",
        "        distillation_loss_fn,\n",
        "        alpha=0.1,\n",
        "        temperature=1,\n",
        "    ):\n",
        "\n",
        "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n",
        "        self.student_loss_fn = student_loss_fn\n",
        "        self.distillation_loss_fn = distillation_loss_fn\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # Unpack data\n",
        "        x, y = data\n",
        "\n",
        "        # Forward pass of teacher\n",
        "        teacher_predictions = self.teacher(x, training=False)\n",
        "        teacher_predictions += self.teacher(x, training=False)\n",
        "        teacher_predictions += self.teacher(x, training=False)\n",
        "        teacher_predictions = teacher_predictions/3\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Forward pass of student\n",
        "            student_predictions = self.student(x, training=True)\n",
        "\n",
        "            # Compute losses\n",
        "            student_loss = self.student_loss_fn(y, student_predictions)\n",
        "            distillation_loss = self.distillation_loss_fn(\n",
        "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
        "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
        "            )\n",
        "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.student.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # Update the metrics configured in `compile()`.\n",
        "        self.compiled_metrics.update_state(y, student_predictions)\n",
        "\n",
        "        # Return a dict of performance\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update(\n",
        "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
        "        )\n",
        "        return results\n",
        "\n",
        "    def test_step(self, data):\n",
        "        # Unpack the data\n",
        "        x, y = data\n",
        "\n",
        "        # Compute predictions\n",
        "        y_prediction = self.student(x, training=False)\n",
        "\n",
        "        # Calculate the loss\n",
        "        student_loss = self.student_loss_fn(y, y_prediction)\n",
        "\n",
        "        # Update the metrics.\n",
        "        self.compiled_metrics.update_state(y, y_prediction)\n",
        "\n",
        "        # Return a dict of performance\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update({\"student_loss\": student_loss})\n",
        "        return results\n",
        "    \n",
        "    def call(self, inputs, *args, **kwargs):\n",
        "        return self.model(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSqcF7gpflfj"
      },
      "source": [
        "### 실행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7whKvK8czWv",
        "scrolled": false,
        "outputId": "488ae652-6fa8-4093-d204-889b5964a731"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counter({8: 111, 9: 105, 1: 104, 0: 102, 2: 100, 7: 99, 3: 99, 6: 97, 4: 96, 5: 87})\n",
            "**********itr 1 teacher evaluation**********\n",
            "layer 개수 : 26\n",
            "test set 성능 : 0.5966\n",
            "time : 794.2165970802307\n",
            "**********itr 1 student evaluation (before & after)**********\n",
            "test set 성능 : 0.1\n",
            "test set 성능 : 0.6208\n",
            "Counter({8: 361, 9: 355, 1: 354, 0: 352, 2: 350, 7: 349, 3: 349, 6: 347, 4: 346, 5: 337})\n",
            "**********itr 2 teacher evaluation**********\n",
            "layer 개수 : 26\n",
            "test set 성능 : 0.6422\n",
            "time : 4521.169994592667\n",
            "**********itr 2 student evaluation (before & after)**********\n",
            "test set 성능 : 0.1115\n",
            "test set 성능 : 0.6496\n",
            "Counter({8: 611, 9: 605, 1: 604, 0: 602, 2: 600, 7: 599, 3: 599, 6: 597, 4: 596, 5: 587})\n",
            "**********itr 3 teacher evaluation**********\n",
            "layer 개수 : 26\n",
            "test set 성능 : 0.653\n",
            "time : 10601.529772043228\n",
            "**********itr 3 student evaluation (before & after)**********\n",
            "test set 성능 : 0.1086\n",
            "test set 성능 : 0.6431\n",
            "Counter({8: 861, 9: 855, 1: 854, 0: 852, 2: 850, 7: 849, 3: 849, 6: 847, 4: 846, 5: 837})\n",
            "**********itr 4 teacher evaluation**********\n",
            "layer 개수 : 26\n",
            "test set 성능 : 0.6614\n",
            "time : 19237.054450035095\n",
            "**********itr 4 student evaluation (before & after)**********\n",
            "test set 성능 : 0.0534\n",
            "test set 성능 : 0.654\n",
            "Counter({8: 1111, 9: 1105, 1: 1104, 0: 1102, 2: 1100, 7: 1099, 3: 1099, 6: 1097, 4: 1096, 5: 1087})\n",
            "**********itr 5 teacher evaluation**********\n",
            "layer 개수 : 26\n",
            "test set 성능 : 0.6656\n",
            "time : 30561.27812409401\n",
            "**********itr 5 student evaluation (before & after)**********\n",
            "test set 성능 : 0.0705\n",
            "test set 성능 : 0.647\n",
            "Counter({8: 1361, 9: 1355, 1: 1354, 0: 1352, 2: 1350, 7: 1349, 3: 1349, 6: 1347, 4: 1346, 5: 1337})\n",
            "**********itr 6 teacher evaluation**********\n",
            "layer 개수 : 26\n",
            "test set 성능 : 0.6631\n",
            "time : 44733.4925570488\n",
            "**********itr 6 student evaluation (before & after)**********\n",
            "test set 성능 : 0.1007\n",
            "test set 성능 : 0.61\n",
            "Counter({8: 1611, 9: 1605, 1: 1604, 0: 1602, 2: 1600, 7: 1599, 3: 1599, 6: 1597, 4: 1596, 5: 1587})\n",
            "**********itr 7 teacher evaluation**********\n",
            "layer 개수 : 20\n",
            "test set 성능 : 0.673\n",
            "time : 57524.78969502449\n",
            "**********itr 7 student evaluation (before & after)**********\n",
            "test set 성능 : 0.1187\n",
            "test set 성능 : 0.6691\n",
            "Counter({8: 1861, 9: 1855, 1: 1854, 0: 1852, 2: 1850, 7: 1849, 3: 1849, 6: 1847, 4: 1846, 5: 1837})\n",
            "**********itr 8 teacher evaluation**********\n",
            "layer 개수 : 20\n",
            "test set 성능 : 0.6699\n",
            "time : 72134.2438158989\n",
            "**********itr 8 student evaluation (before & after)**********\n",
            "test set 성능 : 0.1071\n",
            "test set 성능 : 0.671\n",
            "Counter({8: 2111, 9: 2105, 1: 2104, 0: 2102, 2: 2100, 7: 2099, 3: 2099, 6: 2097, 4: 2096, 5: 2087})\n",
            "**********itr 9 teacher evaluation**********\n",
            "layer 개수 : 20\n",
            "test set 성능 : 0.6783\n",
            "time : 89054.62065052986\n",
            "**********itr 9 student evaluation (before & after)**********\n",
            "test set 성능 : 0.0894\n",
            "test set 성능 : 0.6675\n",
            "Counter({8: 2361, 9: 2355, 1: 2354, 0: 2352, 2: 2350, 7: 2349, 3: 2349, 6: 2347, 4: 2346, 5: 2337})\n",
            "**********itr 10 teacher evaluation**********\n",
            "layer 개수 : 20\n",
            "test set 성능 : 0.6767\n",
            "time : 105797.40094971657\n",
            "**********itr 10 student evaluation (before & after)**********\n",
            "test set 성능 : 0.106\n",
            "test set 성능 : 0.6621\n",
            "Counter({8: 2611, 9: 2605, 1: 2604, 0: 2602, 2: 2600, 7: 2599, 3: 2599, 6: 2597, 4: 2596, 5: 2587})\n",
            "**********itr 11 teacher evaluation**********\n",
            "layer 개수 : 20\n",
            "test set 성능 : 0.6765\n",
            "time : 124945.53453230858\n",
            "**********itr 11 student evaluation (before & after)**********\n",
            "test set 성능 : 0.1015\n",
            "test set 성능 : 0.6706\n",
            "Counter({8: 2861, 9: 2855, 1: 2854, 0: 2852, 2: 2850, 7: 2849, 3: 2849, 6: 2847, 4: 2846, 5: 2837})\n",
            "**********itr 12 teacher evaluation**********\n",
            "layer 개수 : 20\n",
            "test set 성능 : 0.679\n",
            "time : 145149.66326451302\n",
            "**********itr 12 student evaluation (before & after)**********\n",
            "test set 성능 : 0.1001\n",
            "test set 성능 : 0.6763\n",
            "Counter({8: 3111, 9: 3105, 1: 3104, 0: 3102, 2: 3100, 7: 3099, 3: 3099, 6: 3097, 4: 3096, 5: 3087})\n",
            "**********itr 13 teacher evaluation**********\n",
            "layer 개수 : 20\n",
            "test set 성능 : 0.6856\n",
            "time : 167294.82964873314\n",
            "**********itr 13 student evaluation (before & after)**********\n",
            "test set 성능 : 0.0899\n",
            "test set 성능 : 0.6766\n",
            "Counter({8: 3361, 9: 3355, 1: 3354, 0: 3352, 2: 3350, 7: 3349, 3: 3349, 6: 3347, 4: 3346, 5: 3337})\n",
            "**********itr 14 teacher evaluation**********\n",
            "layer 개수 : 20\n",
            "test set 성능 : 0.6842\n",
            "time : 191089.09907460213\n",
            "**********itr 14 student evaluation (before & after)**********\n",
            "test set 성능 : 0.1134\n",
            "test set 성능 : 0.6703\n",
            "Counter({8: 3611, 9: 3605, 1: 3604, 0: 3602, 7: 3599, 3: 3599, 6: 3597, 4: 3596, 2: 3589, 5: 3587})\n",
            "**********itr 15 teacher evaluation**********\n",
            "layer 개수 : 14\n",
            "test set 성능 : 0.679\n",
            "time : 209128.97786808014\n",
            "**********itr 15 student evaluation (before & after)**********\n",
            "test set 성능 : 0.0702\n",
            "test set 성능 : 0.6702\n",
            "Counter({8: 3861, 9: 3855, 1: 3854, 3: 3849, 6: 3847, 5: 3837, 0: 3829, 7: 3828, 2: 3780, 4: 3759})\n",
            "**********itr 16 teacher evaluation**********\n",
            "layer 개수 : 14\n",
            "test set 성능 : 0.6773\n",
            "time : 227660.30906033516\n",
            "**********itr 16 student evaluation (before & after)**********\n",
            "test set 성능 : 0.1002\n",
            "test set 성능 : 0.6731\n",
            "Counter({8: 4111, 9: 4105, 1: 4104, 6: 4097, 5: 4072, 3: 4053, 2: 3927, 7: 3910, 0: 3909, 4: 3824})\n",
            "**********itr 17 teacher evaluation**********\n",
            "layer 개수 : 14\n",
            "test set 성능 : 0.6825\n",
            "time : 247277.44523000717\n",
            "**********itr 17 student evaluation (before & after)**********\n",
            "test set 성능 : 0.1001\n",
            "test set 성능 : 0.658\n",
            "Counter({8: 4361, 1: 4354, 6: 4347, 9: 4249, 3: 4188, 5: 4122, 0: 3977, 2: 3975, 7: 3964, 4: 3942})\n",
            "**********itr 18 teacher evaluation**********\n",
            "layer 개수 : 14\n",
            "test set 성능 : 0.6778\n",
            "time : 267990.2713048458\n",
            "**********itr 18 student evaluation (before & after)**********\n",
            "test set 성능 : 0.1\n",
            "test set 성능 : 0.665\n",
            "Counter({8: 4481, 1: 4403, 6: 4387, 9: 4267, 3: 4221, 5: 4171, 2: 4016, 0: 4003, 7: 3983, 4: 3952})\n",
            "**********itr 19 teacher evaluation**********\n",
            "layer 개수 : 14\n",
            "test set 성능 : 0.6777\n",
            "time : 288584.30953741074\n",
            "**********itr 19 student evaluation (before & after)**********\n",
            "test set 성능 : 0.1001\n",
            "test set 성능 : 0.6704\n",
            "Counter({8: 4535, 1: 4447, 6: 4446, 9: 4300, 3: 4276, 5: 4202, 2: 4024, 0: 4016, 7: 3988, 4: 3988})\n",
            "**********itr 20 teacher evaluation**********\n",
            "layer 개수 : 14\n",
            "test set 성능 : 0.6772\n",
            "time : 307942.38812446594\n",
            "time : 307942.38812446594\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "\n",
        "for itr in range(20):\n",
        "  if itr == 0:\n",
        "    model = create_cnn_1(1)\n",
        "    depth = len(model.layers)\n",
        "  else:\n",
        "    if itr <= 2:\n",
        "        student = create_cnn_1(1)\n",
        "    elif itr <= 5:\n",
        "        student = create_cnn_1(1/2)\n",
        "    elif itr <= 9:\n",
        "        student = create_cnn_2(1)\n",
        "    elif itr <= 13:\n",
        "        student = create_cnn_2(1/2)\n",
        "    elif itr <= 16:\n",
        "        student = create_cnn_3(1)\n",
        "    elif itr <= 19:\n",
        "        student = create_cnn_3(1/2)\n",
        "    print(\"**********itr \" + str(itr) + \" student evaluation (before & after)**********\")\n",
        "    model_test_eval(student, test_images, test_labels)\n",
        "    distiller = Distiller(student=student, teacher=model)\n",
        "    distiller.compile(\n",
        "        optimizer=keras.optimizers.Adam(),\n",
        "        metrics=[keras.metrics.CategoricalAccuracy()],\n",
        "        student_loss_fn=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "        distillation_loss_fn=keras.losses.KLDivergence(),\n",
        "        alpha=0.1,\n",
        "        temperature=1,\n",
        "    )\n",
        "\n",
        "    # Distill teacher to student\n",
        "    distiller.fit(lbl_train_images, lbl_train_labels, epochs=500,\n",
        "                  batch_size=64, verbose=0)\n",
        "    model_test_eval(student, test_images, test_labels)\n",
        "    depth = 0\n",
        "    depth += len(student.layers)\n",
        "    model = student\n",
        "\n",
        "  print(Counter(np.argmax(lbl_train_labels, axis=1)))\n",
        "  print(\"**********itr \" + str(itr+1) + \" teacher evaluation**********\")\n",
        "  print(\"layer 개수 : \" + str(depth))\n",
        "  predictions = fit_and_labeling_cnn_13(1000, 64)\n",
        "  lbl_train_images, lbl_train_labels, ubl_train_images = label_selecting()\n",
        "  del predictions\n",
        "  gc.collect()\n",
        "  print(\"time :\", time.time() - start)\n",
        "\n",
        "print(\"time :\", time.time() - start)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Keras_UPS_TS.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}