{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YianKim/2022_uncertainty_aware_semisupervise/blob/main/Keras_UncertaintyBootstrap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ik7Qx5iO8lQ_"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import clone_model\n",
        "\n",
        "import PIL\n",
        "from PIL import Image\n",
        "\n",
        "import pickle\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "from keras.layers.core import Lambda\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import MaxPooling2D, AveragePooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras import optimizers\n",
        "from keras.callbacks import *\n",
        "from sklearn.metrics import *\n",
        "from keras.models import load_model\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as backend\n",
        "import math\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Hmq32hTH-Jv"
      },
      "source": [
        "# SVHN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUCvct_dH5m1"
      },
      "outputs": [],
      "source": [
        "# from scipy.io import loadmat\n",
        "# train_raw = loadmat('train_32x32.mat')\n",
        "# test_raw = loadmat('test_32x32.mat')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzWKzGmGtk2h"
      },
      "outputs": [],
      "source": [
        "# def dummy_labels(labels):\n",
        "#   zero_labels = np.zeros([labels.shape[0], 10], np.int8)  \n",
        "#   for i in range(labels.shape[0]):\n",
        "#     zero_labels[i][labels[i]] = 1\n",
        "#   return(zero_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65QFu6LVMxl9"
      },
      "outputs": [],
      "source": [
        "# train_images = train_raw['X']\n",
        "# train_labels = train_raw['y']\n",
        "\n",
        "# test_images = test_raw['X']\n",
        "# test_labels = dummy_labels(test_raw['y']-1)\n",
        "\n",
        "# train_images = train_images.swapaxes(2,3).swapaxes(1,2).swapaxes(0,1)\n",
        "# test_images = test_images.swapaxes(2,3).swapaxes(1,2).swapaxes(0,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fm_NgimXNonL"
      },
      "outputs": [],
      "source": [
        "# temp = [0,0,0,0,0,0,0,0,0,0]\n",
        "# label_indx = []\n",
        "# unlabel_indx = []\n",
        "\n",
        "# for i in range(73257) :\n",
        "#   if temp[(train_labels).reshape([-1])[i]-1] < 25 :\n",
        "#     temp[(train_labels).reshape([-1])[i]-1] += 1\n",
        "#     label_indx.append(i)\n",
        "#   else :\n",
        "#     unlabel_indx.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZADJPIIOZD2"
      },
      "outputs": [],
      "source": [
        "# lbl_train_images = train_images[label_indx]\n",
        "# lbl_train_labels = dummy_labels(train_labels[label_indx]-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHe18DTWUu3-"
      },
      "outputs": [],
      "source": [
        "# ubl_train_images = train_images[unlabel_indx]\n",
        "# ubl_train_labels = dummy_labels(train_labels[unlabel_indx]-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCGWSJL0MJ3W"
      },
      "source": [
        "# CIFAR 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAzGxvve8pgp"
      },
      "outputs": [],
      "source": [
        "cifar10 = keras.datasets.cifar10 \n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "train_images = train_images\n",
        "test_images = test_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqUm92WTGW3p"
      },
      "outputs": [],
      "source": [
        "def dummy_labels(labels):\n",
        "  zero_labels = np.zeros([labels.shape[0], 10], np.int8)  \n",
        "  for i in range(labels.shape[0]):\n",
        "    zero_labels[i][labels[i]] = 1\n",
        "  return(zero_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFVXfHQNGtmq"
      },
      "outputs": [],
      "source": [
        "train_labels = dummy_labels(train_labels)\n",
        "test_labels = dummy_labels(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOqDJtXCG2ov"
      },
      "outputs": [],
      "source": [
        "# 1000 labeled, 49000 unlabeled\n",
        "random.seed(10)\n",
        "indx = random.sample(range(train_labels.shape[0]),train_labels.shape[0])\n",
        "\n",
        "lbl_train_images = train_images[indx[:1000]]\n",
        "ubl_train_images = train_images[indx[1000:]]\n",
        "\n",
        "lbl_train_labels = train_labels[indx[:1000]]\n",
        "ubl_train_labels = train_labels[indx[1000:]]\n",
        "\n",
        "# valids1 =  train_images[indx[800:1000]]\n",
        "# valids2 =  train_labels[indx[800:1000]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7hfth6hMMxW"
      },
      "source": [
        "#MAin\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKyn6Njs7vqa"
      },
      "outputs": [],
      "source": [
        "class SGDR(Callback):\n",
        "\n",
        "    def __init__(self, min_lr=0.0, max_lr=0.03, base_epochs=20, mul_epochs=2):\n",
        "        super(SGDR, self).__init__()\n",
        "\n",
        "        self.min_lr = min_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.base_epochs = base_epochs\n",
        "        self.mul_epochs = mul_epochs\n",
        "\n",
        "        self.cycles = 0.\n",
        "        self.cycle_iterations = 0.\n",
        "        self.trn_iterations = 0.\n",
        "\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self, new_min_lr=None, new_max_lr=None,\n",
        "               new_base_epochs=None, new_mul_epochs=None):\n",
        "        \"\"\"Resets cycle iterations.\"\"\"\n",
        "        \n",
        "        if new_min_lr != None:\n",
        "            self.min_lr = new_min_lr\n",
        "        if new_max_lr != None:\n",
        "            self.max_lr = new_max_lr\n",
        "        if new_base_epochs != None:\n",
        "            self.base_epochs = new_base_epochs\n",
        "        if new_mul_epochs != None:\n",
        "            self.mul_epochs = new_mul_epochs\n",
        "        self.cycles = 0.\n",
        "        self.cycle_iterations = 0.\n",
        "        \n",
        "    def sgdr(self):\n",
        "        \n",
        "        cycle_epochs = self.base_epochs * (self.mul_epochs ** self.cycles)\n",
        "        tide = ((self.cycles == 0) * 1) * (self.cycle_iterations*self.max_lr + (self.base_epochs - self.cycle_iterations)*self.min_lr) / self.base_epochs + ((self.cycles != 0) * 1)*(self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(np.pi * (self.cycle_iterations + 1) / cycle_epochs)))\n",
        "        return tide\n",
        "        \n",
        "    def on_train_begin(self, logs=None):\n",
        "        \n",
        "        if self.cycle_iterations == 0:\n",
        "            K.set_value(self.model.optimizer.lr, self.max_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.sgdr())\n",
        "            \n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        \n",
        "        logs = logs or {}\n",
        "        logs['lr'] = K.get_value(self.model.optimizer.lr)\n",
        "        \n",
        "        self.trn_iterations += 1\n",
        "        self.cycle_iterations += 1\n",
        "        if self.cycle_iterations >= self.base_epochs * (self.mul_epochs ** self.cycles):\n",
        "            self.cycles += 1\n",
        "            self.cycle_iterations = 0\n",
        "            K.set_value(self.model.optimizer.lr, self.max_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.sgdr())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRCDNliwfudJ"
      },
      "outputs": [],
      "source": [
        "def PermaDropout(rate):\n",
        "    return Lambda(lambda x: K.dropout(x, level=rate))\n",
        "\n",
        "def create_cnn_13():\n",
        "  conv1a = Conv2D(128, (3,3), padding = 'same')\n",
        "  bn1a = BatchNormalization()\n",
        "  conv1b = Conv2D(128, (3,3), padding = 'same')\n",
        "  bn1b = BatchNormalization()\n",
        "  conv1c = Conv2D(128, (3,3), padding = 'same')\n",
        "  bn1c = BatchNormalization()\n",
        "  pl1 = MaxPooling2D(2, 2)\n",
        "  MCdrop1 = PermaDropout(0.5)\n",
        "\n",
        "  conv2a = Conv2D(256, (3,3), padding = 'same')\n",
        "  bn2a = BatchNormalization()\n",
        "  conv2b = Conv2D(256, (3,3), padding = 'same')\n",
        "  bn2b = BatchNormalization()\n",
        "  conv2c = Conv2D(256, (3,3), padding = 'same')\n",
        "  bn2c = BatchNormalization()\n",
        "  pl2 = MaxPooling2D(2, 2)\n",
        "  MCdrop2 = PermaDropout(0.5)\n",
        "\n",
        "  conv3a = Conv2D(512, (3,3))\n",
        "  bn3a = BatchNormalization()\n",
        "  conv3b = Conv2D(256, (1,1))\n",
        "  bn3b = BatchNormalization()\n",
        "  conv3c = Conv2D(128, (1,1))\n",
        "  bn3c = BatchNormalization()\n",
        "  pl3 = AveragePooling2D(6, 2)\n",
        "\n",
        "  fc = Dense(10)\n",
        "  activ = keras.layers.LeakyReLU(0.1)\n",
        "\n",
        "  model = Sequential([\n",
        "                      keras.Input(shape=(32, 32, 3)), \n",
        "                      tfa.layers.WeightNormalization(conv1a), bn1a, activ,\n",
        "                      tfa.layers.WeightNormalization(conv1b), bn1b, activ,\n",
        "                      tfa.layers.WeightNormalization(conv1c), bn1c, activ,\n",
        "                      pl1, MCdrop1,\n",
        "\n",
        "                      tfa.layers.WeightNormalization(conv2a), bn2a, activ,\n",
        "                      tfa.layers.WeightNormalization(conv2b), bn2b, activ,\n",
        "                      tfa.layers.WeightNormalization(conv2c), bn2c, activ,\n",
        "                      pl2, MCdrop2,\n",
        "\n",
        "                      tfa.layers.WeightNormalization(conv3a), bn3a, activ,\n",
        "                      tfa.layers.WeightNormalization(conv3b), bn3b, activ,\n",
        "                      tfa.layers.WeightNormalization(conv3c), bn3c, activ,\n",
        "                      pl3, Flatten(),\n",
        "                      \n",
        "                      fc\n",
        "                      ])\n",
        "  \n",
        "  return model\n",
        "\n",
        "def compile_cnn_13(model):\n",
        "\n",
        "  opt = keras.optimizers.SGD(0.03, momentum=0.9)\n",
        "\n",
        "  model.compile(\n",
        "    optimizer = opt,\n",
        "    loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        "  )\n",
        "\n",
        "  return model\n",
        "\n",
        "def cnn_13():\n",
        "\n",
        "  model = create_cnn_13()\n",
        "  model = compile_cnn_13(model)\n",
        "\n",
        "  return model\n",
        "\n",
        "def fit_and_labeling_cnn_13(Epoch, Batch):\n",
        "\n",
        "  lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
        "  early_stopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=30, mode='auto')\n",
        "  sgdr = SGDR(min_lr=0.0, max_lr=0.03, base_epochs=20) #스케줄러\n",
        "\n",
        "  model.fit(\n",
        "      x=X,\n",
        "      y=y,\n",
        "      epochs=Epoch,\n",
        "      verbose=0,\n",
        "      batch_size=Batch,\n",
        "      callbacks=[sgdr]\n",
        "  )\n",
        "    \n",
        "  model_test_eval(model, test_images, test_labels)\n",
        "  T = 1\n",
        "\n",
        "  for predsamples in (range(10)):\n",
        "    if predsamples == 0 :\n",
        "      predictions = np.array(tf.nn.softmax(model.predict(ubl_train_images)/T))\n",
        "      predictions = predictions.reshape((1,) + predictions.shape)\n",
        "    else:\n",
        "      pred = np.array(tf.nn.softmax(model.predict(ubl_train_images)/T))\n",
        "      pred = pred.reshape((1,) + pred.shape)\n",
        "      predictions = np.concatenate((predictions, pred))\n",
        "\n",
        "  return predictions\n",
        "\n",
        "def model_test_eval(model, test_images, test_labels):\n",
        "  T = 1\n",
        "  pred = np.array(tf.nn.softmax(model.predict(test_images)/T))\n",
        "  for i in range(1,10):\n",
        "    pred += np.array(tf.nn.softmax(model.predict(test_images)))\n",
        "  acc = (np.argmax(pred,axis=1) == np.argmax(test_labels,axis=1))*1\n",
        "  acc = sum(acc)/len(acc)\n",
        "  print(\"test set 성능 : \" + str(acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdpEv0sAQoil"
      },
      "outputs": [],
      "source": [
        "def basic_augmentation(imagearray):\n",
        "  image = Image.fromarray(imagearray)\n",
        "  tr1 = transforms.RandomHorizontalFlip()\n",
        "  tr2 = transforms.RandomRotation(10)\n",
        "  tr3 = transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2)\n",
        "  tr4 = transforms.RandomAffine(0, shear=10, scale=(0.8, 1.2))\n",
        "  image = tr1(tr2(tr3(tr4(image))))\n",
        "  return(np.array(image))\n",
        "\n",
        "def makeaugs(n, input):\n",
        "  augs = []\n",
        "  for j in range(n):\n",
        "    for i in input:\n",
        "      augs.append(basic_augmentation(np.array(i, np.uint8)))\n",
        "  return(np.array(augs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSjGBMuwQoil"
      },
      "outputs": [],
      "source": [
        "def sample_beta_distribution(size, concentration_0=0.3, concentration_1=0.3):\n",
        "    gamma_1_sample = tf.random.gamma(shape=[size], alpha=concentration_1)\n",
        "    gamma_2_sample = tf.random.gamma(shape=[size], alpha=concentration_0)\n",
        "    return gamma_1_sample / (gamma_1_sample + gamma_2_sample)\n",
        "\n",
        "def mixup (size, data, alpha = 0.2):\n",
        "  image, label = data\n",
        "  L = sample_beta_distribution(size, alpha, alpha)\n",
        "  XL = tf.reshape(L, (size, 1, 1, 1))\n",
        "  YL = tf.reshape(L, (size, 1))\n",
        "  IND1 = np.random.choice(len(label), size)\n",
        "  IND2 = np.random.choice(len(label), size)\n",
        "  newimage = XL*image[IND1] + (1-XL)*image[IND2]\n",
        "  newlabel = YL*label[IND1] + (1-YL)*label[IND2]\n",
        "  return (newimage, newlabel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz5CFL960tsP"
      },
      "outputs": [],
      "source": [
        "# model = cnn_13()\n",
        "# X = lbl_train_images\n",
        "# y = lbl_train_labels\n",
        "\n",
        "# X = np.concatenate([makeaugs(10, X), X])\n",
        "# y = np.concatenate([y,y,y,y,y,y,y,y,y,y, y])\n",
        "\n",
        "# predictions = fit_and_labeling_cnn_13(200, 64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blQslYoiQoim"
      },
      "outputs": [],
      "source": [
        "# schedule = list(2**np.array(list(range(5)))/(2**5))\n",
        "# schedule = [[i,i,i,i,i] for i in schedule]\n",
        "# schedule = np.array(schedule).reshape([-1])\n",
        "\n",
        "# alpha = 0.2\n",
        "# first = time.time()\n",
        "\n",
        "# for iter in range(len(schedule)):\n",
        "#     pseudo = np.argmax(np.mean(predictions, axis=0), axis=1)\n",
        "#     conf = np.max(np.mean(predictions, axis=0), axis=1)\n",
        "#     uncert = np.std(predictions, axis=0)\n",
        "#     uncert = np.array([uncert[i][pseudo[i]] for i in range(len(pseudo))])\n",
        "#     cert = 1-uncert\n",
        "    \n",
        "#     ubl_pseudo_labels = []\n",
        "#     for i in pseudo:\n",
        "#         temp = [0,0,0,0,0,0,0,0,0,0]\n",
        "#         temp[i] = 1\n",
        "#         ubl_pseudo_labels.append(temp)\n",
        "#     ubl_pseudo_labels = np.array(ubl_pseudo_labels)\n",
        "\n",
        "#     score = alpha*conf + (1-alpha)*cert\n",
        "#     score = (score-min(score))/(max(score)-min(score))+0.0001\n",
        "#     score = np.exp(score/schedule[iter])\n",
        "#     score = score/sum(score)\n",
        "\n",
        "#     indx = np.random.choice(len(score), 50000, p = score)\n",
        "\n",
        "#     X = ubl_train_images[indx]\n",
        "#     y = ubl_pseudo_labels[indx]\n",
        "\n",
        "#     X = np.concatenate([lbl_train_images, X])\n",
        "#     y = np.concatenate([lbl_train_labels, y])\n",
        "\n",
        "#     size = len(y) * 4\n",
        "#     newimage, newlabel = mixup(size, (X, y))\n",
        "#     augimage, auglabel = makeaugs(4, X), np.concatenate((y,y,y,y))\n",
        "#     X = np.concatenate((newimage, augimage))\n",
        "#     y = np.concatenate((newlabel, auglabel))\n",
        "\n",
        "#     print(\"< iter \"+str(iter)+\" evaluation >\")\n",
        "#     predictions = fit_and_labeling_cnn_13(25, 64)\n",
        "#     print(time.time() - first)\n",
        "    \n",
        "#     del newimage, newlabel, augimage, auglabel, X, y\n",
        "#     gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NR0h9IlxQoim"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "wlvPpnlEQoim",
        "outputId": "75856890-9cd7-4a17-a469-7b166b5422b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<<< alpha 0 evaluation >>>\n",
            "test set 성능 : 0.5987\n",
            "< iter 0 evaluation >\n",
            "test set 성능 : 0.6687\n",
            "4783.803838014603\n",
            "< iter 1 evaluation >\n",
            "test set 성능 : 0.6809\n",
            "9549.411909103394\n",
            "< iter 2 evaluation >\n",
            "test set 성능 : 0.6853\n",
            "14341.644701242447\n",
            "< iter 3 evaluation >\n",
            "test set 성능 : 0.6996\n",
            "19135.61491894722\n",
            "< iter 4 evaluation >\n",
            "test set 성능 : 0.7042\n",
            "23910.841858148575\n",
            "< iter 5 evaluation >\n",
            "test set 성능 : 0.7079\n",
            "28718.722818374634\n",
            "< iter 6 evaluation >\n",
            "test set 성능 : 0.7183\n",
            "33548.474012851715\n",
            "< iter 7 evaluation >\n",
            "test set 성능 : 0.726\n",
            "38351.46155834198\n",
            "< iter 8 evaluation >\n",
            "test set 성능 : 0.7232\n",
            "43181.923592567444\n",
            "< iter 9 evaluation >\n",
            "test set 성능 : 0.7318\n",
            "48032.58542919159\n",
            "<<< alpha 0.2 evaluation >>>\n",
            "test set 성능 : 0.6129\n",
            "< iter 0 evaluation >\n",
            "test set 성능 : 0.6598\n",
            "4829.364767074585\n",
            "< iter 1 evaluation >\n",
            "test set 성능 : 0.6783\n",
            "9665.686887741089\n",
            "< iter 2 evaluation >\n",
            "test set 성능 : 0.6917\n",
            "14522.985300064087\n",
            "< iter 3 evaluation >\n",
            "test set 성능 : 0.697\n",
            "19379.943120479584\n",
            "< iter 4 evaluation >\n",
            "test set 성능 : 0.7059\n",
            "24235.826816797256\n",
            "< iter 5 evaluation >\n",
            "test set 성능 : 0.7178\n",
            "29131.680171966553\n",
            "< iter 6 evaluation >\n",
            "test set 성능 : 0.7214\n",
            "34002.9441344738\n",
            "< iter 7 evaluation >\n",
            "test set 성능 : 0.7289\n",
            "38878.457111120224\n",
            "< iter 8 evaluation >\n",
            "test set 성능 : 0.7322\n",
            "43776.2232363224\n",
            "< iter 9 evaluation >\n",
            "test set 성능 : 0.7359\n",
            "48672.807284116745\n",
            "<<< alpha 0.4 evaluation >>>\n",
            "test set 성능 : 0.611\n",
            "< iter 0 evaluation >\n",
            "test set 성능 : 0.6687\n",
            "4920.7452437877655\n",
            "< iter 1 evaluation >\n",
            "test set 성능 : 0.6871\n",
            "9858.999803066254\n",
            "< iter 2 evaluation >\n",
            "test set 성능 : 0.6986\n",
            "14788.313618183136\n",
            "< iter 3 evaluation >\n",
            "test set 성능 : 0.7062\n",
            "19728.429060697556\n",
            "< iter 4 evaluation >\n",
            "test set 성능 : 0.7109\n",
            "24687.643452644348\n",
            "< iter 5 evaluation >\n",
            "test set 성능 : 0.7214\n",
            "29666.176280736923\n",
            "< iter 6 evaluation >\n",
            "test set 성능 : 0.7244\n",
            "34631.43549704552\n",
            "< iter 7 evaluation >\n",
            "test set 성능 : 0.73\n",
            "39618.94045615196\n",
            "< iter 8 evaluation >\n",
            "test set 성능 : 0.7323\n",
            "44616.47123169899\n",
            "< iter 9 evaluation >\n",
            "test set 성능 : 0.7328\n",
            "49593.94083118439\n",
            "<<< alpha 0.6 evaluation >>>\n",
            "test set 성능 : 0.6046\n",
            "< iter 0 evaluation >\n",
            "test set 성능 : 0.6637\n",
            "4997.288788557053\n",
            "< iter 1 evaluation >\n",
            "test set 성능 : 0.686\n",
            "10016.13076543808\n",
            "< iter 2 evaluation >\n",
            "test set 성능 : 0.6973\n",
            "15002.956176280975\n",
            "< iter 3 evaluation >\n",
            "test set 성능 : 0.6982\n",
            "20016.69614124298\n",
            "< iter 4 evaluation >\n",
            "test set 성능 : 0.7061\n",
            "25037.629477262497\n",
            "< iter 5 evaluation >\n",
            "test set 성능 : 0.717\n",
            "30037.589279413223\n",
            "< iter 6 evaluation >\n",
            "test set 성능 : 0.7264\n",
            "35065.872270822525\n",
            "< iter 7 evaluation >\n",
            "test set 성능 : 0.7317\n",
            "40141.736724853516\n",
            "< iter 8 evaluation >\n",
            "test set 성능 : 0.7365\n",
            "45197.71412730217\n",
            "< iter 9 evaluation >\n",
            "test set 성능 : 0.7363\n",
            "50272.90678191185\n",
            "<<< alpha 0.8 evaluation >>>\n"
          ]
        }
      ],
      "source": [
        "for alphas in [0, 0.2, 0.4, 0.6, 0.8, 1.0]:\n",
        "    \n",
        "    print(\"<<< alpha \"+str(alphas)+\" evaluation >>>\")\n",
        "    \n",
        "    model = cnn_13()\n",
        "    X = lbl_train_images\n",
        "    y = lbl_train_labels\n",
        "\n",
        "    X = np.concatenate([makeaugs(10, X), X])\n",
        "    y = np.concatenate([y,y,y,y,y,y,y,y,y,y, y])\n",
        "\n",
        "    predictions = fit_and_labeling_cnn_13(200, 64)\n",
        "\n",
        "#     schedule = list(2**np.array(list(range(5)))/(2**5))\n",
        "    schedule = list(2**np.array(list(range(2)))/(2**5))\n",
        "    schedule = [[i,i,i,i,i] for i in schedule]\n",
        "    schedule = np.array(schedule).reshape([-1])\n",
        "\n",
        "    alpha = alphas\n",
        "    first = time.time()\n",
        "\n",
        "    for iter in range(len(schedule)):\n",
        "        pseudo = np.argmax(np.mean(predictions, axis=0), axis=1)\n",
        "        conf = np.max(np.mean(predictions, axis=0), axis=1)\n",
        "        uncert = np.std(predictions, axis=0)\n",
        "        uncert = np.array([uncert[i][pseudo[i]] for i in range(len(pseudo))])\n",
        "        cert = 1-uncert\n",
        "\n",
        "        ubl_pseudo_labels = []\n",
        "        for i in pseudo:\n",
        "            temp = [0,0,0,0,0,0,0,0,0,0]\n",
        "            temp[i] = 1\n",
        "            ubl_pseudo_labels.append(temp)\n",
        "        ubl_pseudo_labels = np.array(ubl_pseudo_labels)\n",
        "\n",
        "        score = alpha*conf + (1-alpha)*cert\n",
        "        score = (score-min(score))/(max(score)-min(score))+0.0001\n",
        "        score = np.exp(score/schedule[iter])\n",
        "        score = score/sum(score)\n",
        "\n",
        "        indx = np.random.choice(len(score), 50000, p = score)\n",
        "\n",
        "        X = ubl_train_images[indx]\n",
        "        y = ubl_pseudo_labels[indx]\n",
        "\n",
        "        X = np.concatenate([lbl_train_images, X])\n",
        "        y = np.concatenate([lbl_train_labels, y])\n",
        "\n",
        "        size = len(y) * 4\n",
        "        newimage, newlabel = mixup(size, (X, y))\n",
        "        augimage, auglabel = makeaugs(4, X), np.concatenate((y,y,y,y))\n",
        "        X = np.concatenate((newimage, augimage))\n",
        "        y = np.concatenate((newlabel, auglabel))\n",
        "\n",
        "        print(\"< iter \"+str(iter)+\" evaluation >\")\n",
        "        predictions = fit_and_labeling_cnn_13(25, 64)\n",
        "        print(time.time() - first)\n",
        "\n",
        "        del newimage, newlabel, augimage, auglabel, X, y\n",
        "        gc.collect()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Keras_UncertaintyBootstrap.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}