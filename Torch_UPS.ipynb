{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Torch_UPS.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPCTqHFNR91dSirBiXMEXRW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YianKim/2022_uncertainty_aware_semisupervise/blob/main/Torch_UPS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeP-K3X_wckI",
        "outputId": "bbcecd01-420d-4090-8f57-47addd9c7a3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 29.1 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 28.4 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 19.3 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 40 kB 17.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 61 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 71 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 81 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 92 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 102 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 112 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 122 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 125 kB 9.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import time\n",
        "from copy import deepcopy\n",
        "from collections import OrderedDict\n",
        "import pickle\n",
        "import numpy as np\n",
        "from re import search\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from tensorboardX import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "hYgB5FyDverl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Load & Augmentation"
      ],
      "metadata": {
        "id": "q-QRkj6lvF8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_list():  # 16 oeprations and their ranges\n",
        "    # https://github.com/google-research/uda/blob/master/image/randaugment/policies.py#L57\n",
        "    # l = [\n",
        "    #     (Identity, 0., 1.0),\n",
        "    #     (ShearX, 0., 0.3),  # 0\n",
        "    #     (ShearY, 0., 0.3),  # 1\n",
        "    #     (TranslateX, 0., 0.33),  # 2\n",
        "    #     (TranslateY, 0., 0.33),  # 3\n",
        "    #     (Rotate, 0, 30),  # 4\n",
        "    #     (AutoContrast, 0, 1),  # 5\n",
        "    #     (Invert, 0, 1),  # 6\n",
        "    #     (Equalize, 0, 1),  # 7\n",
        "    #     (Solarize, 0, 110),  # 8\n",
        "    #     (Posterize, 4, 8),  # 9\n",
        "    #     # (Contrast, 0.1, 1.9),  # 10\n",
        "    #     (Color, 0.1, 1.9),  # 11\n",
        "    #     (Brightness, 0.1, 1.9),  # 12\n",
        "    #     (Sharpness, 0.1, 1.9),  # 13\n",
        "    #     # (Cutout, 0, 0.2),  # 14\n",
        "    #     # (SamplePairing(imgs), 0, 0.4),  # 15\n",
        "    # ]\n",
        "\n",
        "    # https://github.com/tensorflow/tpu/blob/8462d083dd89489a79e3200bcc8d4063bf362186/models/official/efficientnet/autoaugment.py#L505\n",
        "    l = [\n",
        "        (AutoContrast, 0, 1),\n",
        "        (Equalize, 0, 1),\n",
        "        (Invert, 0, 1),\n",
        "        (Rotate, 0, 30),\n",
        "        (Posterize, 0, 4),\n",
        "        (Solarize, 0, 256),\n",
        "        (SolarizeAdd, 0, 110),\n",
        "        (Color, 0.1, 1.9),\n",
        "        (Contrast, 0.1, 1.9),\n",
        "        (Brightness, 0.1, 1.9),\n",
        "        (Sharpness, 0.1, 1.9),\n",
        "        (ShearX, 0., 0.3),\n",
        "        (ShearY, 0., 0.3),\n",
        "        (CutoutAbs, 0, 40),\n",
        "        (TranslateXabs, 0., 100),\n",
        "        (TranslateYabs, 0., 100),\n",
        "    ]\n",
        "\n",
        "    return l"
      ],
      "metadata": {
        "id": "uzxVik4EqwPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RandAugment:\n",
        "    def __init__(self, n, m):\n",
        "        self.n = n\n",
        "        self.m = m      # [0, 30]\n",
        "        self.augment_list = augment_list()\n",
        "\n",
        "    def __call__(self, img):\n",
        "        ops = random.choices(self.augment_list, k=self.n)\n",
        "        for op, minval, maxval in ops:\n",
        "            val = (float(self.m) / 30) * float(maxval - minval) + minval\n",
        "            img = op(img, val)\n",
        "\n",
        "        return img"
      ],
      "metadata": {
        "id": "Cw_bld7hkUyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CutoutRandom(object):\n",
        "    def __init__(self, n_holes, length, random=True):\n",
        "        self.n_holes = n_holes\n",
        "        self.length = length\n",
        "        self.random = random\n",
        "\n",
        "    def __call__(self, img):\n",
        "        h = img.size(1)\n",
        "        w = img.size(2)\n",
        "        length = random.randint(1, self.length)\n",
        "        mask = np.ones((h, w), np.float32)\n",
        "\n",
        "        for n in range(self.n_holes):\n",
        "            y = np.random.randint(h)\n",
        "            x = np.random.randint(w)\n",
        "\n",
        "            y1 = np.clip(y - length // 2, 0, h)\n",
        "            y2 = np.clip(y + length // 2, 0, h)\n",
        "            x1 = np.clip(x - length // 2, 0, w)\n",
        "            x2 = np.clip(x + length // 2, 0, w)\n",
        "\n",
        "            mask[y1: y2, x1: x2] = 0.\n",
        "\n",
        "        mask = torch.from_numpy(mask)\n",
        "        mask = mask.expand_as(img)\n",
        "        img = img * mask\n",
        "\n",
        "        return img"
      ],
      "metadata": {
        "id": "kle7rsRPqIUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cifar10(root='data/datasets', n_lbl=4000, ssl_idx=None, pseudo_lbl=None, itr=0, split_txt=''):\n",
        "    os.makedirs(root, exist_ok=True) #create the root directory for saving data\n",
        "    # augmentations\n",
        "    transform_train = transforms.Compose([\n",
        "        RandAugment(3,4),  #from https://arxiv.org/pdf/1909.13719.pdf. For CIFAR-10 M=3, N=4\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(size=32, padding=int(32*0.125), padding_mode='reflect'),\n",
        "        transforms.ColorJitter(\n",
        "            brightness=0.4,\n",
        "            contrast=0.4,\n",
        "            saturation=0.4,\n",
        "        ),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),\n",
        "        CutoutRandom(n_holes=1, length=16, random=True)\n",
        "    ])\n",
        "    \n",
        "    transform_val = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616))\n",
        "    ])\n",
        "\n",
        "    if ssl_idx is None:\n",
        "        base_dataset = datasets.CIFAR10(root, train=True, download=True)\n",
        "        train_lbl_idx, train_unlbl_idx = lbl_unlbl_split(base_dataset.targets, n_lbl, 10)\n",
        "        \n",
        "        os.makedirs('data/splits', exist_ok=True)\n",
        "        f = open(os.path.join('data/splits', f'cifar10_basesplit_{n_lbl}_{split_txt}.pkl'),\"wb\")\n",
        "        lbl_unlbl_dict = {'lbl_idx': train_lbl_idx, 'unlbl_idx': train_unlbl_idx}\n",
        "        pickle.dump(lbl_unlbl_dict,f)\n",
        "    \n",
        "    else:\n",
        "        lbl_unlbl_dict = pickle.load(open(ssl_idx, 'rb'))\n",
        "        train_lbl_idx = lbl_unlbl_dict['lbl_idx']\n",
        "        train_unlbl_idx = lbl_unlbl_dict['unlbl_idx']\n",
        "\n",
        "    lbl_idx = train_lbl_idx\n",
        "    if pseudo_lbl is not None:\n",
        "        pseudo_lbl_dict = pickle.load(open(pseudo_lbl, 'rb'))\n",
        "        pseudo_idx = pseudo_lbl_dict['pseudo_idx']\n",
        "        pseudo_target = pseudo_lbl_dict['pseudo_target']\n",
        "        nl_idx = pseudo_lbl_dict['nl_idx']\n",
        "        nl_mask = pseudo_lbl_dict['nl_mask']\n",
        "        lbl_idx = np.array(lbl_idx + pseudo_idx)\n",
        "\n",
        "        #balance the labeled and unlabeled data \n",
        "        if len(nl_idx) > len(lbl_idx):\n",
        "            exapand_labeled = len(nl_idx) // len(lbl_idx)\n",
        "            lbl_idx = np.hstack([lbl_idx for _ in range(exapand_labeled)])\n",
        "\n",
        "            if len(lbl_idx) < len(nl_idx):\n",
        "                diff = len(nl_idx) - len(lbl_idx)\n",
        "                lbl_idx = np.hstack((lbl_idx, np.random.choice(lbl_idx, diff)))\n",
        "            else:\n",
        "                assert len(lbl_idx) == len(nl_idx)\n",
        "    else:\n",
        "        pseudo_idx = None\n",
        "        pseudo_target = None\n",
        "        nl_idx = None\n",
        "        nl_mask = None\n",
        "\n",
        "    train_lbl_dataset = CIFAR10SSL(\n",
        "        root, lbl_idx, train=True, transform=transform_train,\n",
        "        pseudo_idx=pseudo_idx, pseudo_target=pseudo_target,\n",
        "        nl_idx=nl_idx, nl_mask=nl_mask)\n",
        "    \n",
        "    if nl_idx is not None:\n",
        "        train_nl_dataset = CIFAR10SSL(\n",
        "            root, np.array(nl_idx), train=True, transform=transform_train,\n",
        "            pseudo_idx=pseudo_idx, pseudo_target=pseudo_target,\n",
        "            nl_idx=nl_idx, nl_mask=nl_mask)\n",
        "\n",
        "    train_unlbl_dataset = CIFAR10SSL(\n",
        "    root, train_unlbl_idx, train=True, transform=transform_val)\n",
        "\n",
        "    test_dataset = datasets.CIFAR10(root, train=False, transform=transform_val, download=True)\n",
        "\n",
        "    if nl_idx is not None:\n",
        "        return train_lbl_dataset, train_nl_dataset, train_unlbl_dataset, test_dataset\n",
        "    else:\n",
        "        return train_lbl_dataset, train_unlbl_dataset, train_unlbl_dataset, test_dataset\n",
        "\n",
        "\n",
        "def get_cifar100(root='data/datasets', n_lbl=10000, ssl_idx=None, pseudo_lbl=None, itr=0, split_txt=''):\n",
        "    ## augmentations\n",
        "    transform_train = transforms.Compose([\n",
        "        RandAugment(3,4),  #from https://arxiv.org/pdf/1909.13719.pdf. For CIFAR-10 M=3, N=4\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(size=32, padding=int(32*0.125), padding_mode='reflect'),\n",
        "        transforms.ColorJitter(\n",
        "            brightness=0.4,\n",
        "            contrast=0.4,\n",
        "            saturation=0.4,\n",
        "        ),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.5071, 0.4867, 0.4408), std=(0.2675, 0.2565, 0.2761)),\n",
        "        CutoutRandom(n_holes=1, length=16, random=True)\n",
        "    ])\n",
        "    \n",
        "    transform_val = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.5071, 0.4867, 0.4408), std=(0.2675, 0.2565, 0.2761))\n",
        "    ])\n",
        "\n",
        "    if ssl_idx is None:\n",
        "        base_dataset = datasets.CIFAR100(root, train=True, download=True)\n",
        "        train_lbl_idx, train_unlbl_idx = lbl_unlbl_split(base_dataset.targets, n_lbl, 100)\n",
        "        \n",
        "        f = open(os.path.join('data/splits', f'cifar100_basesplit_{n_lbl}_{split_txt}.pkl'),\"wb\")\n",
        "        lbl_unlbl_dict = {'lbl_idx': train_lbl_idx, 'unlbl_idx': train_unlbl_idx}\n",
        "        pickle.dump(lbl_unlbl_dict,f)\n",
        "    \n",
        "    else:\n",
        "        lbl_unlbl_dict = pickle.load(open(ssl_idx, 'rb'))\n",
        "        train_lbl_idx = lbl_unlbl_dict['lbl_idx']\n",
        "        train_unlbl_idx = lbl_unlbl_dict['unlbl_idx']\n",
        "\n",
        "    lbl_idx = train_lbl_idx\n",
        "    if pseudo_lbl is not None:\n",
        "        pseudo_lbl_dict = pickle.load(open(pseudo_lbl, 'rb'))\n",
        "        pseudo_idx = pseudo_lbl_dict['pseudo_idx']\n",
        "        pseudo_target = pseudo_lbl_dict['pseudo_target']\n",
        "        nl_idx = pseudo_lbl_dict['nl_idx']\n",
        "        nl_mask = pseudo_lbl_dict['nl_mask']\n",
        "        lbl_idx = np.array(lbl_idx + pseudo_idx)\n",
        "\n",
        "        #balance the labeled and unlabeled data \n",
        "        if len(nl_idx) > len(lbl_idx):\n",
        "            exapand_labeled = len(nl_idx) // len(lbl_idx)\n",
        "            lbl_idx = np.hstack([lbl_idx for _ in range(exapand_labeled)])\n",
        "\n",
        "            if len(lbl_idx) < len(nl_idx):\n",
        "                diff = len(nl_idx) - len(lbl_idx)\n",
        "                lbl_idx = np.hstack((lbl_idx, np.random.choice(lbl_idx, diff)))\n",
        "            else:\n",
        "                assert len(lbl_idx) == len(nl_idx)\n",
        "    else:\n",
        "        pseudo_idx = None\n",
        "        pseudo_target = None\n",
        "        nl_idx = None\n",
        "        nl_mask = None\n",
        "\n",
        "    train_lbl_dataset = CIFAR100SSL(\n",
        "        root, lbl_idx, train=True, transform=transform_train,\n",
        "        pseudo_idx=pseudo_idx, pseudo_target=pseudo_target,\n",
        "        nl_idx=nl_idx, nl_mask=nl_mask)\n",
        "    \n",
        "    if nl_idx is not None:\n",
        "        train_nl_dataset = CIFAR100SSL(\n",
        "            root, np.array(nl_idx), train=True, transform=transform_train,\n",
        "            pseudo_idx=pseudo_idx, pseudo_target=pseudo_target,\n",
        "            nl_idx=nl_idx, nl_mask=nl_mask)\n",
        "\n",
        "    train_unlbl_dataset = CIFAR100SSL(\n",
        "    root, train_unlbl_idx, train=True, transform=transform_val)\n",
        "\n",
        "    test_dataset = datasets.CIFAR100(root, train=False, transform=transform_val, download=True)\n",
        "\n",
        "    if nl_idx is not None:\n",
        "        return train_lbl_dataset, train_nl_dataset, train_unlbl_dataset, test_dataset\n",
        "    else:\n",
        "        return train_lbl_dataset, train_unlbl_dataset, train_unlbl_dataset, test_dataset\n",
        "\n",
        "\n",
        "def lbl_unlbl_split(lbls, n_lbl, n_class):\n",
        "    lbl_per_class = n_lbl // n_class\n",
        "    lbls = np.array(lbls)\n",
        "    lbl_idx = []\n",
        "    unlbl_idx = []\n",
        "    for i in range(n_class):\n",
        "        idx = np.where(lbls == i)[0]\n",
        "        np.random.shuffle(idx)\n",
        "        lbl_idx.extend(idx[:lbl_per_class])\n",
        "        unlbl_idx.extend(idx[lbl_per_class:])\n",
        "    return lbl_idx, unlbl_idx\n",
        "\n",
        "\n",
        "class CIFAR10SSL(datasets.CIFAR10):\n",
        "    def __init__(self, root, indexs, train=True,\n",
        "                 transform=None, target_transform=None,\n",
        "                 download=True, pseudo_idx=None, pseudo_target=None,\n",
        "                 nl_idx=None, nl_mask=None):\n",
        "        super().__init__(root, train=train,\n",
        "                         transform=transform,\n",
        "                         target_transform=target_transform,\n",
        "                         download=download)\n",
        "        \n",
        "        self.targets = np.array(self.targets)\n",
        "        self.nl_mask = np.ones((len(self.targets), len(np.unique(self.targets))))\n",
        "        \n",
        "        if nl_mask is not None:\n",
        "            self.nl_mask[nl_idx] = nl_mask\n",
        "\n",
        "        if pseudo_target is not None:\n",
        "            self.targets[pseudo_idx] = pseudo_target\n",
        "\n",
        "        if indexs is not None:\n",
        "            indexs = np.array(indexs)\n",
        "            self.data = self.data[indexs]\n",
        "            self.targets = np.array(self.targets)[indexs]\n",
        "            self.nl_mask = np.array(self.nl_mask)[indexs]\n",
        "            self.indexs = indexs\n",
        "        else:\n",
        "            self.indexs = np.arange(len(self.targets))\n",
        "        \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.data[index], self.targets[index]\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return img, target, self.indexs[index], self.nl_mask[index]\n",
        "\n",
        "\n",
        "class CIFAR100SSL(datasets.CIFAR100):\n",
        "    def __init__(self, root, indexs, train=True,\n",
        "                 transform=None, target_transform=None,\n",
        "                 download=True, pseudo_idx=None, pseudo_target=None,\n",
        "                 nl_idx=None, nl_mask=None):\n",
        "        super().__init__(root, train=train,\n",
        "                         transform=transform,\n",
        "                         target_transform=target_transform,\n",
        "                         download=download)\n",
        "        \n",
        "        self.targets = np.array(self.targets)\n",
        "        self.nl_mask = np.ones((len(self.targets), len(np.unique(self.targets))))\n",
        "        \n",
        "        if nl_mask is not None:\n",
        "            self.nl_mask[nl_idx] = nl_mask\n",
        "\n",
        "        if pseudo_target is not None:\n",
        "            self.targets[pseudo_idx] = pseudo_target\n",
        "\n",
        "        if indexs is not None:\n",
        "            indexs = np.array(indexs)\n",
        "            self.data = self.data[indexs]\n",
        "            self.targets = np.array(self.targets)[indexs]\n",
        "            self.nl_mask = np.array(self.nl_mask)[indexs]\n",
        "            self.indexs = indexs\n",
        "        else:\n",
        "            self.indexs = np.arange(len(self.targets))\n",
        "        \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.data[index], self.targets[index]\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "            \n",
        "        return img, target, self.indexs[index], self.nl_mask[index]"
      ],
      "metadata": {
        "id": "BucWBSrNkRsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# dd\n"
      ],
      "metadata": {
        "id": "3dyxymXS2SlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iteration = 100\n",
        "for itr in range(iteration):"
      ],
      "metadata": {
        "id": "tpKCDEVN18ka"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}