{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YianKim/2022_uncertainty_aware_semisupervise/blob/main/Keras_UPS_SVHN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow_addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiCmW9OvtVfs",
        "outputId": "53ec992d-fc50-4700-8e9e-3c3ccc8ae7d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.16.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 29.7 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 38.0 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 41.6 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 45.0 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61 kB 33.5 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 26.3 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92 kB 30.2 MB/s eta 0:00:01\r\u001b[K     |███                             | 102 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 112 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████                            | 143 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████                           | 174 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 194 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 204 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 225 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 235 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████                         | 245 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 256 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 266 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 276 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 286 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 296 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 307 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 317 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 327 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 337 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 358 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 368 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 378 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 389 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 399 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 409 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 419 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 430 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 440 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 450 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 460 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 471 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 481 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 491 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 501 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 512 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 522 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 532 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 542 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 552 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 563 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 573 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 583 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 593 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 604 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 614 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 624 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 634 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 645 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 655 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 665 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 675 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 686 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 696 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 706 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 716 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 727 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 737 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 747 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 757 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 768 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 778 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 788 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 798 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 808 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 819 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 829 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 839 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 849 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 860 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 870 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 880 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 890 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 901 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 911 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 921 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 931 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 942 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 952 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 962 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 972 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 983 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 993 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.0 MB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.0 MB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.0 MB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.0 MB 32.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.0 MB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1 MB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.1 MB 32.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.1 MB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.1 MB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.1 MB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.1 MB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.1 MB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 32.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.16.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ik7Qx5iO8lQ_"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import clone_model\n",
        "\n",
        "import PIL\n",
        "from PIL import Image\n",
        "\n",
        "import pickle\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "from keras.layers.core import Lambda\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import MaxPooling2D, AveragePooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras import optimizers\n",
        "from keras.callbacks import *\n",
        "from sklearn.metrics import *\n",
        "from keras.models import load_model\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as backend\n",
        "import math\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dummy_labels(labels):\n",
        "  zero_labels = np.zeros([labels.shape[0], 10], np.int8)  \n",
        "  for i in range(labels.shape[0]):\n",
        "    zero_labels[i][labels[i]] = 1\n",
        "  return(zero_labels)"
      ],
      "metadata": {
        "id": "XzWKzGmGtk2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVHN"
      ],
      "metadata": {
        "id": "0Hmq32hTH-Jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCUaBhoYMQHF",
        "outputId": "9f7dd53f-ef33-453e-a53b-081a2fd7be49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.io import loadmat\n",
        "train_raw = loadmat('/content/drive/MyDrive/SVHN/train_32x32.mat')\n",
        "test_raw = loadmat('/content/drive/MyDrive/SVHN/test_32x32.mat')"
      ],
      "metadata": {
        "id": "HUCvct_dH5m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = train_raw['X']\n",
        "train_labels = train_raw['y']\n",
        "\n",
        "test_images = test_raw['X']\n",
        "test_labels = dummy_labels(test_raw['y']-1)\n",
        "\n",
        "train_images = train_images.swapaxes(2,3).swapaxes(1,2).swapaxes(0,1)\n",
        "test_images = test_images.swapaxes(2,3).swapaxes(1,2).swapaxes(0,1)"
      ],
      "metadata": {
        "id": "65QFu6LVMxl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp = [0,0,0,0,0,0,0,0,0,0]\n",
        "label_indx = []\n",
        "unlabel_indx = []\n",
        "\n",
        "for i in range(73257) :\n",
        "  if temp[(train_labels).reshape([-1])[i]-1] < 25 :\n",
        "    temp[(train_labels).reshape([-1])[i]-1] += 1\n",
        "    label_indx.append(i)\n",
        "  else :\n",
        "    unlabel_indx.append(i)"
      ],
      "metadata": {
        "id": "fm_NgimXNonL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lbl_train_images = train_images[label_indx]\n",
        "lbl_train_labels = dummy_labels(train_labels[label_indx]-1)"
      ],
      "metadata": {
        "id": "3ZADJPIIOZD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ubl_train_images = train_images[unlabel_indx]\n",
        "ubl_train_labels = dummy_labels(train_labels[unlabel_indx]-1)"
      ],
      "metadata": {
        "id": "fHe18DTWUu3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqLLHcZLuZmP"
      },
      "source": [
        "# pseudo labeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glpcPC8EsUZl"
      },
      "outputs": [],
      "source": [
        "def basic_augmentation(imagearray):\n",
        "  image = Image.fromarray(imagearray)\n",
        "  \n",
        "  tr2 = transforms.RandomRotation(15)\n",
        "  tr3 = transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2)\n",
        "  image = tr2(tr3(image))\n",
        "  return(np.array(image))\n",
        "\n",
        "def makeaugs(n, input):\n",
        "  augs = []\n",
        "  for j in range(n):\n",
        "    for i in input:\n",
        "      augs.append(basic_augmentation(np.array(i*255, np.uint8)))\n",
        "  return(np.array(augs)/255)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hwy4DFT_BI1i"
      },
      "source": [
        "### Mixup Augmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpmSPXnnBLyT"
      },
      "outputs": [],
      "source": [
        "def sample_beta_distribution(size, concentration_0=0.2, concentration_1=0.2):\n",
        "    gamma_1_sample = tf.random.gamma(shape=[size], alpha=concentration_1)\n",
        "    gamma_2_sample = tf.random.gamma(shape=[size], alpha=concentration_0)\n",
        "    return gamma_1_sample / (gamma_1_sample + gamma_2_sample)\n",
        "\n",
        "def mixup (size, data, alpha = 0.2):\n",
        "  image, label = data\n",
        "  L = sample_beta_distribution(size, alpha, alpha)\n",
        "  XL = tf.reshape(L, (size, 1, 1, 1))\n",
        "  YL = tf.reshape(L, (size, 1))\n",
        "  IND1 = np.random.choice(len(label), size)\n",
        "  IND2 = np.random.choice(len(label), size)\n",
        "  newimage = XL*image[IND1] + (1-XL)*image[IND2]\n",
        "  newlabel = YL*label[IND1] + (1-YL)*label[IND2]\n",
        "  return (newimage, newlabel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewpemDXc8nEY"
      },
      "source": [
        "### 스케줄러"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKyn6Njs7vqa"
      },
      "outputs": [],
      "source": [
        "class SGDR(Callback):\n",
        "\n",
        "    def __init__(self, min_lr=0.0, max_lr=0.03, base_epochs=20, mul_epochs=2):\n",
        "        super(SGDR, self).__init__()\n",
        "\n",
        "        self.min_lr = min_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.base_epochs = base_epochs\n",
        "        self.mul_epochs = mul_epochs\n",
        "\n",
        "        self.cycles = 0.\n",
        "        self.cycle_iterations = 0.\n",
        "        self.trn_iterations = 0.\n",
        "\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self, new_min_lr=None, new_max_lr=None,\n",
        "               new_base_epochs=None, new_mul_epochs=None):\n",
        "        \"\"\"Resets cycle iterations.\"\"\"\n",
        "        \n",
        "        if new_min_lr != None:\n",
        "            self.min_lr = new_min_lr\n",
        "        if new_max_lr != None:\n",
        "            self.max_lr = new_max_lr\n",
        "        if new_base_epochs != None:\n",
        "            self.base_epochs = new_base_epochs\n",
        "        if new_mul_epochs != None:\n",
        "            self.mul_epochs = new_mul_epochs\n",
        "        self.cycles = 0.\n",
        "        self.cycle_iterations = 0.\n",
        "        \n",
        "    def sgdr(self):\n",
        "        \n",
        "        cycle_epochs = self.base_epochs * (self.mul_epochs ** self.cycles)\n",
        "        tide = ((self.cycles == 0) * 1) * (self.cycle_iterations*self.max_lr + (self.base_epochs - self.cycle_iterations)*self.min_lr) / self.base_epochs + ((self.cycles != 0) * 1)*(self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(np.pi * (self.cycle_iterations + 1) / cycle_epochs)))\n",
        "        return tide\n",
        "        \n",
        "    def on_train_begin(self, logs=None):\n",
        "        \n",
        "        if self.cycle_iterations == 0:\n",
        "            K.set_value(self.model.optimizer.lr, self.max_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.sgdr())\n",
        "            \n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        \n",
        "        logs = logs or {}\n",
        "        logs['lr'] = K.get_value(self.model.optimizer.lr)\n",
        "        \n",
        "        self.trn_iterations += 1\n",
        "        self.cycle_iterations += 1\n",
        "        if self.cycle_iterations >= self.base_epochs * (self.mul_epochs ** self.cycles):\n",
        "            self.cycles += 1\n",
        "            self.cycle_iterations = 0\n",
        "            K.set_value(self.model.optimizer.lr, self.max_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.sgdr())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjpoH_dl8qPt"
      },
      "source": [
        "### main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRCDNliwfudJ"
      },
      "outputs": [],
      "source": [
        "def PermaDropout(rate):\n",
        "    return Lambda(lambda x: K.dropout(x, level=rate))\n",
        "\n",
        "def create_cnn_13():\n",
        "  inputlayer = keras.Input(shape=(32, 32, 3))\n",
        "\n",
        "  conv1a = Conv2D(64, (5,5), padding = 'same')\n",
        "  bn1a = BatchNormalization()\n",
        "  conv1b = Conv2D(64, (5,5), padding = 'same')\n",
        "  bn1b = BatchNormalization()\n",
        "  conv1c = Conv2D(64, (5,5), padding = 'same')\n",
        "  bn1c = BatchNormalization()\n",
        "  pl1 = MaxPooling2D(2, 2)\n",
        "  MCdrop1 = PermaDropout(0.5)\n",
        "\n",
        "  fc1 = Dense(1024)\n",
        "  fc2 = Dense(10)\n",
        "  activ = keras.layers.LeakyReLU(0.1)\n",
        "\n",
        "  model = Sequential([\n",
        "                  inputlayer, \n",
        "                  tfa.layers.WeightNormalization(conv1a), bn1a, activ, pl1,\n",
        "                  tfa.layers.WeightNormalization(conv1b), bn1b, activ, pl1,\n",
        "                  tfa.layers.WeightNormalization(conv1c), bn1c, activ, pl1,\n",
        "                  MCdrop1, Flatten(),\n",
        "\n",
        "                  fc1, fc2\n",
        "                  ])\n",
        "  \n",
        "  return model\n",
        "\n",
        "def compile_cnn_13(model):\n",
        "\n",
        "  opt = keras.optimizers.SGD(0.03, momentum=0.9)\n",
        "\n",
        "  model.compile(\n",
        "    optimizer = opt,\n",
        "    loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        "  )\n",
        "\n",
        "  return model\n",
        "\n",
        "def cnn_13():\n",
        "\n",
        "  model = create_cnn_13()\n",
        "  model = compile_cnn_13(model)\n",
        "\n",
        "  return model\n",
        "\n",
        "def fit_and_labeling_cnn_13(Epoch, Batch):\n",
        "\n",
        "  X = lbl_train_images\n",
        "  y = lbl_train_labels\n",
        "\n",
        "  lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
        "  early_stopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=30, mode='auto')\n",
        "  sgdr = SGDR(min_lr=0.0, max_lr=0.03, base_epochs=20) #스케줄러\n",
        "  \n",
        "  # size = len(y) * 3\n",
        "  # newimage, newlabel = mixup(size, (X, y))\n",
        "  augimage, auglabel = makeaugs(3, X), np.concatenate((y,y,y))\n",
        "  X = np.concatenate((X, augimage))\n",
        "  y = np.concatenate((y, auglabel))\n",
        "  # del newimage, newlabel\n",
        "\n",
        "  model.fit(\n",
        "      x=X,\n",
        "      y=y,\n",
        "      epochs=Epoch,\n",
        "      verbose=0,\n",
        "#       validation_data = (valids1, valids2),\n",
        "      batch_size=Batch,\n",
        "#       callbacks=[sgdr, early_stopper]\n",
        "      callbacks=[sgdr]\n",
        "  )\n",
        "    \n",
        "  model_test_eval(model, test_images, test_labels)\n",
        "  T = 1\n",
        "\n",
        "  for predsamples in (range(10)):\n",
        "    if predsamples == 0 :\n",
        "      predictions = np.array(tf.nn.softmax(model.predict(ubl_train_images)/T))\n",
        "      predictions = predictions.reshape((1,) + predictions.shape)\n",
        "    else:\n",
        "      pred = np.array(tf.nn.softmax(model.predict(ubl_train_images)/T))\n",
        "      pred = pred.reshape((1,) + pred.shape)\n",
        "      predictions = np.concatenate((predictions, pred))\n",
        "\n",
        "  return predictions\n",
        "\n",
        "def model_test_eval(model, test_images, test_labels):\n",
        "  T = 1\n",
        "  pred = np.array(tf.nn.softmax(model.predict(test_images)/T))\n",
        "  for i in range(1,10):\n",
        "    pred += np.array(tf.nn.softmax(model.predict(test_images)))\n",
        "  acc = (np.argmax(pred,axis=1) == np.argmax(test_labels,axis=1))*1\n",
        "  acc = sum(acc)/len(acc)\n",
        "  print(\"test set 성능 : \" + str(acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEBsyUAAg-W2"
      },
      "outputs": [],
      "source": [
        "def label_selecting():\n",
        "  K_conf = 0.9\n",
        "  K_uncert = 0.05\n",
        "\n",
        "  pseudo = np.argmax(np.mean(predictions, axis=0), axis=1)\n",
        "  conf = np.max(np.mean(predictions, axis=0), axis=1)\n",
        "  uncert = np.std(predictions, axis=0)\n",
        "  uncert = np.array([uncert[i][pseudo[i]] for i in range(len(pseudo))])\n",
        "\n",
        "  select_pseudo = (1*(conf > K_conf)) * (1*(uncert < K_uncert))\n",
        "\n",
        "  labels = []\n",
        "  for i in pseudo:\n",
        "    temp = [0,0,0,0,0,0,0,0,0,0]\n",
        "    temp[i] = 1\n",
        "    labels.append(temp)\n",
        "  pseudo = np.array(labels)\n",
        "#   pseudo = np.mean(predictions, axis=0)\n",
        "\n",
        "  lbl_idx = []\n",
        "  ubl_idx = []\n",
        "  k = 0\n",
        "  for i in select_pseudo:\n",
        "    if i == 1:\n",
        "      lbl_idx.append(k)\n",
        "    if i == 0:\n",
        "      ubl_idx.append(k)\n",
        "    k += 1\n",
        "\n",
        "    \n",
        "  ubl_append = ubl_train_images[lbl_idx]\n",
        "  pseudo_append = pseudo[lbl_idx]\n",
        "    \n",
        "  if itr < 20:\n",
        "      try: \n",
        "        numsamples = np.min(list(Counter(np.argmax(pseudo_append, axis=1)).values()))\n",
        "      except:\n",
        "        numsamples = 0\n",
        "      multlabel = np.argmax(pseudo_append, axis=1)\n",
        "      sufindx = random.sample(range(len(multlabel)), len(multlabel))\n",
        "\n",
        "      idxcounter = [0,0,0,0,0,0,0,0,0,0]\n",
        "      idxsample = []\n",
        "\n",
        "      for i in sufindx:\n",
        "#         if idxcounter[multlabel[i]] < numsamples+25:\n",
        "        if idxcounter[multlabel[i]] < min(500, numsamples):\n",
        "          idxcounter[multlabel[i]] += 1\n",
        "          idxsample.append(i)\n",
        "      \n",
        "      image1 = np.concatenate((lbl_train_images, ubl_append[idxsample]))\n",
        "      label1 = np.concatenate((lbl_train_labels, pseudo_append[idxsample]))\n",
        "      image2 = np.concatenate((ubl_train_images[ubl_idx], ubl_append[np.delete(list(range(len(ubl_append))), idxsample)]))\n",
        "  \n",
        "  else:\n",
        "      image1 = np.concatenate((lbl_train_images, ubl_append))\n",
        "      label1 = np.concatenate((lbl_train_labels, pseudo_append))\n",
        "      image2 = ubl_train_images[ubl_idx]\n",
        "\n",
        "  return image1, label1, image2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGe5AxpTsUZp",
        "outputId": "3fcdb692-2e15-4676-8433-fc159fd9d16d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({0: 25, 8: 25, 1: 25, 2: 25, 4: 25, 7: 25, 6: 25, 3: 25, 5: 25, 9: 25})\n",
            "test set 성능 : 0.4957744314689613\n",
            "time : 181.35410523414612\n",
            "Counter({0: 70, 8: 70, 1: 70, 2: 70, 4: 70, 7: 70, 6: 70, 3: 70, 5: 70, 9: 70})\n",
            "test set 성능 : 0.5515135218192994\n",
            "time : 442.6598792076111\n",
            "Counter({0: 223, 8: 223, 1: 223, 2: 223, 4: 223, 7: 223, 6: 223, 3: 223, 5: 223, 9: 223})\n",
            "test set 성능 : 0.5803242163491088\n",
            "time : 950.1974520683289\n",
            "Counter({0: 423, 8: 423, 1: 423, 2: 423, 4: 423, 7: 423, 6: 423, 3: 423, 5: 423, 9: 423})\n",
            "test set 성능 : 0.6161263060848187\n",
            "time : 1781.8189961910248\n",
            "Counter({0: 690, 8: 690, 1: 690, 2: 690, 4: 690, 7: 690, 6: 690, 3: 690, 5: 690, 9: 690})\n",
            "test set 성능 : 0.6354102642901045\n",
            "time : 3076.934863805771\n",
            "Counter({0: 817, 8: 817, 1: 817, 2: 817, 4: 817, 7: 817, 6: 817, 3: 817, 5: 817, 9: 817})\n",
            "test set 성능 : 0.6324139520590043\n",
            "time : 4560.2981333732605\n",
            "Counter({0: 901, 8: 901, 1: 901, 2: 901, 4: 901, 7: 901, 6: 901, 3: 901, 5: 901, 9: 901})\n",
            "test set 성능 : 0.6364858635525507\n",
            "time : 6274.34410572052\n",
            "Counter({0: 991, 8: 991, 1: 991, 2: 991, 4: 991, 7: 991, 6: 991, 3: 991, 5: 991, 9: 991})\n",
            "test set 성능 : 0.6480101413644745\n",
            "time : 8118.372513771057\n",
            "Counter({0: 1111, 8: 1111, 1: 1111, 2: 1111, 4: 1111, 7: 1111, 6: 1111, 3: 1111, 5: 1111, 9: 1111})\n",
            "test set 성능 : 0.6449370006146281\n",
            "time : 10132.93713593483\n",
            "Counter({0: 1233, 8: 1233, 1: 1233, 2: 1233, 4: 1233, 7: 1233, 6: 1233, 3: 1233, 5: 1233, 9: 1233})\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "\n",
        "for itr in range(20):\n",
        "  model = cnn_13()\n",
        "  print(Counter(np.argmax(lbl_train_labels, axis=1)))\n",
        "  predictions = fit_and_labeling_cnn_13(200, 64)\n",
        "  lbl_train_images, lbl_train_labels, ubl_train_images = label_selecting()\n",
        "  del predictions\n",
        "#   teacher_model = model\n",
        "  gc.collect()\n",
        "  print(\"time :\", time.time() - start)\n",
        "\n",
        "print(\"time :\", time.time() - start)\n",
        "# CNN-13(update) + mixup augmentation(x4) + itr10balancing < 0.64\n",
        "# CNN-13(update) + mixup augmentation(x8) + itr5balancing < 0.68?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Edf_8P0osUZp"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Keras_UPS_SVHN.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}