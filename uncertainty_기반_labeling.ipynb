{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YianKim/2022_uncertainty_aware_semisupervise/blob/main/uncertainty_%EA%B8%B0%EB%B0%98_labeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FQpjWg4Ld6m"
      },
      "source": [
        "## 구글 드라이브\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyTEuhg7fR9P",
        "outputId": "893d7f34-68aa-46ed-94a4-c9bb089122ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQy7KUb6NBs-"
      },
      "source": [
        "## 라이브러리 불러오기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XRPQRtXGmm2W"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import PIL\n",
        "import pickle\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "from keras.layers.core import Lambda\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import LSTM\n",
        "from keras import optimizers\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard, ModelCheckpoint\n",
        "from sklearn.metrics import *\n",
        "from keras.models import load_model\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpAjYytuPDaU"
      },
      "source": [
        "## 데이터 불러오기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "e6b1qO5dSAyP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11ebe977-a7a9-4217-8f41-891a657599da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(data, labels), (test_data, test_labels) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "B-e24WMOTQ4d"
      },
      "outputs": [],
      "source": [
        "train_data = data[range(10000)].reshape([10000,28,28,1])\n",
        "train_labels = labels[range(10000)].reshape([10000,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EhhKskDITIEm"
      },
      "outputs": [],
      "source": [
        "unlab_data = data[range(10000,60000)].reshape([50000,28,28,1])\n",
        "unlab_labels = labels[range(10000,60000)].reshape([50000,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "W5aNLvjxVUq0"
      },
      "outputs": [],
      "source": [
        "test_data = test_data.reshape([10000,28,28,1])\n",
        "test_labels = test_labels.reshape([10000,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2K2wXxMmUxRV"
      },
      "outputs": [],
      "source": [
        "train_labels2 = []\n",
        "unlab_labels2 = []\n",
        "test_labels2 = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8c9LL8JVThd-"
      },
      "outputs": [],
      "source": [
        "for i in range(10000):\n",
        "  white = [0,0,0,0,0,0,0,0,0,0]\n",
        "  white[train_labels[i][0]] = 1\n",
        "  train_labels2.append(white)\n",
        "\n",
        "for i in range(50000):\n",
        "  white = [0,0,0,0,0,0,0,0,0,0]\n",
        "  white[unlab_labels[i][0]] = 1\n",
        "  unlab_labels2.append(white)\n",
        "\n",
        "for i in range(10000):\n",
        "  white = [0,0,0,0,0,0,0,0,0,0]\n",
        "  white[test_labels[i][0]] = 1\n",
        "  test_labels2.append(white)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iULFrchUU5nt"
      },
      "outputs": [],
      "source": [
        "train_labels = np.array(train_labels2)\n",
        "unlab_labels = np.array(unlab_labels2)\n",
        "test_labels = np.array(test_labels2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa7KYMfrkKU5"
      },
      "source": [
        "## CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zn3v_W8wJQ68"
      },
      "outputs": [],
      "source": [
        "def PermaDropout(rate):\n",
        "    return Lambda(lambda x: K.dropout(x, level=rate))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5scE9yKkIZb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd502ca2-707a-4f7b-94d6-04e54e8f53b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2000\n",
            "110/110 [==============================] - 1s 9ms/step - loss: 2.7861 - accuracy: 0.6353 - val_loss: 0.8022 - val_accuracy: 0.7300 - lr: 0.0010\n",
            "Epoch 2/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.6622 - accuracy: 0.7697 - val_loss: 0.6885 - val_accuracy: 0.7657 - lr: 0.0010\n",
            "Epoch 3/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.5660 - accuracy: 0.7983 - val_loss: 0.6609 - val_accuracy: 0.7703 - lr: 0.0010\n",
            "Epoch 4/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.5037 - accuracy: 0.8180 - val_loss: 0.6342 - val_accuracy: 0.7733 - lr: 0.0010\n",
            "Epoch 5/2000\n",
            "110/110 [==============================] - 1s 9ms/step - loss: 0.4535 - accuracy: 0.8319 - val_loss: 0.6077 - val_accuracy: 0.7847 - lr: 0.0010\n",
            "Epoch 6/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.4147 - accuracy: 0.8441 - val_loss: 0.6093 - val_accuracy: 0.7960 - lr: 0.0010\n",
            "Epoch 7/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.3875 - accuracy: 0.8536 - val_loss: 0.5940 - val_accuracy: 0.8060 - lr: 0.0010\n",
            "Epoch 8/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.3651 - accuracy: 0.8620 - val_loss: 0.6273 - val_accuracy: 0.7880 - lr: 0.0010\n",
            "Epoch 9/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.3484 - accuracy: 0.8674 - val_loss: 0.5626 - val_accuracy: 0.8103 - lr: 0.0010\n",
            "Epoch 10/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.3273 - accuracy: 0.8746 - val_loss: 0.5927 - val_accuracy: 0.8113 - lr: 0.0010\n",
            "Epoch 11/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.3090 - accuracy: 0.8860 - val_loss: 0.5859 - val_accuracy: 0.8240 - lr: 0.0010\n",
            "Epoch 12/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.3131 - accuracy: 0.8794 - val_loss: 0.5742 - val_accuracy: 0.8090 - lr: 0.0010\n",
            "Epoch 13/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.2778 - accuracy: 0.8929 - val_loss: 0.5907 - val_accuracy: 0.8323 - lr: 9.0000e-04\n",
            "Epoch 14/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.2729 - accuracy: 0.8961 - val_loss: 0.5712 - val_accuracy: 0.8227 - lr: 9.0000e-04\n",
            "Epoch 15/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.2541 - accuracy: 0.9053 - val_loss: 0.5767 - val_accuracy: 0.8237 - lr: 9.0000e-04\n",
            "Epoch 16/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.2243 - accuracy: 0.9087 - val_loss: 0.5890 - val_accuracy: 0.8267 - lr: 8.1000e-04\n",
            "Epoch 17/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.2309 - accuracy: 0.9153 - val_loss: 0.5225 - val_accuracy: 0.8367 - lr: 8.1000e-04\n",
            "Epoch 18/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.2139 - accuracy: 0.9154 - val_loss: 0.6367 - val_accuracy: 0.8207 - lr: 8.1000e-04\n",
            "Epoch 19/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.2108 - accuracy: 0.9201 - val_loss: 0.6230 - val_accuracy: 0.8290 - lr: 8.1000e-04\n",
            "Epoch 20/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.2063 - accuracy: 0.9216 - val_loss: 0.5511 - val_accuracy: 0.8253 - lr: 8.1000e-04\n",
            "Epoch 21/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.1996 - accuracy: 0.9220 - val_loss: 0.6019 - val_accuracy: 0.8410 - lr: 7.2900e-04\n",
            "Epoch 22/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.1856 - accuracy: 0.9277 - val_loss: 0.6083 - val_accuracy: 0.8363 - lr: 7.2900e-04\n",
            "Epoch 23/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.1837 - accuracy: 0.9327 - val_loss: 0.5663 - val_accuracy: 0.8403 - lr: 7.2900e-04\n",
            "Epoch 24/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.1730 - accuracy: 0.9323 - val_loss: 0.6454 - val_accuracy: 0.8340 - lr: 6.5610e-04\n",
            "Epoch 25/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.1771 - accuracy: 0.9337 - val_loss: 0.5955 - val_accuracy: 0.8400 - lr: 6.5610e-04\n",
            "Epoch 26/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.1683 - accuracy: 0.9359 - val_loss: 0.6574 - val_accuracy: 0.8490 - lr: 6.5610e-04\n",
            "Epoch 27/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.1513 - accuracy: 0.9437 - val_loss: 0.5983 - val_accuracy: 0.8420 - lr: 5.9049e-04\n",
            "Epoch 28/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.1570 - accuracy: 0.9377 - val_loss: 0.5854 - val_accuracy: 0.8427 - lr: 5.9049e-04\n",
            "Epoch 29/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.1430 - accuracy: 0.9453 - val_loss: 0.6304 - val_accuracy: 0.8423 - lr: 5.9049e-04\n",
            "Epoch 30/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.1546 - accuracy: 0.9417 - val_loss: 0.6161 - val_accuracy: 0.8453 - lr: 5.3144e-04\n",
            "Epoch 31/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.1304 - accuracy: 0.9476 - val_loss: 0.6837 - val_accuracy: 0.8493 - lr: 5.3144e-04\n",
            "Epoch 32/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.1324 - accuracy: 0.9506 - val_loss: 0.6736 - val_accuracy: 0.8417 - lr: 5.3144e-04\n",
            "Epoch 33/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.1230 - accuracy: 0.9506 - val_loss: 0.6286 - val_accuracy: 0.8537 - lr: 4.7830e-04\n",
            "Epoch 34/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.1200 - accuracy: 0.9549 - val_loss: 0.6975 - val_accuracy: 0.8487 - lr: 4.7830e-04\n",
            "Epoch 35/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.1096 - accuracy: 0.9594 - val_loss: 0.6967 - val_accuracy: 0.8410 - lr: 4.7830e-04\n",
            "Epoch 36/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.1157 - accuracy: 0.9544 - val_loss: 0.6762 - val_accuracy: 0.8450 - lr: 4.3047e-04\n",
            "Epoch 37/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.1123 - accuracy: 0.9571 - val_loss: 0.6750 - val_accuracy: 0.8483 - lr: 4.3047e-04\n",
            "Epoch 38/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.1140 - accuracy: 0.9581 - val_loss: 0.6487 - val_accuracy: 0.8533 - lr: 4.3047e-04\n",
            "Epoch 39/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.1128 - accuracy: 0.9583 - val_loss: 0.7209 - val_accuracy: 0.8430 - lr: 3.8742e-04\n",
            "Epoch 40/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.0980 - accuracy: 0.9617 - val_loss: 0.6998 - val_accuracy: 0.8470 - lr: 3.8742e-04\n",
            "Epoch 41/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.1133 - accuracy: 0.9594 - val_loss: 0.6909 - val_accuracy: 0.8463 - lr: 3.8742e-04\n",
            "Epoch 42/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0996 - accuracy: 0.9633 - val_loss: 0.6437 - val_accuracy: 0.8487 - lr: 3.4868e-04\n",
            "Epoch 43/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.0894 - accuracy: 0.9647 - val_loss: 0.7056 - val_accuracy: 0.8480 - lr: 3.4868e-04\n",
            "Epoch 44/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0814 - accuracy: 0.9701 - val_loss: 0.7521 - val_accuracy: 0.8453 - lr: 3.4868e-04\n",
            "Epoch 45/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0893 - accuracy: 0.9651 - val_loss: 0.7849 - val_accuracy: 0.8517 - lr: 3.1381e-04\n",
            "Epoch 46/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0838 - accuracy: 0.9679 - val_loss: 0.7041 - val_accuracy: 0.8613 - lr: 3.1381e-04\n",
            "Epoch 47/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0802 - accuracy: 0.9704 - val_loss: 0.7954 - val_accuracy: 0.8493 - lr: 3.1381e-04\n",
            "Epoch 48/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0749 - accuracy: 0.9723 - val_loss: 0.7951 - val_accuracy: 0.8473 - lr: 2.8243e-04\n",
            "Epoch 49/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0765 - accuracy: 0.9710 - val_loss: 0.7604 - val_accuracy: 0.8540 - lr: 2.8243e-04\n",
            "Epoch 50/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0735 - accuracy: 0.9710 - val_loss: 0.8436 - val_accuracy: 0.8450 - lr: 2.8243e-04\n",
            "Epoch 51/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0710 - accuracy: 0.9734 - val_loss: 0.7736 - val_accuracy: 0.8533 - lr: 2.5419e-04\n",
            "Epoch 52/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0754 - accuracy: 0.9733 - val_loss: 0.7376 - val_accuracy: 0.8623 - lr: 2.5419e-04\n",
            "Epoch 53/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0749 - accuracy: 0.9721 - val_loss: 0.7772 - val_accuracy: 0.8520 - lr: 2.5419e-04\n",
            "Epoch 54/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0674 - accuracy: 0.9726 - val_loss: 0.7939 - val_accuracy: 0.8547 - lr: 2.2877e-04\n",
            "Epoch 55/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.0652 - accuracy: 0.9763 - val_loss: 0.7681 - val_accuracy: 0.8520 - lr: 2.2877e-04\n",
            "Epoch 56/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0588 - accuracy: 0.9763 - val_loss: 0.8275 - val_accuracy: 0.8483 - lr: 2.2877e-04\n",
            "Epoch 57/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0555 - accuracy: 0.9789 - val_loss: 0.7841 - val_accuracy: 0.8540 - lr: 2.0589e-04\n",
            "Epoch 58/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0637 - accuracy: 0.9756 - val_loss: 0.7394 - val_accuracy: 0.8577 - lr: 2.0589e-04\n",
            "Epoch 59/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.0688 - accuracy: 0.9726 - val_loss: 0.7667 - val_accuracy: 0.8583 - lr: 2.0589e-04\n",
            "Epoch 60/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.0619 - accuracy: 0.9764 - val_loss: 0.7992 - val_accuracy: 0.8530 - lr: 1.8530e-04\n",
            "Epoch 61/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0554 - accuracy: 0.9791 - val_loss: 0.8148 - val_accuracy: 0.8540 - lr: 1.8530e-04\n",
            "Epoch 62/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.0569 - accuracy: 0.9784 - val_loss: 0.8313 - val_accuracy: 0.8507 - lr: 1.8530e-04\n",
            "Epoch 63/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.0588 - accuracy: 0.9761 - val_loss: 0.8346 - val_accuracy: 0.8527 - lr: 1.6677e-04\n",
            "Epoch 64/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.0566 - accuracy: 0.9790 - val_loss: 0.8305 - val_accuracy: 0.8557 - lr: 1.6677e-04\n",
            "Epoch 65/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0587 - accuracy: 0.9784 - val_loss: 0.8132 - val_accuracy: 0.8493 - lr: 1.6677e-04\n",
            "Epoch 66/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0503 - accuracy: 0.9817 - val_loss: 0.7835 - val_accuracy: 0.8600 - lr: 1.5009e-04\n",
            "Epoch 67/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0503 - accuracy: 0.9811 - val_loss: 0.8349 - val_accuracy: 0.8570 - lr: 1.5009e-04\n",
            "Epoch 68/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0479 - accuracy: 0.9830 - val_loss: 0.8480 - val_accuracy: 0.8530 - lr: 1.5009e-04\n",
            "Epoch 69/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.0548 - accuracy: 0.9781 - val_loss: 0.7804 - val_accuracy: 0.8563 - lr: 1.3509e-04\n",
            "Epoch 70/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0458 - accuracy: 0.9814 - val_loss: 0.8095 - val_accuracy: 0.8593 - lr: 1.3509e-04\n",
            "Epoch 71/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.0464 - accuracy: 0.9817 - val_loss: 0.8521 - val_accuracy: 0.8570 - lr: 1.3509e-04\n",
            "Epoch 72/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0464 - accuracy: 0.9819 - val_loss: 0.8642 - val_accuracy: 0.8537 - lr: 1.2158e-04\n",
            "Epoch 73/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.0440 - accuracy: 0.9844 - val_loss: 0.8261 - val_accuracy: 0.8647 - lr: 1.2158e-04\n",
            "Epoch 74/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.0511 - accuracy: 0.9817 - val_loss: 0.8611 - val_accuracy: 0.8500 - lr: 1.2158e-04\n",
            "Epoch 75/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0419 - accuracy: 0.9844 - val_loss: 0.8675 - val_accuracy: 0.8560 - lr: 1.0942e-04\n",
            "Epoch 76/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0467 - accuracy: 0.9813 - val_loss: 0.7556 - val_accuracy: 0.8637 - lr: 1.0942e-04\n",
            "Epoch 77/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0461 - accuracy: 0.9824 - val_loss: 0.8812 - val_accuracy: 0.8587 - lr: 1.0942e-04\n",
            "Epoch 78/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.0448 - accuracy: 0.9831 - val_loss: 0.8422 - val_accuracy: 0.8577 - lr: 9.8477e-05\n",
            "Epoch 79/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0473 - accuracy: 0.9831 - val_loss: 0.8772 - val_accuracy: 0.8567 - lr: 9.8477e-05\n",
            "Epoch 80/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0464 - accuracy: 0.9819 - val_loss: 0.8125 - val_accuracy: 0.8683 - lr: 9.8477e-05\n",
            "Epoch 81/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.0421 - accuracy: 0.9843 - val_loss: 0.7982 - val_accuracy: 0.8613 - lr: 8.8629e-05\n",
            "Epoch 82/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.0397 - accuracy: 0.9843 - val_loss: 0.8793 - val_accuracy: 0.8537 - lr: 8.8629e-05\n",
            "Epoch 83/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0392 - accuracy: 0.9866 - val_loss: 0.8348 - val_accuracy: 0.8583 - lr: 8.8629e-05\n",
            "Epoch 84/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0415 - accuracy: 0.9854 - val_loss: 0.8629 - val_accuracy: 0.8613 - lr: 7.9766e-05\n",
            "Epoch 85/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0416 - accuracy: 0.9847 - val_loss: 0.7607 - val_accuracy: 0.8537 - lr: 7.9766e-05\n",
            "Epoch 86/2000\n",
            "110/110 [==============================] - 1s 10ms/step - loss: 0.0386 - accuracy: 0.9867 - val_loss: 0.8730 - val_accuracy: 0.8593 - lr: 7.9766e-05\n",
            "Epoch 87/2000\n",
            "110/110 [==============================] - 1s 10ms/step - loss: 0.0358 - accuracy: 0.9877 - val_loss: 0.8336 - val_accuracy: 0.8617 - lr: 7.1790e-05\n",
            "Epoch 88/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0379 - accuracy: 0.9850 - val_loss: 0.8536 - val_accuracy: 0.8673 - lr: 7.1790e-05\n",
            "Epoch 89/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.0369 - accuracy: 0.9847 - val_loss: 0.7964 - val_accuracy: 0.8667 - lr: 7.1790e-05\n",
            "Epoch 90/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0386 - accuracy: 0.9860 - val_loss: 0.8580 - val_accuracy: 0.8650 - lr: 6.4611e-05\n",
            "Epoch 91/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0366 - accuracy: 0.9880 - val_loss: 0.8496 - val_accuracy: 0.8610 - lr: 6.4611e-05\n",
            "Epoch 92/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.0318 - accuracy: 0.9896 - val_loss: 0.9588 - val_accuracy: 0.8627 - lr: 6.4611e-05\n",
            "Epoch 93/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.0368 - accuracy: 0.9850 - val_loss: 0.8527 - val_accuracy: 0.8667 - lr: 5.8150e-05\n",
            "Epoch 94/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.0384 - accuracy: 0.9853 - val_loss: 0.8806 - val_accuracy: 0.8647 - lr: 5.8150e-05\n",
            "Epoch 95/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0383 - accuracy: 0.9863 - val_loss: 0.9311 - val_accuracy: 0.8583 - lr: 5.8150e-05\n",
            "Epoch 96/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0376 - accuracy: 0.9851 - val_loss: 0.8524 - val_accuracy: 0.8587 - lr: 5.2335e-05\n",
            "Epoch 97/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.0403 - accuracy: 0.9851 - val_loss: 0.9141 - val_accuracy: 0.8583 - lr: 5.2335e-05\n",
            "Epoch 98/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0370 - accuracy: 0.9871 - val_loss: 0.8291 - val_accuracy: 0.8630 - lr: 5.2335e-05\n",
            "Epoch 99/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.0361 - accuracy: 0.9844 - val_loss: 0.8712 - val_accuracy: 0.8617 - lr: 4.7101e-05\n",
            "Epoch 100/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.0341 - accuracy: 0.9854 - val_loss: 0.8790 - val_accuracy: 0.8673 - lr: 4.7101e-05\n",
            "Epoch 101/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.0351 - accuracy: 0.9876 - val_loss: 0.8558 - val_accuracy: 0.8627 - lr: 4.7101e-05\n",
            "Epoch 102/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.0334 - accuracy: 0.9873 - val_loss: 0.8305 - val_accuracy: 0.8610 - lr: 4.2391e-05\n",
            "Epoch 103/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.0352 - accuracy: 0.9880 - val_loss: 0.8511 - val_accuracy: 0.8587 - lr: 4.2391e-05\n",
            "Epoch 104/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.0320 - accuracy: 0.9881 - val_loss: 0.8320 - val_accuracy: 0.8617 - lr: 4.2391e-05\n",
            "Epoch 105/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.0348 - accuracy: 0.9866 - val_loss: 0.9001 - val_accuracy: 0.8563 - lr: 3.8152e-05\n",
            "Epoch 106/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0304 - accuracy: 0.9894 - val_loss: 0.8651 - val_accuracy: 0.8577 - lr: 3.8152e-05\n",
            "Epoch 107/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0339 - accuracy: 0.9874 - val_loss: 0.8657 - val_accuracy: 0.8670 - lr: 3.8152e-05\n",
            "Epoch 108/2000\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 0.0333 - accuracy: 0.9869 - val_loss: 0.8706 - val_accuracy: 0.8670 - lr: 3.4337e-05\n",
            "Epoch 109/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.0294 - accuracy: 0.9887 - val_loss: 0.9148 - val_accuracy: 0.8603 - lr: 3.4337e-05\n",
            "Epoch 110/2000\n",
            "110/110 [==============================] - 1s 7ms/step - loss: 0.0276 - accuracy: 0.9903 - val_loss: 0.8359 - val_accuracy: 0.8567 - lr: 3.4337e-05\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0f703c9d10>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "#######HYPERPARAMATERS###########\n",
        "epochs = 2000\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "#################################\n",
        "  \n",
        "model = Sequential()\n",
        "    \n",
        "model.add(Conv2D(32, kernel_size = (3,3), input_shape=(28,28,1), activation='relu'))\n",
        "model.add(Conv2D(64,(3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(PermaDropout(0.5))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(PermaDropout(0.5))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "adam = keras.optimizers.Adam(learning_rate)\n",
        "model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    \n",
        "# print(model.summary())\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
        "early_stopper = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=30, mode='auto')\n",
        "checkpointer = ModelCheckpoint('weights.hd5', monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "model.fit(\n",
        "          train_data,\n",
        "          train_labels,\n",
        "          epochs = epochs,\n",
        "          batch_size = batch_size,\n",
        "          validation_split = 0.3,\n",
        "          shuffle = True,\n",
        "          callbacks=[lr_reducer, early_stopper]\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVOc3hYIkXrl"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/MyDrive/파이썬스터디 프로젝트/labeling_cnn.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5D8i_BCkdjS"
      },
      "outputs": [],
      "source": [
        "model = keras.models.load_model('/content/drive/MyDrive/파이썬스터디 프로젝트/labeling_cnn.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## resnet"
      ],
      "metadata": {
        "id": "hIAa0HMvS5CT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import Tensor\n",
        "from tensorflow.keras.layers import Input, Conv2D, ReLU, BatchNormalization,\\\n",
        "                                    Add, AveragePooling2D, Flatten, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def PermaDropout(rate):\n",
        "    return Lambda(lambda x: K.dropout(x, level=rate))\n",
        "\n",
        "def relu_bn(inputs: Tensor) -> Tensor:\n",
        "    relu = ReLU()(inputs)\n",
        "    bn = BatchNormalization()(relu)\n",
        "    return bn\n",
        "\n",
        "def residual_block(x: Tensor, downsample: bool, filters: int, kernel_size: int = 3) -> Tensor:\n",
        "    y = Conv2D(kernel_size=kernel_size,\n",
        "               strides= (1 if not downsample else 2),\n",
        "               filters=filters,\n",
        "               padding=\"same\")(x)\n",
        "    y = relu_bn(y)\n",
        "    y = Conv2D(kernel_size=kernel_size,\n",
        "               strides=1,\n",
        "               filters=filters,\n",
        "               padding=\"same\")(y)\n",
        "\n",
        "    if downsample:\n",
        "        x = Conv2D(kernel_size=1,\n",
        "                   strides=2,\n",
        "                   filters=filters,\n",
        "                   padding=\"same\")(x)\n",
        "    out = Add()([x, y])\n",
        "    out = relu_bn(out)\n",
        "    return out\n",
        "\n",
        "def create_res_net():\n",
        "    \n",
        "    inputs = Input(shape=(28, 28, 1))\n",
        "    num_filters = 64\n",
        "    \n",
        "    t = BatchNormalization()(inputs)\n",
        "    t = Conv2D(kernel_size=3,\n",
        "               strides=1,\n",
        "               filters=num_filters,\n",
        "               padding=\"same\")(t)\n",
        "    t = relu_bn(t)\n",
        "    t = PermaDropout(0.5)(t)\n",
        "    \n",
        "    num_blocks_list = [2, 5, 5, 2]\n",
        "    for i in range(len(num_blocks_list)):\n",
        "        num_blocks = num_blocks_list[i]\n",
        "        for j in range(num_blocks):\n",
        "            t = residual_block(t, downsample=(j==0 and i!=0), filters=num_filters)\n",
        "        num_filters *= 2\n",
        "        t = PermaDropout(0.5)(t)\n",
        "    \n",
        "    t = AveragePooling2D(4)(t)\n",
        "    t = Flatten()(t)\n",
        "    outputs = Dense(10, activation='softmax')(t)\n",
        "    \n",
        "    model = Model(inputs, outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "P64aH7gGPPTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_res_net() # or create_plain_net()\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NNvVlDJSCt5",
        "outputId": "32c074f5-dd43-4f22-a8ae-8872e126b4a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_8 (InputLayer)           [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " batch_normalization_186 (Batch  (None, 28, 28, 1)   4           ['input_8[0][0]']                \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_197 (Conv2D)            (None, 28, 28, 64)   640         ['batch_normalization_186[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_179 (ReLU)               (None, 28, 28, 64)   0           ['conv2d_197[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_187 (Batch  (None, 28, 28, 64)  256         ['re_lu_179[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " lambda_12 (Lambda)             (None, 28, 28, 64)   0           ['batch_normalization_187[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_198 (Conv2D)            (None, 28, 28, 64)   36928       ['lambda_12[0][0]']              \n",
            "                                                                                                  \n",
            " re_lu_180 (ReLU)               (None, 28, 28, 64)   0           ['conv2d_198[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_188 (Batch  (None, 28, 28, 64)  256         ['re_lu_180[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_199 (Conv2D)            (None, 28, 28, 64)   36928       ['batch_normalization_188[0][0]']\n",
            "                                                                                                  \n",
            " add_86 (Add)                   (None, 28, 28, 64)   0           ['lambda_12[0][0]',              \n",
            "                                                                  'conv2d_199[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_181 (ReLU)               (None, 28, 28, 64)   0           ['add_86[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_189 (Batch  (None, 28, 28, 64)  256         ['re_lu_181[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_200 (Conv2D)            (None, 28, 28, 64)   36928       ['batch_normalization_189[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_182 (ReLU)               (None, 28, 28, 64)   0           ['conv2d_200[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_190 (Batch  (None, 28, 28, 64)  256         ['re_lu_182[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_201 (Conv2D)            (None, 28, 28, 64)   36928       ['batch_normalization_190[0][0]']\n",
            "                                                                                                  \n",
            " add_87 (Add)                   (None, 28, 28, 64)   0           ['batch_normalization_189[0][0]',\n",
            "                                                                  'conv2d_201[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_183 (ReLU)               (None, 28, 28, 64)   0           ['add_87[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_191 (Batch  (None, 28, 28, 64)  256         ['re_lu_183[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " lambda_13 (Lambda)             (None, 28, 28, 64)   0           ['batch_normalization_191[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_202 (Conv2D)            (None, 14, 14, 128)  73856       ['lambda_13[0][0]']              \n",
            "                                                                                                  \n",
            " re_lu_184 (ReLU)               (None, 14, 14, 128)  0           ['conv2d_202[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_192 (Batch  (None, 14, 14, 128)  512        ['re_lu_184[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_204 (Conv2D)            (None, 14, 14, 128)  8320        ['lambda_13[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_203 (Conv2D)            (None, 14, 14, 128)  147584      ['batch_normalization_192[0][0]']\n",
            "                                                                                                  \n",
            " add_88 (Add)                   (None, 14, 14, 128)  0           ['conv2d_204[0][0]',             \n",
            "                                                                  'conv2d_203[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_185 (ReLU)               (None, 14, 14, 128)  0           ['add_88[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_193 (Batch  (None, 14, 14, 128)  512        ['re_lu_185[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_205 (Conv2D)            (None, 14, 14, 128)  147584      ['batch_normalization_193[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_186 (ReLU)               (None, 14, 14, 128)  0           ['conv2d_205[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_194 (Batch  (None, 14, 14, 128)  512        ['re_lu_186[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_206 (Conv2D)            (None, 14, 14, 128)  147584      ['batch_normalization_194[0][0]']\n",
            "                                                                                                  \n",
            " add_89 (Add)                   (None, 14, 14, 128)  0           ['batch_normalization_193[0][0]',\n",
            "                                                                  'conv2d_206[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_187 (ReLU)               (None, 14, 14, 128)  0           ['add_89[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_195 (Batch  (None, 14, 14, 128)  512        ['re_lu_187[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_207 (Conv2D)            (None, 14, 14, 128)  147584      ['batch_normalization_195[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_188 (ReLU)               (None, 14, 14, 128)  0           ['conv2d_207[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_196 (Batch  (None, 14, 14, 128)  512        ['re_lu_188[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_208 (Conv2D)            (None, 14, 14, 128)  147584      ['batch_normalization_196[0][0]']\n",
            "                                                                                                  \n",
            " add_90 (Add)                   (None, 14, 14, 128)  0           ['batch_normalization_195[0][0]',\n",
            "                                                                  'conv2d_208[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_189 (ReLU)               (None, 14, 14, 128)  0           ['add_90[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_197 (Batch  (None, 14, 14, 128)  512        ['re_lu_189[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_209 (Conv2D)            (None, 14, 14, 128)  147584      ['batch_normalization_197[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_190 (ReLU)               (None, 14, 14, 128)  0           ['conv2d_209[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_198 (Batch  (None, 14, 14, 128)  512        ['re_lu_190[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_210 (Conv2D)            (None, 14, 14, 128)  147584      ['batch_normalization_198[0][0]']\n",
            "                                                                                                  \n",
            " add_91 (Add)                   (None, 14, 14, 128)  0           ['batch_normalization_197[0][0]',\n",
            "                                                                  'conv2d_210[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_191 (ReLU)               (None, 14, 14, 128)  0           ['add_91[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_199 (Batch  (None, 14, 14, 128)  512        ['re_lu_191[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_211 (Conv2D)            (None, 14, 14, 128)  147584      ['batch_normalization_199[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_192 (ReLU)               (None, 14, 14, 128)  0           ['conv2d_211[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_200 (Batch  (None, 14, 14, 128)  512        ['re_lu_192[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_212 (Conv2D)            (None, 14, 14, 128)  147584      ['batch_normalization_200[0][0]']\n",
            "                                                                                                  \n",
            " add_92 (Add)                   (None, 14, 14, 128)  0           ['batch_normalization_199[0][0]',\n",
            "                                                                  'conv2d_212[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_193 (ReLU)               (None, 14, 14, 128)  0           ['add_92[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_201 (Batch  (None, 14, 14, 128)  512        ['re_lu_193[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " lambda_14 (Lambda)             (None, 14, 14, 128)  0           ['batch_normalization_201[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_213 (Conv2D)            (None, 7, 7, 256)    295168      ['lambda_14[0][0]']              \n",
            "                                                                                                  \n",
            " re_lu_194 (ReLU)               (None, 7, 7, 256)    0           ['conv2d_213[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_202 (Batch  (None, 7, 7, 256)   1024        ['re_lu_194[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_215 (Conv2D)            (None, 7, 7, 256)    33024       ['lambda_14[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_214 (Conv2D)            (None, 7, 7, 256)    590080      ['batch_normalization_202[0][0]']\n",
            "                                                                                                  \n",
            " add_93 (Add)                   (None, 7, 7, 256)    0           ['conv2d_215[0][0]',             \n",
            "                                                                  'conv2d_214[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_195 (ReLU)               (None, 7, 7, 256)    0           ['add_93[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_203 (Batch  (None, 7, 7, 256)   1024        ['re_lu_195[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_216 (Conv2D)            (None, 7, 7, 256)    590080      ['batch_normalization_203[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_196 (ReLU)               (None, 7, 7, 256)    0           ['conv2d_216[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_204 (Batch  (None, 7, 7, 256)   1024        ['re_lu_196[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_217 (Conv2D)            (None, 7, 7, 256)    590080      ['batch_normalization_204[0][0]']\n",
            "                                                                                                  \n",
            " add_94 (Add)                   (None, 7, 7, 256)    0           ['batch_normalization_203[0][0]',\n",
            "                                                                  'conv2d_217[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_197 (ReLU)               (None, 7, 7, 256)    0           ['add_94[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_205 (Batch  (None, 7, 7, 256)   1024        ['re_lu_197[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_218 (Conv2D)            (None, 7, 7, 256)    590080      ['batch_normalization_205[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_198 (ReLU)               (None, 7, 7, 256)    0           ['conv2d_218[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_206 (Batch  (None, 7, 7, 256)   1024        ['re_lu_198[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_219 (Conv2D)            (None, 7, 7, 256)    590080      ['batch_normalization_206[0][0]']\n",
            "                                                                                                  \n",
            " add_95 (Add)                   (None, 7, 7, 256)    0           ['batch_normalization_205[0][0]',\n",
            "                                                                  'conv2d_219[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_199 (ReLU)               (None, 7, 7, 256)    0           ['add_95[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_207 (Batch  (None, 7, 7, 256)   1024        ['re_lu_199[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_220 (Conv2D)            (None, 7, 7, 256)    590080      ['batch_normalization_207[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_200 (ReLU)               (None, 7, 7, 256)    0           ['conv2d_220[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_208 (Batch  (None, 7, 7, 256)   1024        ['re_lu_200[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_221 (Conv2D)            (None, 7, 7, 256)    590080      ['batch_normalization_208[0][0]']\n",
            "                                                                                                  \n",
            " add_96 (Add)                   (None, 7, 7, 256)    0           ['batch_normalization_207[0][0]',\n",
            "                                                                  'conv2d_221[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_201 (ReLU)               (None, 7, 7, 256)    0           ['add_96[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_209 (Batch  (None, 7, 7, 256)   1024        ['re_lu_201[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_222 (Conv2D)            (None, 7, 7, 256)    590080      ['batch_normalization_209[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_202 (ReLU)               (None, 7, 7, 256)    0           ['conv2d_222[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_210 (Batch  (None, 7, 7, 256)   1024        ['re_lu_202[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_223 (Conv2D)            (None, 7, 7, 256)    590080      ['batch_normalization_210[0][0]']\n",
            "                                                                                                  \n",
            " add_97 (Add)                   (None, 7, 7, 256)    0           ['batch_normalization_209[0][0]',\n",
            "                                                                  'conv2d_223[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_203 (ReLU)               (None, 7, 7, 256)    0           ['add_97[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_211 (Batch  (None, 7, 7, 256)   1024        ['re_lu_203[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " lambda_15 (Lambda)             (None, 7, 7, 256)    0           ['batch_normalization_211[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_224 (Conv2D)            (None, 4, 4, 512)    1180160     ['lambda_15[0][0]']              \n",
            "                                                                                                  \n",
            " re_lu_204 (ReLU)               (None, 4, 4, 512)    0           ['conv2d_224[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_212 (Batch  (None, 4, 4, 512)   2048        ['re_lu_204[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_226 (Conv2D)            (None, 4, 4, 512)    131584      ['lambda_15[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_225 (Conv2D)            (None, 4, 4, 512)    2359808     ['batch_normalization_212[0][0]']\n",
            "                                                                                                  \n",
            " add_98 (Add)                   (None, 4, 4, 512)    0           ['conv2d_226[0][0]',             \n",
            "                                                                  'conv2d_225[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_205 (ReLU)               (None, 4, 4, 512)    0           ['add_98[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_213 (Batch  (None, 4, 4, 512)   2048        ['re_lu_205[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_227 (Conv2D)            (None, 4, 4, 512)    2359808     ['batch_normalization_213[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_206 (ReLU)               (None, 4, 4, 512)    0           ['conv2d_227[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_214 (Batch  (None, 4, 4, 512)   2048        ['re_lu_206[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_228 (Conv2D)            (None, 4, 4, 512)    2359808     ['batch_normalization_214[0][0]']\n",
            "                                                                                                  \n",
            " add_99 (Add)                   (None, 4, 4, 512)    0           ['batch_normalization_213[0][0]',\n",
            "                                                                  'conv2d_228[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_207 (ReLU)               (None, 4, 4, 512)    0           ['add_99[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_215 (Batch  (None, 4, 4, 512)   2048        ['re_lu_207[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " lambda_16 (Lambda)             (None, 4, 4, 512)    0           ['batch_normalization_215[0][0]']\n",
            "                                                                                                  \n",
            " average_pooling2d_6 (AveragePo  (None, 1, 1, 512)   0           ['lambda_16[0][0]']              \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " flatten_6 (Flatten)            (None, 512)          0           ['average_pooling2d_6[0][0]']    \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 10)           5130        ['flatten_6[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 15,618,830\n",
            "Trainable params: 15,606,412\n",
            "Non-trainable params: 12,418\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
        "early_stopper = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=30, mode='auto')\n",
        "\n",
        "model.fit(\n",
        "    x=train_data,\n",
        "    y=train_labels,\n",
        "    epochs=500,\n",
        "    verbose=1,\n",
        "    validation_split=0.3,\n",
        "    batch_size=64,\n",
        "    callbacks=[lr_reducer, early_stopper]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwkScUQtSJfJ",
        "outputId": "56ee06a0-7ecb-414e-d73f-bfb16bdf562a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "110/110 [==============================] - 33s 236ms/step - loss: 1.2659 - accuracy: 0.5479 - val_loss: 29.7893 - val_accuracy: 0.1130 - lr: 0.0010\n",
            "Epoch 2/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.6934 - accuracy: 0.7389 - val_loss: 5.3014 - val_accuracy: 0.1453 - lr: 0.0010\n",
            "Epoch 3/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.5644 - accuracy: 0.7911 - val_loss: 2.9814 - val_accuracy: 0.2857 - lr: 0.0010\n",
            "Epoch 4/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.4975 - accuracy: 0.8141 - val_loss: 1.2112 - val_accuracy: 0.6627 - lr: 0.0010\n",
            "Epoch 5/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.4525 - accuracy: 0.8384 - val_loss: 0.7615 - val_accuracy: 0.7643 - lr: 0.0010\n",
            "Epoch 6/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.4306 - accuracy: 0.8421 - val_loss: 0.7135 - val_accuracy: 0.7747 - lr: 0.0010\n",
            "Epoch 7/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.3930 - accuracy: 0.8551 - val_loss: 0.5938 - val_accuracy: 0.8067 - lr: 0.0010\n",
            "Epoch 8/500\n",
            "110/110 [==============================] - 27s 247ms/step - loss: 0.3854 - accuracy: 0.8554 - val_loss: 0.6636 - val_accuracy: 0.7890 - lr: 0.0010\n",
            "Epoch 9/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.3750 - accuracy: 0.8589 - val_loss: 0.6584 - val_accuracy: 0.8023 - lr: 0.0010\n",
            "Epoch 10/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.3468 - accuracy: 0.8733 - val_loss: 0.6081 - val_accuracy: 0.8053 - lr: 0.0010\n",
            "Epoch 11/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.3242 - accuracy: 0.8803 - val_loss: 0.5900 - val_accuracy: 0.8090 - lr: 9.0000e-04\n",
            "Epoch 12/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.3155 - accuracy: 0.8776 - val_loss: 0.4571 - val_accuracy: 0.8430 - lr: 9.0000e-04\n",
            "Epoch 13/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.2968 - accuracy: 0.8884 - val_loss: 1.1109 - val_accuracy: 0.6920 - lr: 9.0000e-04\n",
            "Epoch 14/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.3048 - accuracy: 0.8893 - val_loss: 0.5432 - val_accuracy: 0.8193 - lr: 9.0000e-04\n",
            "Epoch 15/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.2806 - accuracy: 0.8963 - val_loss: 0.4334 - val_accuracy: 0.8530 - lr: 9.0000e-04\n",
            "Epoch 16/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.2692 - accuracy: 0.9021 - val_loss: 0.4074 - val_accuracy: 0.8597 - lr: 9.0000e-04\n",
            "Epoch 17/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.2580 - accuracy: 0.9083 - val_loss: 0.4459 - val_accuracy: 0.8500 - lr: 9.0000e-04\n",
            "Epoch 18/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.2740 - accuracy: 0.8974 - val_loss: 0.4689 - val_accuracy: 0.8370 - lr: 9.0000e-04\n",
            "Epoch 19/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.2433 - accuracy: 0.9093 - val_loss: 0.4341 - val_accuracy: 0.8627 - lr: 9.0000e-04\n",
            "Epoch 20/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.2355 - accuracy: 0.9149 - val_loss: 0.5201 - val_accuracy: 0.8330 - lr: 8.1000e-04\n",
            "Epoch 21/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.2221 - accuracy: 0.9193 - val_loss: 0.4295 - val_accuracy: 0.8570 - lr: 8.1000e-04\n",
            "Epoch 22/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.2110 - accuracy: 0.9219 - val_loss: 0.4236 - val_accuracy: 0.8633 - lr: 8.1000e-04\n",
            "Epoch 23/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.1879 - accuracy: 0.9326 - val_loss: 0.4642 - val_accuracy: 0.8607 - lr: 7.2900e-04\n",
            "Epoch 24/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.1865 - accuracy: 0.9324 - val_loss: 0.4005 - val_accuracy: 0.8687 - lr: 7.2900e-04\n",
            "Epoch 25/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.1660 - accuracy: 0.9377 - val_loss: 0.4964 - val_accuracy: 0.8490 - lr: 7.2900e-04\n",
            "Epoch 26/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.1775 - accuracy: 0.9376 - val_loss: 0.5066 - val_accuracy: 0.8483 - lr: 7.2900e-04\n",
            "Epoch 27/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.1655 - accuracy: 0.9384 - val_loss: 0.4734 - val_accuracy: 0.8617 - lr: 7.2900e-04\n",
            "Epoch 28/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.1758 - accuracy: 0.9391 - val_loss: 0.4829 - val_accuracy: 0.8580 - lr: 6.5610e-04\n",
            "Epoch 29/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.1469 - accuracy: 0.9454 - val_loss: 0.4839 - val_accuracy: 0.8597 - lr: 6.5610e-04\n",
            "Epoch 30/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.1276 - accuracy: 0.9530 - val_loss: 0.6265 - val_accuracy: 0.8193 - lr: 6.5610e-04\n",
            "Epoch 31/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.1367 - accuracy: 0.9503 - val_loss: 0.4662 - val_accuracy: 0.8723 - lr: 5.9049e-04\n",
            "Epoch 32/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.1258 - accuracy: 0.9551 - val_loss: 0.4427 - val_accuracy: 0.8743 - lr: 5.9049e-04\n",
            "Epoch 33/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.1200 - accuracy: 0.9570 - val_loss: 0.4612 - val_accuracy: 0.8703 - lr: 5.9049e-04\n",
            "Epoch 34/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.0808 - accuracy: 0.9713 - val_loss: 0.6031 - val_accuracy: 0.8520 - lr: 5.3144e-04\n",
            "Epoch 35/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.0993 - accuracy: 0.9651 - val_loss: 0.4352 - val_accuracy: 0.8793 - lr: 5.3144e-04\n",
            "Epoch 36/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.0820 - accuracy: 0.9717 - val_loss: 0.4839 - val_accuracy: 0.8757 - lr: 5.3144e-04\n",
            "Epoch 37/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.0765 - accuracy: 0.9743 - val_loss: 0.4904 - val_accuracy: 0.8780 - lr: 4.7830e-04\n",
            "Epoch 38/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.0729 - accuracy: 0.9743 - val_loss: 0.5195 - val_accuracy: 0.8787 - lr: 4.7830e-04\n",
            "Epoch 39/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.0751 - accuracy: 0.9739 - val_loss: 0.5517 - val_accuracy: 0.8690 - lr: 4.7830e-04\n",
            "Epoch 40/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.0581 - accuracy: 0.9810 - val_loss: 0.5492 - val_accuracy: 0.8790 - lr: 4.3047e-04\n",
            "Epoch 41/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.0780 - accuracy: 0.9723 - val_loss: 0.5008 - val_accuracy: 0.8823 - lr: 4.3047e-04\n",
            "Epoch 42/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.0582 - accuracy: 0.9810 - val_loss: 0.5467 - val_accuracy: 0.8763 - lr: 4.3047e-04\n",
            "Epoch 43/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.0547 - accuracy: 0.9796 - val_loss: 0.5592 - val_accuracy: 0.8710 - lr: 3.8742e-04\n",
            "Epoch 44/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.0452 - accuracy: 0.9850 - val_loss: 0.5586 - val_accuracy: 0.8827 - lr: 3.8742e-04\n",
            "Epoch 45/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.0500 - accuracy: 0.9821 - val_loss: 0.5667 - val_accuracy: 0.8753 - lr: 3.8742e-04\n",
            "Epoch 46/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.0346 - accuracy: 0.9880 - val_loss: 0.5667 - val_accuracy: 0.8837 - lr: 3.4868e-04\n",
            "Epoch 47/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.0475 - accuracy: 0.9839 - val_loss: 0.5830 - val_accuracy: 0.8790 - lr: 3.4868e-04\n",
            "Epoch 48/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.0243 - accuracy: 0.9921 - val_loss: 0.6060 - val_accuracy: 0.8813 - lr: 3.4868e-04\n",
            "Epoch 49/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.0275 - accuracy: 0.9903 - val_loss: 0.6264 - val_accuracy: 0.8763 - lr: 3.1381e-04\n",
            "Epoch 50/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.0289 - accuracy: 0.9903 - val_loss: 0.6460 - val_accuracy: 0.8813 - lr: 3.1381e-04\n",
            "Epoch 51/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.0356 - accuracy: 0.9894 - val_loss: 0.6271 - val_accuracy: 0.8757 - lr: 3.1381e-04\n",
            "Epoch 52/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.0299 - accuracy: 0.9890 - val_loss: 0.6069 - val_accuracy: 0.8840 - lr: 2.8243e-04\n",
            "Epoch 53/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.0200 - accuracy: 0.9941 - val_loss: 0.6039 - val_accuracy: 0.8823 - lr: 2.8243e-04\n",
            "Epoch 54/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.0236 - accuracy: 0.9920 - val_loss: 0.6963 - val_accuracy: 0.8740 - lr: 2.8243e-04\n",
            "Epoch 55/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.0222 - accuracy: 0.9931 - val_loss: 0.6354 - val_accuracy: 0.8897 - lr: 2.5419e-04\n",
            "Epoch 56/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.0187 - accuracy: 0.9940 - val_loss: 0.6539 - val_accuracy: 0.8813 - lr: 2.5419e-04\n",
            "Epoch 57/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.0155 - accuracy: 0.9949 - val_loss: 0.6950 - val_accuracy: 0.8793 - lr: 2.5419e-04\n",
            "Epoch 58/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.0172 - accuracy: 0.9940 - val_loss: 0.6749 - val_accuracy: 0.8840 - lr: 2.2877e-04\n",
            "Epoch 59/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.0189 - accuracy: 0.9941 - val_loss: 0.6757 - val_accuracy: 0.8840 - lr: 2.2877e-04\n",
            "Epoch 60/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.0251 - accuracy: 0.9924 - val_loss: 0.6649 - val_accuracy: 0.8753 - lr: 2.2877e-04\n",
            "Epoch 61/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.0155 - accuracy: 0.9949 - val_loss: 0.6483 - val_accuracy: 0.8830 - lr: 2.0589e-04\n",
            "Epoch 62/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.0132 - accuracy: 0.9963 - val_loss: 0.6627 - val_accuracy: 0.8840 - lr: 2.0589e-04\n",
            "Epoch 63/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.0102 - accuracy: 0.9967 - val_loss: 0.6847 - val_accuracy: 0.8813 - lr: 2.0589e-04\n",
            "Epoch 64/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.0098 - accuracy: 0.9973 - val_loss: 0.6786 - val_accuracy: 0.8817 - lr: 1.8530e-04\n",
            "Epoch 65/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.0035 - accuracy: 0.9991 - val_loss: 0.6724 - val_accuracy: 0.8860 - lr: 1.8530e-04\n",
            "Epoch 66/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.0047 - accuracy: 0.9989 - val_loss: 0.6841 - val_accuracy: 0.8897 - lr: 1.8530e-04\n",
            "Epoch 67/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.0044 - accuracy: 0.9990 - val_loss: 0.6948 - val_accuracy: 0.8883 - lr: 1.6677e-04\n",
            "Epoch 68/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.0051 - accuracy: 0.9980 - val_loss: 0.7141 - val_accuracy: 0.8883 - lr: 1.6677e-04\n",
            "Epoch 69/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.0143 - accuracy: 0.9960 - val_loss: 0.7154 - val_accuracy: 0.8800 - lr: 1.6677e-04\n",
            "Epoch 70/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.0119 - accuracy: 0.9967 - val_loss: 0.7308 - val_accuracy: 0.8813 - lr: 1.5009e-04\n",
            "Epoch 71/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.0060 - accuracy: 0.9984 - val_loss: 0.7092 - val_accuracy: 0.8857 - lr: 1.5009e-04\n",
            "Epoch 72/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.0037 - accuracy: 0.9989 - val_loss: 0.7007 - val_accuracy: 0.8837 - lr: 1.5009e-04\n",
            "Epoch 73/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.0051 - accuracy: 0.9983 - val_loss: 0.7372 - val_accuracy: 0.8807 - lr: 1.3509e-04\n",
            "Epoch 74/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.0117 - accuracy: 0.9964 - val_loss: 0.7294 - val_accuracy: 0.8860 - lr: 1.3509e-04\n",
            "Epoch 75/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.0058 - accuracy: 0.9984 - val_loss: 0.7184 - val_accuracy: 0.8850 - lr: 1.3509e-04\n",
            "Epoch 76/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.0036 - accuracy: 0.9991 - val_loss: 0.6992 - val_accuracy: 0.8873 - lr: 1.2158e-04\n",
            "Epoch 77/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.7354 - val_accuracy: 0.8853 - lr: 1.2158e-04\n",
            "Epoch 78/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.0038 - accuracy: 0.9987 - val_loss: 0.7140 - val_accuracy: 0.8853 - lr: 1.2158e-04\n",
            "Epoch 79/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.7297 - val_accuracy: 0.8877 - lr: 1.0942e-04\n",
            "Epoch 80/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 8.6846e-04 - accuracy: 1.0000 - val_loss: 0.7459 - val_accuracy: 0.8820 - lr: 1.0942e-04\n",
            "Epoch 81/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 9.4531e-04 - accuracy: 0.9999 - val_loss: 0.7246 - val_accuracy: 0.8920 - lr: 1.0942e-04\n",
            "Epoch 82/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.7056 - val_accuracy: 0.8897 - lr: 9.8477e-05\n",
            "Epoch 83/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 7.6692e-04 - accuracy: 1.0000 - val_loss: 0.7560 - val_accuracy: 0.8873 - lr: 9.8477e-05\n",
            "Epoch 84/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.0039 - accuracy: 0.9993 - val_loss: 0.7487 - val_accuracy: 0.8900 - lr: 9.8477e-05\n",
            "Epoch 85/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 0.7608 - val_accuracy: 0.8863 - lr: 8.8629e-05\n",
            "Epoch 86/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 5.9731e-04 - accuracy: 1.0000 - val_loss: 0.7807 - val_accuracy: 0.8893 - lr: 8.8629e-05\n",
            "Epoch 87/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 9.5155e-04 - accuracy: 0.9997 - val_loss: 0.7538 - val_accuracy: 0.8873 - lr: 8.8629e-05\n",
            "Epoch 88/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 9.7875e-04 - accuracy: 0.9997 - val_loss: 0.7720 - val_accuracy: 0.8873 - lr: 7.9766e-05\n",
            "Epoch 89/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 9.8941e-04 - accuracy: 0.9996 - val_loss: 0.7877 - val_accuracy: 0.8867 - lr: 7.9766e-05\n",
            "Epoch 90/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.0032 - accuracy: 0.9989 - val_loss: 0.7974 - val_accuracy: 0.8870 - lr: 7.9766e-05\n",
            "Epoch 91/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.0044 - accuracy: 0.9981 - val_loss: 0.8129 - val_accuracy: 0.8807 - lr: 7.1790e-05\n",
            "Epoch 92/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.0041 - accuracy: 0.9987 - val_loss: 0.7939 - val_accuracy: 0.8830 - lr: 7.1790e-05\n",
            "Epoch 93/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.0058 - accuracy: 0.9980 - val_loss: 0.8012 - val_accuracy: 0.8823 - lr: 7.1790e-05\n",
            "Epoch 94/500\n",
            "110/110 [==============================] - 27s 248ms/step - loss: 0.0015 - accuracy: 0.9994 - val_loss: 0.7795 - val_accuracy: 0.8850 - lr: 6.4611e-05\n",
            "Epoch 95/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.0028 - accuracy: 0.9996 - val_loss: 0.7900 - val_accuracy: 0.8833 - lr: 6.4611e-05\n",
            "Epoch 96/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.7955 - val_accuracy: 0.8857 - lr: 6.4611e-05\n",
            "Epoch 97/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.8038 - val_accuracy: 0.8833 - lr: 5.8150e-05\n",
            "Epoch 98/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.7742 - val_accuracy: 0.8867 - lr: 5.8150e-05\n",
            "Epoch 99/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.7823 - val_accuracy: 0.8830 - lr: 5.8150e-05\n",
            "Epoch 100/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 5.5012e-04 - accuracy: 1.0000 - val_loss: 0.8202 - val_accuracy: 0.8840 - lr: 5.2335e-05\n",
            "Epoch 101/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.0015 - accuracy: 0.9994 - val_loss: 0.7971 - val_accuracy: 0.8820 - lr: 5.2335e-05\n",
            "Epoch 102/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.8111 - val_accuracy: 0.8847 - lr: 5.2335e-05\n",
            "Epoch 103/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 8.8922e-04 - accuracy: 0.9997 - val_loss: 0.7886 - val_accuracy: 0.8857 - lr: 4.7101e-05\n",
            "Epoch 104/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 3.0432e-04 - accuracy: 1.0000 - val_loss: 0.8317 - val_accuracy: 0.8847 - lr: 4.7101e-05\n",
            "Epoch 105/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.8228 - val_accuracy: 0.8857 - lr: 4.7101e-05\n",
            "Epoch 106/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 5.8424e-04 - accuracy: 0.9997 - val_loss: 0.8296 - val_accuracy: 0.8827 - lr: 4.2391e-05\n",
            "Epoch 107/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 3.6775e-04 - accuracy: 1.0000 - val_loss: 0.8409 - val_accuracy: 0.8857 - lr: 4.2391e-05\n",
            "Epoch 108/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.0025 - accuracy: 0.9991 - val_loss: 0.8728 - val_accuracy: 0.8813 - lr: 4.2391e-05\n",
            "Epoch 109/500\n",
            "110/110 [==============================] - 25s 227ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.8258 - val_accuracy: 0.8820 - lr: 3.8152e-05\n",
            "Epoch 110/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 0.0010 - accuracy: 0.9996 - val_loss: 0.8238 - val_accuracy: 0.8903 - lr: 3.8152e-05\n",
            "Epoch 111/500\n",
            "110/110 [==============================] - 25s 226ms/step - loss: 5.2642e-04 - accuracy: 0.9997 - val_loss: 0.8269 - val_accuracy: 0.8870 - lr: 3.8152e-05\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0eee732510>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/파이썬스터디 프로젝트/labeling_resnet.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uY9o0L1zlu7j",
        "outputId": "78de9043-5f18-4e98-9402-cee3c8c41826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TD8ppILhFUrz"
      },
      "source": [
        "## 모델 결과"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7Ljx2EWA2v2n"
      },
      "outputs": [],
      "source": [
        "def model_eval():\n",
        "  pred_mu = model.predict(test_data)\n",
        "  for i in range(1, 30):\n",
        "    pred_mu += model.predict(test_data)\n",
        "  pred_mu = pred_mu/30\n",
        "  predicted_test_labels = np.argmax(pred_mu, axis=1)\n",
        "  return(accuracy_score(np.argmax(test_labels, axis=1), predicted_test_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzhQw9NOWjl3",
        "outputId": "702b97b8-db23-459f-ba44-4fe3fa09d7fa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.892"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# model = keras.models.load_model('/content/drive/MyDrive/파이썬스터디 프로젝트/labeling_cnn.h5')\n",
        "model = keras.models.load_model('/content/drive/MyDrive/파이썬스터디 프로젝트/labeling_resnet.h5')\n",
        "model_eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcbs04885lTu"
      },
      "source": [
        "# 라벨링"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAtfoQKLZDJC"
      },
      "source": [
        "## 라벨링; 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GKJgmFGWhlfl"
      },
      "outputs": [],
      "source": [
        "# model = keras.models.load_model('/content/drive/MyDrive/파이썬스터디 프로젝트/labeling_cnn.h5')\n",
        "model = keras.models.load_model('/content/drive/MyDrive/파이썬스터디 프로젝트/labeling_resnet.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_Jb5VX9ZC3y",
        "outputId": "40b13992-a3a4-42da-9df8-9607d409ee9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [40:43<00:00, 81.44s/it]\n",
            "100%|██████████| 50000/50000 [00:09<00:00, 5417.50it/s]\n"
          ]
        }
      ],
      "source": [
        "# Vars : 분산들\n",
        "# Outs : 결과 값들\n",
        "\n",
        "Vars = []\n",
        "Outs = []\n",
        "\n",
        "temp1 = []\n",
        "\n",
        "for i in tqdm(range(30)):\n",
        "  temp1.append(model.predict(unlab_data))\n",
        "\n",
        "for j in tqdm(range(unlab_data.shape[0])):\n",
        "\n",
        "  temp2 = np.array([0,0,0,0,0,0,0,0,0,0], 'float32')\n",
        "  \n",
        "  for i in range(30):\n",
        "    temp2 += temp1[i][j]\n",
        "  Outs.append(temp2/30)\n",
        "  \n",
        "  outputs=[]\n",
        "\n",
        "  for i in range(30):\n",
        "    outputs.append(temp1[i][j][np.argmax(temp2)])\n",
        "  Vars.append(np.var(outputs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-NnAVIXTXlAa"
      },
      "outputs": [],
      "source": [
        "Outs = np.array(Outs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxIFHtGqAF9B"
      },
      "source": [
        "class마다 균등하게 선택"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGGGKCzvAiZb",
        "outputId": "7203945a-585d-4fdb-cf2c-42fdf488c5c5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 5058,\n",
              "         1: 4973,\n",
              "         2: 4984,\n",
              "         3: 4981,\n",
              "         4: 5026,\n",
              "         5: 5011,\n",
              "         6: 4979,\n",
              "         7: 4978,\n",
              "         8: 5010,\n",
              "         9: 5000})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "Counter(np.argmax(unlab_labels, axis=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 불확정성 컷 1"
      ],
      "metadata": {
        "id": "BUrklzVkE66r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDAwvSCVBLA4",
        "outputId": "2fc40e80-4c31-46d8-a5ab-273d54f3d629"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50000/50000 [01:24<00:00, 590.23it/s]\n",
            "100%|██████████| 50000/50000 [01:29<00:00, 561.58it/s]\n",
            "100%|██████████| 50000/50000 [01:25<00:00, 585.09it/s]\n",
            "100%|██████████| 50000/50000 [01:24<00:00, 589.09it/s]\n",
            "100%|██████████| 50000/50000 [01:25<00:00, 585.43it/s]\n",
            "100%|██████████| 50000/50000 [01:26<00:00, 578.97it/s]\n",
            "100%|██████████| 50000/50000 [01:26<00:00, 578.81it/s]\n",
            "100%|██████████| 50000/50000 [01:27<00:00, 573.93it/s]\n",
            "100%|██████████| 50000/50000 [01:27<00:00, 573.30it/s]\n",
            "100%|██████████| 50000/50000 [01:24<00:00, 589.32it/s]\n"
          ]
        }
      ],
      "source": [
        "# upper bound of variance?\n",
        "\n",
        "UB = []\n",
        "\n",
        "for h in range(10):\n",
        "  classvars = []\n",
        "  for i in tqdm(range(50000)):\n",
        "    if np.argmax(unlab_labels, axis=1).tolist()[i]==h:\n",
        "      classvars.append(Vars[i])\n",
        "  UB.append(np.percentile(classvars, 50))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BUHoKvtkJ37"
      },
      "outputs": [],
      "source": [
        "lowvars = []\n",
        "ind = 0 \n",
        "\n",
        "for i in Vars:\n",
        "  if i <= UB[np.argmax(unlab_labels, axis=1)[ind]]:\n",
        "    lowvars.append(ind)\n",
        "  ind += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WhEDezq362P"
      },
      "outputs": [],
      "source": [
        "highvars = []\n",
        "for i in range(unlab_data.shape[0]):\n",
        "  if i not in lowvars:\n",
        "    highvars.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uSdhE1VmF5a",
        "outputId": "c427ea32-0e5c-4928-be41-4cdef55df959"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.89192"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 같은 길이의 랜덤하게 고른 data들에 모델로 label 부여 후 정확도 측정\n",
        "randomindx = random.sample(range(50000), 50000)\n",
        "randomindx2 = randomindx[0:25000]\n",
        "randomindx = randomindx[25000:50000]\n",
        "accuracy_score(np.argmax(np.array(Outs)[randomindx], axis=1), np.argmax(unlab_labels[randomindx], axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtEPD5p5kM-z",
        "outputId": "e6c93b65-dfe5-49f6-ab77-79f7d0814c6c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9728878433874247"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 저분산의 data들만 모아서 모델로 label 부여 후 정확도 측정 : 95% 이상\n",
        "accuracy_score(np.argmax(np.array(Outs)[lowvars], axis=1), np.argmax(unlab_labels[lowvars], axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2ab4GUWwe30"
      },
      "outputs": [],
      "source": [
        "train_data_1 = np.concatenate([train_data, unlab_data[lowvars]], axis=0)\n",
        "train_labels_1 = np.concatenate([train_labels, Outs[lowvars]], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xahhoBY8mKc"
      },
      "outputs": [],
      "source": [
        "shufindx = random.sample(range(train_data_1.shape[0]),train_data_1.shape[0])\n",
        "train_data_1 = train_data_1[shufindx]\n",
        "train_labels_1 = train_labels_1[shufindx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gQWZXNojDvb"
      },
      "outputs": [],
      "source": [
        "train_data_2 = np.concatenate([train_data, unlab_data[randomindx]], axis=0)\n",
        "train_labels_2 = np.concatenate([train_labels, Outs[randomindx]], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tdOxYBQjFGT"
      },
      "outputs": [],
      "source": [
        "shufindx = random.sample(range(train_data_2.shape[0]),train_data_2.shape[0])\n",
        "train_data_2 = train_data_2[shufindx]\n",
        "train_labels_2 = train_labels_2[shufindx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_t9J8vlJ5TDO"
      },
      "outputs": [],
      "source": [
        "unlab_data_1 = unlab_data[highvars]\n",
        "unlab_labels_1 = unlab_labels[highvars]\n",
        "unlab_data_2 = unlab_data[randomindx2]\n",
        "unlab_labels_2 = unlab_labels[randomindx2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7Jq-n0L8diD",
        "outputId": "9dfcefff-d842-4581-de6b-579daea38e94"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({0: 3600,\n",
              "         1: 3599,\n",
              "         2: 3613,\n",
              "         3: 3638,\n",
              "         4: 3469,\n",
              "         5: 3491,\n",
              "         6: 3151,\n",
              "         7: 3507,\n",
              "         8: 3505,\n",
              "         9: 3508})"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Counter(np.argmax(train_labels_1, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CLpAKvu8kD6",
        "outputId": "00fce93a-128c-471e-d460-410106f853da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({0: 3409,\n",
              "         1: 3473,\n",
              "         2: 3582,\n",
              "         3: 3546,\n",
              "         4: 3512,\n",
              "         5: 3488,\n",
              "         6: 3448,\n",
              "         7: 3581,\n",
              "         8: 3447,\n",
              "         9: 3514})"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Counter(np.argmax(train_labels_2, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5t5J130x7pe",
        "outputId": "6c32f60a-2251-4d7c-e35a-b8e445dbb790"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({0: 2529,\n",
              "         1: 2407,\n",
              "         2: 2492,\n",
              "         3: 2490,\n",
              "         4: 2513,\n",
              "         5: 2505,\n",
              "         6: 2489,\n",
              "         7: 2489,\n",
              "         8: 2505,\n",
              "         9: 2500})"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Counter(np.argmax(unlab_labels_1, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBMxw6eEx92l",
        "outputId": "57ac6607-fa7f-4bc9-8e0b-f4e876ab52d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({0: 2565,\n",
              "         1: 2496,\n",
              "         2: 2452,\n",
              "         3: 2497,\n",
              "         4: 2506,\n",
              "         5: 2458,\n",
              "         6: 2504,\n",
              "         7: 2474,\n",
              "         8: 2554,\n",
              "         9: 2494})"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Counter(np.argmax(unlab_labels_2, axis=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 불확정성 컷 2"
      ],
      "metadata": {
        "id": "qXY6ufSkE_rP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# upper bound of variance?\n",
        "\n",
        "UB25 = []\n",
        "UB50 = []\n",
        "UB75 = []\n",
        "\n",
        "\n",
        "for h in range(10):\n",
        "  classvars = []\n",
        "  for i in tqdm(range(50000)):\n",
        "    if np.argmax(unlab_labels, axis=1).tolist()[i]==h:\n",
        "      classvars.append(Vars[i])\n",
        "  UB25.append(np.percentile(classvars, 25))\n",
        "  UB50.append(np.percentile(classvars, 50))\n",
        "  UB75.append(np.percentile(classvars, 75))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEPgF4EWEdGx",
        "outputId": "af8ed83b-4b3c-4607-96c7-d37d239ea526"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50000/50000 [01:31<00:00, 549.17it/s]\n",
            "100%|██████████| 50000/50000 [01:31<00:00, 547.84it/s]\n",
            "100%|██████████| 50000/50000 [01:30<00:00, 551.12it/s]\n",
            "100%|██████████| 50000/50000 [01:29<00:00, 557.97it/s]\n",
            "100%|██████████| 50000/50000 [01:28<00:00, 562.48it/s]\n",
            "100%|██████████| 50000/50000 [01:28<00:00, 566.39it/s]\n",
            "100%|██████████| 50000/50000 [01:27<00:00, 571.34it/s]\n",
            "100%|██████████| 50000/50000 [01:28<00:00, 562.56it/s]\n",
            "100%|██████████| 50000/50000 [01:27<00:00, 571.13it/s]\n",
            "100%|██████████| 50000/50000 [01:27<00:00, 569.78it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# UB25 < UB50 < UB75"
      ],
      "metadata": {
        "id": "F2m-BBSNFRO6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xOq_mV-uFGKX"
      },
      "outputs": [],
      "source": [
        "vars25 = []\n",
        "vars50 = []\n",
        "vars75 = []\n",
        "vars100 = []\n",
        "\n",
        "ind = 0 \n",
        "\n",
        "\n",
        "for i in Vars:\n",
        "  if i <= UB25[np.argmax(unlab_labels, axis=1)[ind]]:\n",
        "    vars25.append(ind)\n",
        "  elif i <= UB50[np.argmax(unlab_labels, axis=1)[ind]]:\n",
        "    vars50.append(ind)\n",
        "  elif i <= UB75[np.argmax(unlab_labels, axis=1)[ind]]:\n",
        "    vars75.append(ind)\n",
        "  else:\n",
        "    vars100.append(ind)\n",
        "  ind += 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k1 = random.sample(range(len(vars25)), len(vars25))\n",
        "k2 = random.sample(range(len(vars50)), len(vars50))\n",
        "k3 = random.sample(range(len(vars75)), len(vars75))\n",
        "k4 = random.sample(range(len(vars100)), len(vars100))\n",
        "\n",
        "lowvars = k1[0:np.int(len(k1)/2)] + k2[0:np.int(len(k2)/2)] + k3[0:np.int(len(k3)/2)] + k4[0:np.int(len(k4)/2)]\n",
        "highvars = k1[np.int(len(k1)/2):len(k1)] + k2[np.int(len(k2)/2):len(k2)] + k3[np.int(len(k3)/2):len(k3)] + k4[np.int(len(k4)/2):len(k4)]"
      ],
      "metadata": {
        "id": "5kx2MFUxKh6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44b9f540-36cf-4024-8d86-f4c56444ae2b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  import sys\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c006b77d-63ae-4c75-c99c-3ff59314150f",
        "id": "cHARP-fyFGKX"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.89432"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# 같은 길이의 랜덤하게 고른 data들에 모델로 label 부여 후 정확도 측정\n",
        "randomindx = random.sample(range(50000), 50000)\n",
        "randomindx2 = randomindx[0:25000]\n",
        "randomindx = randomindx[25000:50000]\n",
        "accuracy_score(np.argmax(np.array(Outs)[randomindx], axis=1), np.argmax(unlab_labels[randomindx], axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1d88274-4e01-4ce1-de96-d3d2ff93b3fb",
        "id": "DU2TXlLaFGKY"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8876755070202809"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# 층화추출\n",
        "accuracy_score(np.argmax(np.array(Outs)[lowvars], axis=1), np.argmax(unlab_labels[lowvars], axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1ka9xFulFGKY"
      },
      "outputs": [],
      "source": [
        "train_data_1 = np.concatenate([train_data, unlab_data[lowvars]], axis=0)\n",
        "train_labels_1 = np.concatenate([train_labels, Outs[lowvars]], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "uLqnYj4ZFGKZ"
      },
      "outputs": [],
      "source": [
        "shufindx = random.sample(range(train_data_1.shape[0]),train_data_1.shape[0])\n",
        "train_data_1 = train_data_1[shufindx]\n",
        "train_labels_1 = train_labels_1[shufindx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "nvb7FhRaFGKZ"
      },
      "outputs": [],
      "source": [
        "train_data_2 = np.concatenate([train_data, unlab_data[randomindx]], axis=0)\n",
        "train_labels_2 = np.concatenate([train_labels, Outs[randomindx]], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "PsSWcAFHFGKZ"
      },
      "outputs": [],
      "source": [
        "shufindx = random.sample(range(train_data_2.shape[0]),train_data_2.shape[0])\n",
        "train_data_2 = train_data_2[shufindx]\n",
        "train_labels_2 = train_labels_2[shufindx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "F39Oc7V8FGKZ"
      },
      "outputs": [],
      "source": [
        "unlab_data_1 = unlab_data[highvars]\n",
        "unlab_labels_1 = unlab_labels[highvars]\n",
        "unlab_data_2 = unlab_data[randomindx2]\n",
        "unlab_labels_2 = unlab_labels[randomindx2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a25f961a-7a2b-4b16-ff17-b341d28cc6a5",
        "id": "MMw4VfmaFGKZ"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 3539,\n",
              "         1: 3508,\n",
              "         2: 3750,\n",
              "         3: 3530,\n",
              "         4: 3372,\n",
              "         5: 3486,\n",
              "         6: 3373,\n",
              "         7: 3539,\n",
              "         8: 3401,\n",
              "         9: 3501})"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "Counter(np.argmax(train_labels_1, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40233ed7-5b59-468e-87ec-6c513852e97f",
        "id": "o5ZZKXxBFGKZ"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 3497,\n",
              "         1: 3448,\n",
              "         2: 3693,\n",
              "         3: 3566,\n",
              "         4: 3372,\n",
              "         5: 3444,\n",
              "         6: 3363,\n",
              "         7: 3591,\n",
              "         8: 3444,\n",
              "         9: 3582})"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "Counter(np.argmax(train_labels_2, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce384ea4-a0ee-4f47-e870-069d1222d094",
        "id": "Y05Wb3HZFGKa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 2507,\n",
              "         1: 2456,\n",
              "         2: 2436,\n",
              "         3: 2505,\n",
              "         4: 2490,\n",
              "         5: 2529,\n",
              "         6: 2595,\n",
              "         7: 2522,\n",
              "         8: 2389,\n",
              "         9: 2572})"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "Counter(np.argmax(unlab_labels_1, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "405ba621-ed80-4b0d-d346-b70a09a29194",
        "id": "qHWvlgD4FGKa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 2559,\n",
              "         1: 2517,\n",
              "         2: 2497,\n",
              "         3: 2470,\n",
              "         4: 2534,\n",
              "         5: 2471,\n",
              "         6: 2491,\n",
              "         7: 2484,\n",
              "         8: 2549,\n",
              "         9: 2428})"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "Counter(np.argmax(unlab_labels_2, axis=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOtFA9fr5oDF"
      },
      "source": [
        "## 모델링\n",
        "\n",
        "간단한 모델에서 받은 지식를 복잡한 모델에서 학습 \n",
        "\n",
        "uncertainty aware data vs random sampling data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CNN"
      ],
      "metadata": {
        "id": "IQb_xC_smn88"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7Pd9SYK5tv_"
      },
      "outputs": [],
      "source": [
        "def PermaDropout(rate):\n",
        "    return Lambda(lambda x: K.dropout(x, level=rate))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6WgyM3Y5twA",
        "outputId": "7a0ad540-3b41-4fb4-fb0d-2fcc236bd2cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2000\n",
            "383/383 [==============================] - 5s 12ms/step - loss: 1.1925 - accuracy: 0.6126 - val_loss: 0.6793 - val_accuracy: 0.7716 - lr: 0.0010\n",
            "Epoch 2/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.6240 - accuracy: 0.7889 - val_loss: 0.5945 - val_accuracy: 0.7977 - lr: 0.0010\n",
            "Epoch 3/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.5377 - accuracy: 0.8277 - val_loss: 0.5035 - val_accuracy: 0.8390 - lr: 0.0010\n",
            "Epoch 4/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.4882 - accuracy: 0.8422 - val_loss: 0.4723 - val_accuracy: 0.8481 - lr: 0.0010\n",
            "Epoch 5/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.4643 - accuracy: 0.8558 - val_loss: 0.4740 - val_accuracy: 0.8491 - lr: 0.0010\n",
            "Epoch 6/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.4406 - accuracy: 0.8667 - val_loss: 0.4313 - val_accuracy: 0.8682 - lr: 0.0010\n",
            "Epoch 7/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.4175 - accuracy: 0.8724 - val_loss: 0.4325 - val_accuracy: 0.8622 - lr: 0.0010\n",
            "Epoch 8/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.4081 - accuracy: 0.8785 - val_loss: 0.4095 - val_accuracy: 0.8810 - lr: 0.0010\n",
            "Epoch 9/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.4015 - accuracy: 0.8838 - val_loss: 0.4178 - val_accuracy: 0.8727 - lr: 0.0010\n",
            "Epoch 10/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3959 - accuracy: 0.8844 - val_loss: 0.3993 - val_accuracy: 0.8822 - lr: 0.0010\n",
            "Epoch 11/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3904 - accuracy: 0.8880 - val_loss: 0.3913 - val_accuracy: 0.8867 - lr: 0.0010\n",
            "Epoch 12/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3800 - accuracy: 0.8922 - val_loss: 0.3888 - val_accuracy: 0.8875 - lr: 0.0010\n",
            "Epoch 13/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3719 - accuracy: 0.8939 - val_loss: 0.3905 - val_accuracy: 0.8847 - lr: 0.0010\n",
            "Epoch 14/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3694 - accuracy: 0.8978 - val_loss: 0.3836 - val_accuracy: 0.8900 - lr: 0.0010\n",
            "Epoch 15/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3670 - accuracy: 0.8982 - val_loss: 0.3742 - val_accuracy: 0.8931 - lr: 0.0010\n",
            "Epoch 16/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3581 - accuracy: 0.9037 - val_loss: 0.3802 - val_accuracy: 0.8916 - lr: 0.0010\n",
            "Epoch 17/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3573 - accuracy: 0.9050 - val_loss: 0.3800 - val_accuracy: 0.8889 - lr: 0.0010\n",
            "Epoch 18/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3543 - accuracy: 0.9026 - val_loss: 0.3750 - val_accuracy: 0.8933 - lr: 0.0010\n",
            "Epoch 19/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3461 - accuracy: 0.9072 - val_loss: 0.3794 - val_accuracy: 0.8944 - lr: 9.0000e-04\n",
            "Epoch 20/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3407 - accuracy: 0.9085 - val_loss: 0.3697 - val_accuracy: 0.8981 - lr: 9.0000e-04\n",
            "Epoch 21/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3374 - accuracy: 0.9105 - val_loss: 0.3726 - val_accuracy: 0.8960 - lr: 9.0000e-04\n",
            "Epoch 22/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3377 - accuracy: 0.9097 - val_loss: 0.3647 - val_accuracy: 0.8974 - lr: 9.0000e-04\n",
            "Epoch 23/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3332 - accuracy: 0.9129 - val_loss: 0.3722 - val_accuracy: 0.8961 - lr: 9.0000e-04\n",
            "Epoch 24/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3332 - accuracy: 0.9137 - val_loss: 0.3634 - val_accuracy: 0.9028 - lr: 9.0000e-04\n",
            "Epoch 25/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3342 - accuracy: 0.9104 - val_loss: 0.3633 - val_accuracy: 0.8990 - lr: 9.0000e-04\n",
            "Epoch 26/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3289 - accuracy: 0.9144 - val_loss: 0.3617 - val_accuracy: 0.8990 - lr: 9.0000e-04\n",
            "Epoch 27/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3288 - accuracy: 0.9152 - val_loss: 0.3555 - val_accuracy: 0.9000 - lr: 9.0000e-04\n",
            "Epoch 28/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3239 - accuracy: 0.9169 - val_loss: 0.3606 - val_accuracy: 0.9011 - lr: 9.0000e-04\n",
            "Epoch 29/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3221 - accuracy: 0.9182 - val_loss: 0.3559 - val_accuracy: 0.9005 - lr: 9.0000e-04\n",
            "Epoch 30/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3234 - accuracy: 0.9160 - val_loss: 0.3585 - val_accuracy: 0.9009 - lr: 9.0000e-04\n",
            "Epoch 31/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3160 - accuracy: 0.9203 - val_loss: 0.3488 - val_accuracy: 0.9082 - lr: 8.1000e-04\n",
            "Epoch 32/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3155 - accuracy: 0.9203 - val_loss: 0.3507 - val_accuracy: 0.9043 - lr: 8.1000e-04\n",
            "Epoch 33/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3141 - accuracy: 0.9213 - val_loss: 0.3497 - val_accuracy: 0.9068 - lr: 8.1000e-04\n",
            "Epoch 34/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3101 - accuracy: 0.9248 - val_loss: 0.3518 - val_accuracy: 0.9084 - lr: 8.1000e-04\n",
            "Epoch 35/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3044 - accuracy: 0.9241 - val_loss: 0.3422 - val_accuracy: 0.9117 - lr: 7.2900e-04\n",
            "Epoch 36/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3008 - accuracy: 0.9284 - val_loss: 0.3459 - val_accuracy: 0.9093 - lr: 7.2900e-04\n",
            "Epoch 37/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3006 - accuracy: 0.9259 - val_loss: 0.3441 - val_accuracy: 0.9117 - lr: 7.2900e-04\n",
            "Epoch 38/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3018 - accuracy: 0.9252 - val_loss: 0.3438 - val_accuracy: 0.9095 - lr: 7.2900e-04\n",
            "Epoch 39/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2947 - accuracy: 0.9290 - val_loss: 0.3377 - val_accuracy: 0.9141 - lr: 6.5610e-04\n",
            "Epoch 40/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2920 - accuracy: 0.9304 - val_loss: 0.3394 - val_accuracy: 0.9131 - lr: 6.5610e-04\n",
            "Epoch 41/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2903 - accuracy: 0.9347 - val_loss: 0.3384 - val_accuracy: 0.9130 - lr: 6.5610e-04\n",
            "Epoch 42/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2867 - accuracy: 0.9327 - val_loss: 0.3410 - val_accuracy: 0.9113 - lr: 6.5610e-04\n",
            "Epoch 43/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2875 - accuracy: 0.9322 - val_loss: 0.3450 - val_accuracy: 0.9116 - lr: 5.9049e-04\n",
            "Epoch 44/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2854 - accuracy: 0.9344 - val_loss: 0.3382 - val_accuracy: 0.9171 - lr: 5.9049e-04\n",
            "Epoch 45/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2841 - accuracy: 0.9342 - val_loss: 0.3345 - val_accuracy: 0.9128 - lr: 5.9049e-04\n",
            "Epoch 46/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2827 - accuracy: 0.9372 - val_loss: 0.3388 - val_accuracy: 0.9139 - lr: 5.9049e-04\n",
            "Epoch 47/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2827 - accuracy: 0.9376 - val_loss: 0.3314 - val_accuracy: 0.9164 - lr: 5.9049e-04\n",
            "Epoch 48/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2806 - accuracy: 0.9373 - val_loss: 0.3327 - val_accuracy: 0.9148 - lr: 5.9049e-04\n",
            "Epoch 49/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2775 - accuracy: 0.9399 - val_loss: 0.3385 - val_accuracy: 0.9131 - lr: 5.9049e-04\n",
            "Epoch 50/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2785 - accuracy: 0.9392 - val_loss: 0.3312 - val_accuracy: 0.9151 - lr: 5.9049e-04\n",
            "Epoch 51/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2757 - accuracy: 0.9391 - val_loss: 0.3278 - val_accuracy: 0.9187 - lr: 5.9049e-04\n",
            "Epoch 52/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2772 - accuracy: 0.9393 - val_loss: 0.3377 - val_accuracy: 0.9150 - lr: 5.9049e-04\n",
            "Epoch 53/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2762 - accuracy: 0.9394 - val_loss: 0.3349 - val_accuracy: 0.9168 - lr: 5.9049e-04\n",
            "Epoch 54/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2735 - accuracy: 0.9397 - val_loss: 0.3320 - val_accuracy: 0.9180 - lr: 5.9049e-04\n",
            "Epoch 55/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2710 - accuracy: 0.9431 - val_loss: 0.3369 - val_accuracy: 0.9148 - lr: 5.3144e-04\n",
            "Epoch 56/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2709 - accuracy: 0.9418 - val_loss: 0.3315 - val_accuracy: 0.9162 - lr: 5.3144e-04\n",
            "Epoch 57/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2706 - accuracy: 0.9401 - val_loss: 0.3297 - val_accuracy: 0.9175 - lr: 5.3144e-04\n",
            "Epoch 58/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2674 - accuracy: 0.9442 - val_loss: 0.3282 - val_accuracy: 0.9158 - lr: 4.7830e-04\n",
            "Epoch 59/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2657 - accuracy: 0.9444 - val_loss: 0.3340 - val_accuracy: 0.9152 - lr: 4.7830e-04\n",
            "Epoch 60/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2638 - accuracy: 0.9445 - val_loss: 0.3243 - val_accuracy: 0.9182 - lr: 4.7830e-04\n",
            "Epoch 61/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2627 - accuracy: 0.9455 - val_loss: 0.3277 - val_accuracy: 0.9223 - lr: 4.7830e-04\n",
            "Epoch 62/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2612 - accuracy: 0.9464 - val_loss: 0.3260 - val_accuracy: 0.9202 - lr: 4.7830e-04\n",
            "Epoch 63/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2621 - accuracy: 0.9466 - val_loss: 0.3333 - val_accuracy: 0.9147 - lr: 4.7830e-04\n",
            "Epoch 64/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2604 - accuracy: 0.9461 - val_loss: 0.3265 - val_accuracy: 0.9216 - lr: 4.3047e-04\n",
            "Epoch 65/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2590 - accuracy: 0.9482 - val_loss: 0.3276 - val_accuracy: 0.9216 - lr: 4.3047e-04\n",
            "Epoch 66/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2573 - accuracy: 0.9488 - val_loss: 0.3262 - val_accuracy: 0.9228 - lr: 4.3047e-04\n",
            "Epoch 67/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2554 - accuracy: 0.9506 - val_loss: 0.3225 - val_accuracy: 0.9231 - lr: 3.8742e-04\n",
            "Epoch 68/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2545 - accuracy: 0.9494 - val_loss: 0.3246 - val_accuracy: 0.9211 - lr: 3.8742e-04\n",
            "Epoch 69/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2538 - accuracy: 0.9494 - val_loss: 0.3227 - val_accuracy: 0.9207 - lr: 3.8742e-04\n",
            "Epoch 70/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2521 - accuracy: 0.9514 - val_loss: 0.3208 - val_accuracy: 0.9198 - lr: 3.8742e-04\n",
            "Epoch 71/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2520 - accuracy: 0.9520 - val_loss: 0.3237 - val_accuracy: 0.9208 - lr: 3.8742e-04\n",
            "Epoch 72/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2503 - accuracy: 0.9533 - val_loss: 0.3278 - val_accuracy: 0.9208 - lr: 3.8742e-04\n",
            "Epoch 73/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2498 - accuracy: 0.9502 - val_loss: 0.3218 - val_accuracy: 0.9234 - lr: 3.8742e-04\n",
            "Epoch 74/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2496 - accuracy: 0.9530 - val_loss: 0.3219 - val_accuracy: 0.9218 - lr: 3.4868e-04\n",
            "Epoch 75/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2494 - accuracy: 0.9531 - val_loss: 0.3176 - val_accuracy: 0.9246 - lr: 3.4868e-04\n",
            "Epoch 76/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2467 - accuracy: 0.9538 - val_loss: 0.3241 - val_accuracy: 0.9249 - lr: 3.4868e-04\n",
            "Epoch 77/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2474 - accuracy: 0.9541 - val_loss: 0.3273 - val_accuracy: 0.9212 - lr: 3.4868e-04\n",
            "Epoch 78/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2482 - accuracy: 0.9537 - val_loss: 0.3216 - val_accuracy: 0.9223 - lr: 3.4868e-04\n",
            "Epoch 79/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2441 - accuracy: 0.9570 - val_loss: 0.3239 - val_accuracy: 0.9243 - lr: 3.1381e-04\n",
            "Epoch 80/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2430 - accuracy: 0.9561 - val_loss: 0.3215 - val_accuracy: 0.9224 - lr: 3.1381e-04\n",
            "Epoch 81/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2425 - accuracy: 0.9568 - val_loss: 0.3166 - val_accuracy: 0.9244 - lr: 3.1381e-04\n",
            "Epoch 82/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2415 - accuracy: 0.9573 - val_loss: 0.3194 - val_accuracy: 0.9254 - lr: 3.1381e-04\n",
            "Epoch 83/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2430 - accuracy: 0.9559 - val_loss: 0.3182 - val_accuracy: 0.9266 - lr: 3.1381e-04\n",
            "Epoch 84/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2398 - accuracy: 0.9570 - val_loss: 0.3173 - val_accuracy: 0.9238 - lr: 3.1381e-04\n",
            "Epoch 85/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2405 - accuracy: 0.9563 - val_loss: 0.3154 - val_accuracy: 0.9264 - lr: 2.8243e-04\n",
            "Epoch 86/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2392 - accuracy: 0.9591 - val_loss: 0.3173 - val_accuracy: 0.9225 - lr: 2.8243e-04\n",
            "Epoch 87/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2392 - accuracy: 0.9576 - val_loss: 0.3158 - val_accuracy: 0.9267 - lr: 2.8243e-04\n",
            "Epoch 88/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2382 - accuracy: 0.9589 - val_loss: 0.3174 - val_accuracy: 0.9235 - lr: 2.8243e-04\n",
            "Epoch 89/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2381 - accuracy: 0.9578 - val_loss: 0.3216 - val_accuracy: 0.9232 - lr: 2.5419e-04\n",
            "Epoch 90/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2373 - accuracy: 0.9559 - val_loss: 0.3232 - val_accuracy: 0.9223 - lr: 2.5419e-04\n",
            "Epoch 91/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2370 - accuracy: 0.9569 - val_loss: 0.3122 - val_accuracy: 0.9229 - lr: 2.5419e-04\n",
            "Epoch 92/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2367 - accuracy: 0.9580 - val_loss: 0.3132 - val_accuracy: 0.9249 - lr: 2.5419e-04\n",
            "Epoch 93/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2353 - accuracy: 0.9595 - val_loss: 0.3168 - val_accuracy: 0.9270 - lr: 2.5419e-04\n",
            "Epoch 94/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2348 - accuracy: 0.9598 - val_loss: 0.3159 - val_accuracy: 0.9246 - lr: 2.5419e-04\n",
            "Epoch 95/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2353 - accuracy: 0.9589 - val_loss: 0.3111 - val_accuracy: 0.9267 - lr: 2.2877e-04\n",
            "Epoch 96/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2330 - accuracy: 0.9606 - val_loss: 0.3149 - val_accuracy: 0.9251 - lr: 2.2877e-04\n",
            "Epoch 97/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2338 - accuracy: 0.9602 - val_loss: 0.3162 - val_accuracy: 0.9255 - lr: 2.2877e-04\n",
            "Epoch 98/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2327 - accuracy: 0.9632 - val_loss: 0.3172 - val_accuracy: 0.9237 - lr: 2.2877e-04\n",
            "Epoch 99/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2323 - accuracy: 0.9604 - val_loss: 0.3146 - val_accuracy: 0.9288 - lr: 2.0589e-04\n",
            "Epoch 100/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2304 - accuracy: 0.9610 - val_loss: 0.3160 - val_accuracy: 0.9252 - lr: 2.0589e-04\n",
            "Epoch 101/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2301 - accuracy: 0.9619 - val_loss: 0.3141 - val_accuracy: 0.9281 - lr: 2.0589e-04\n",
            "Epoch 102/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2301 - accuracy: 0.9624 - val_loss: 0.3103 - val_accuracy: 0.9272 - lr: 1.8530e-04\n",
            "Epoch 103/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2285 - accuracy: 0.9641 - val_loss: 0.3105 - val_accuracy: 0.9276 - lr: 1.8530e-04\n",
            "Epoch 104/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2296 - accuracy: 0.9607 - val_loss: 0.3150 - val_accuracy: 0.9282 - lr: 1.8530e-04\n",
            "Epoch 105/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2285 - accuracy: 0.9631 - val_loss: 0.3185 - val_accuracy: 0.9253 - lr: 1.8530e-04\n",
            "Epoch 106/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2272 - accuracy: 0.9647 - val_loss: 0.3120 - val_accuracy: 0.9295 - lr: 1.6677e-04\n",
            "Epoch 107/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2278 - accuracy: 0.9617 - val_loss: 0.3093 - val_accuracy: 0.9290 - lr: 1.6677e-04\n",
            "Epoch 108/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2279 - accuracy: 0.9643 - val_loss: 0.3128 - val_accuracy: 0.9290 - lr: 1.6677e-04\n",
            "Epoch 109/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2271 - accuracy: 0.9640 - val_loss: 0.3086 - val_accuracy: 0.9271 - lr: 1.6677e-04\n",
            "Epoch 110/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2267 - accuracy: 0.9625 - val_loss: 0.3135 - val_accuracy: 0.9258 - lr: 1.6677e-04\n",
            "Epoch 111/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2272 - accuracy: 0.9636 - val_loss: 0.3146 - val_accuracy: 0.9280 - lr: 1.6677e-04\n",
            "Epoch 112/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2263 - accuracy: 0.9653 - val_loss: 0.3124 - val_accuracy: 0.9271 - lr: 1.6677e-04\n",
            "Epoch 113/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2269 - accuracy: 0.9631 - val_loss: 0.3161 - val_accuracy: 0.9278 - lr: 1.5009e-04\n",
            "Epoch 114/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2265 - accuracy: 0.9654 - val_loss: 0.3145 - val_accuracy: 0.9278 - lr: 1.5009e-04\n",
            "Epoch 115/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2262 - accuracy: 0.9631 - val_loss: 0.3131 - val_accuracy: 0.9294 - lr: 1.5009e-04\n",
            "Epoch 116/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2240 - accuracy: 0.9659 - val_loss: 0.3115 - val_accuracy: 0.9278 - lr: 1.3509e-04\n",
            "Epoch 117/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2246 - accuracy: 0.9671 - val_loss: 0.3110 - val_accuracy: 0.9271 - lr: 1.3509e-04\n",
            "Epoch 118/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2247 - accuracy: 0.9647 - val_loss: 0.3130 - val_accuracy: 0.9277 - lr: 1.3509e-04\n",
            "Epoch 119/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2242 - accuracy: 0.9651 - val_loss: 0.3099 - val_accuracy: 0.9250 - lr: 1.2158e-04\n",
            "Epoch 120/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2236 - accuracy: 0.9652 - val_loss: 0.3142 - val_accuracy: 0.9296 - lr: 1.2158e-04\n",
            "Epoch 121/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2241 - accuracy: 0.9651 - val_loss: 0.3116 - val_accuracy: 0.9273 - lr: 1.2158e-04\n",
            "Epoch 122/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2236 - accuracy: 0.9650 - val_loss: 0.3099 - val_accuracy: 0.9298 - lr: 1.0942e-04\n",
            "Epoch 123/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2221 - accuracy: 0.9651 - val_loss: 0.3085 - val_accuracy: 0.9299 - lr: 1.0942e-04\n",
            "Epoch 124/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2226 - accuracy: 0.9655 - val_loss: 0.3085 - val_accuracy: 0.9310 - lr: 1.0942e-04\n",
            "Epoch 125/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2224 - accuracy: 0.9668 - val_loss: 0.3102 - val_accuracy: 0.9299 - lr: 1.0942e-04\n",
            "Epoch 126/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2227 - accuracy: 0.9657 - val_loss: 0.3055 - val_accuracy: 0.9282 - lr: 1.0942e-04\n",
            "Epoch 127/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2214 - accuracy: 0.9664 - val_loss: 0.3084 - val_accuracy: 0.9287 - lr: 1.0942e-04\n",
            "Epoch 128/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2218 - accuracy: 0.9667 - val_loss: 0.3139 - val_accuracy: 0.9274 - lr: 1.0942e-04\n",
            "Epoch 129/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2213 - accuracy: 0.9658 - val_loss: 0.3057 - val_accuracy: 0.9288 - lr: 1.0942e-04\n",
            "Epoch 130/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2220 - accuracy: 0.9664 - val_loss: 0.3068 - val_accuracy: 0.9287 - lr: 9.8477e-05\n",
            "Epoch 131/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2219 - accuracy: 0.9665 - val_loss: 0.3082 - val_accuracy: 0.9282 - lr: 9.8477e-05\n",
            "Epoch 132/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2203 - accuracy: 0.9666 - val_loss: 0.3069 - val_accuracy: 0.9288 - lr: 9.8477e-05\n",
            "Epoch 133/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2199 - accuracy: 0.9662 - val_loss: 0.3093 - val_accuracy: 0.9302 - lr: 8.8629e-05\n",
            "Epoch 134/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2204 - accuracy: 0.9672 - val_loss: 0.3073 - val_accuracy: 0.9322 - lr: 8.8629e-05\n",
            "Epoch 135/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2198 - accuracy: 0.9669 - val_loss: 0.3104 - val_accuracy: 0.9270 - lr: 8.8629e-05\n",
            "Epoch 136/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2193 - accuracy: 0.9673 - val_loss: 0.3063 - val_accuracy: 0.9299 - lr: 7.9766e-05\n",
            "Epoch 137/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2200 - accuracy: 0.9664 - val_loss: 0.3075 - val_accuracy: 0.9304 - lr: 7.9766e-05\n",
            "Epoch 138/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2187 - accuracy: 0.9698 - val_loss: 0.3096 - val_accuracy: 0.9297 - lr: 7.9766e-05\n",
            "Epoch 139/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2197 - accuracy: 0.9667 - val_loss: 0.3105 - val_accuracy: 0.9297 - lr: 7.1790e-05\n",
            "Epoch 140/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2185 - accuracy: 0.9679 - val_loss: 0.3119 - val_accuracy: 0.9317 - lr: 7.1790e-05\n",
            "Epoch 141/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2184 - accuracy: 0.9663 - val_loss: 0.3128 - val_accuracy: 0.9302 - lr: 7.1790e-05\n",
            "Epoch 142/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2187 - accuracy: 0.9664 - val_loss: 0.3106 - val_accuracy: 0.9306 - lr: 6.4611e-05\n",
            "Epoch 143/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2185 - accuracy: 0.9673 - val_loss: 0.3115 - val_accuracy: 0.9286 - lr: 6.4611e-05\n",
            "Epoch 144/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2188 - accuracy: 0.9676 - val_loss: 0.3050 - val_accuracy: 0.9300 - lr: 6.4611e-05\n",
            "Epoch 145/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2170 - accuracy: 0.9693 - val_loss: 0.3105 - val_accuracy: 0.9297 - lr: 6.4611e-05\n",
            "Epoch 146/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2182 - accuracy: 0.9683 - val_loss: 0.3107 - val_accuracy: 0.9301 - lr: 6.4611e-05\n",
            "Epoch 147/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2182 - accuracy: 0.9669 - val_loss: 0.3065 - val_accuracy: 0.9328 - lr: 6.4611e-05\n",
            "Epoch 148/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2173 - accuracy: 0.9689 - val_loss: 0.3084 - val_accuracy: 0.9289 - lr: 5.8150e-05\n",
            "Epoch 149/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2186 - accuracy: 0.9675 - val_loss: 0.3093 - val_accuracy: 0.9300 - lr: 5.8150e-05\n",
            "Epoch 150/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2164 - accuracy: 0.9680 - val_loss: 0.3067 - val_accuracy: 0.9287 - lr: 5.8150e-05\n",
            "Epoch 151/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2178 - accuracy: 0.9680 - val_loss: 0.3070 - val_accuracy: 0.9303 - lr: 5.2335e-05\n",
            "Epoch 152/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2174 - accuracy: 0.9689 - val_loss: 0.3127 - val_accuracy: 0.9299 - lr: 5.2335e-05\n",
            "Epoch 153/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2178 - accuracy: 0.9663 - val_loss: 0.3101 - val_accuracy: 0.9313 - lr: 5.2335e-05\n",
            "Epoch 154/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2164 - accuracy: 0.9691 - val_loss: 0.3104 - val_accuracy: 0.9283 - lr: 4.7101e-05\n",
            "Epoch 155/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2161 - accuracy: 0.9702 - val_loss: 0.3078 - val_accuracy: 0.9290 - lr: 4.7101e-05\n",
            "Epoch 156/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2169 - accuracy: 0.9683 - val_loss: 0.3101 - val_accuracy: 0.9289 - lr: 4.7101e-05\n",
            "Epoch 157/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2163 - accuracy: 0.9689 - val_loss: 0.3103 - val_accuracy: 0.9296 - lr: 4.2391e-05\n",
            "Epoch 158/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2168 - accuracy: 0.9695 - val_loss: 0.3051 - val_accuracy: 0.9310 - lr: 4.2391e-05\n",
            "Epoch 159/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2163 - accuracy: 0.9694 - val_loss: 0.3092 - val_accuracy: 0.9322 - lr: 4.2391e-05\n",
            "Epoch 160/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2172 - accuracy: 0.9666 - val_loss: 0.3089 - val_accuracy: 0.9264 - lr: 3.8152e-05\n",
            "Epoch 161/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2163 - accuracy: 0.9683 - val_loss: 0.3056 - val_accuracy: 0.9317 - lr: 3.8152e-05\n",
            "Epoch 162/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2164 - accuracy: 0.9681 - val_loss: 0.3113 - val_accuracy: 0.9281 - lr: 3.8152e-05\n",
            "Epoch 163/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2158 - accuracy: 0.9695 - val_loss: 0.3079 - val_accuracy: 0.9290 - lr: 3.4337e-05\n",
            "Epoch 164/2000\n",
            "383/383 [==============================] - 4s 11ms/step - loss: 0.2161 - accuracy: 0.9697 - val_loss: 0.3077 - val_accuracy: 0.9308 - lr: 3.4337e-05\n",
            "Epoch 165/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2163 - accuracy: 0.9687 - val_loss: 0.3064 - val_accuracy: 0.9297 - lr: 3.4337e-05\n",
            "Epoch 166/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2151 - accuracy: 0.9686 - val_loss: 0.3097 - val_accuracy: 0.9288 - lr: 3.0903e-05\n",
            "Epoch 167/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2160 - accuracy: 0.9693 - val_loss: 0.3093 - val_accuracy: 0.9281 - lr: 3.0903e-05\n",
            "Epoch 168/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2162 - accuracy: 0.9684 - val_loss: 0.3083 - val_accuracy: 0.9295 - lr: 3.0903e-05\n",
            "Epoch 169/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2163 - accuracy: 0.9703 - val_loss: 0.3087 - val_accuracy: 0.9307 - lr: 2.7813e-05\n",
            "Epoch 170/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2150 - accuracy: 0.9703 - val_loss: 0.3090 - val_accuracy: 0.9282 - lr: 2.7813e-05\n",
            "Epoch 171/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2146 - accuracy: 0.9687 - val_loss: 0.3065 - val_accuracy: 0.9299 - lr: 2.7813e-05\n",
            "Epoch 172/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2154 - accuracy: 0.9702 - val_loss: 0.3126 - val_accuracy: 0.9274 - lr: 2.5032e-05\n",
            "Epoch 173/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2149 - accuracy: 0.9711 - val_loss: 0.3084 - val_accuracy: 0.9294 - lr: 2.5032e-05\n",
            "Epoch 174/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2148 - accuracy: 0.9687 - val_loss: 0.3055 - val_accuracy: 0.9302 - lr: 2.5032e-05\n",
            "Epoch 175/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2156 - accuracy: 0.9697 - val_loss: 0.3062 - val_accuracy: 0.9305 - lr: 2.2528e-05\n",
            "Epoch 176/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2142 - accuracy: 0.9714 - val_loss: 0.3066 - val_accuracy: 0.9320 - lr: 2.2528e-05\n",
            "Epoch 177/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2164 - accuracy: 0.9689 - val_loss: 0.3062 - val_accuracy: 0.9305 - lr: 2.2528e-05\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0e700ac2d0>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "#######HYPERPARAMATERS###########\n",
        "epochs = 2000\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "#################################\n",
        "  \n",
        "model = Sequential()\n",
        "    \n",
        "model.add(Conv2D(32, kernel_size = (3,3), input_shape=(28,28,1), activation='relu'))\n",
        "\n",
        "model.add(Conv2D(64,(3,3), activation='relu'))\n",
        "model.add(Conv2D(64,(3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(PermaDropout(0.5))\n",
        "\n",
        "model.add(Conv2D(128,(3,3), activation='relu'))\n",
        "model.add(Conv2D(128,(3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(PermaDropout(0.5))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(PermaDropout(0.5))\n",
        "\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "adam = keras.optimizers.Adam(learning_rate)\n",
        "model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    \n",
        "# print(model.summary())\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
        "early_stopper = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=30, mode='auto')\n",
        "checkpointer = ModelCheckpoint('weights.hd5', monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "\n",
        "model.fit(\n",
        "          train_data_1,\n",
        "          train_labels_1,\n",
        "          epochs = epochs,\n",
        "          batch_size = batch_size,\n",
        "          validation_split = 0.3,\n",
        "          shuffle = True,\n",
        "          callbacks=[lr_reducer, early_stopper]\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_zB1OL252B3"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/MyDrive/파이썬스터디 프로젝트/labeling_cnn2.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbEZKkhn52B4",
        "outputId": "5cb1c6de-40ff-42f1-c138-c7c1442822f2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.895"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "model = keras.models.load_model('/content/drive/MyDrive/파이썬스터디 프로젝트/labeling_cnn2.h5')\n",
        "model_eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KTK643Nkt4E",
        "outputId": "da482a4a-47c5-4871-eba9-70f98b2f1e93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2000\n",
            "383/383 [==============================] - 5s 11ms/step - loss: 1.3102 - accuracy: 0.5779 - val_loss: 0.7070 - val_accuracy: 0.7507 - lr: 0.0010\n",
            "Epoch 2/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.6338 - accuracy: 0.7814 - val_loss: 0.5697 - val_accuracy: 0.8127 - lr: 0.0010\n",
            "Epoch 3/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.5559 - accuracy: 0.8154 - val_loss: 0.5158 - val_accuracy: 0.8337 - lr: 0.0010\n",
            "Epoch 4/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.5010 - accuracy: 0.8418 - val_loss: 0.4802 - val_accuracy: 0.8527 - lr: 0.0010\n",
            "Epoch 5/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.4655 - accuracy: 0.8553 - val_loss: 0.4506 - val_accuracy: 0.8637 - lr: 0.0010\n",
            "Epoch 6/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.4408 - accuracy: 0.8658 - val_loss: 0.4524 - val_accuracy: 0.8623 - lr: 0.0010\n",
            "Epoch 7/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.4275 - accuracy: 0.8711 - val_loss: 0.4266 - val_accuracy: 0.8743 - lr: 0.0010\n",
            "Epoch 8/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.4129 - accuracy: 0.8790 - val_loss: 0.4233 - val_accuracy: 0.8745 - lr: 0.0010\n",
            "Epoch 9/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.4019 - accuracy: 0.8853 - val_loss: 0.3994 - val_accuracy: 0.8856 - lr: 0.0010\n",
            "Epoch 10/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3895 - accuracy: 0.8876 - val_loss: 0.4112 - val_accuracy: 0.8819 - lr: 0.0010\n",
            "Epoch 11/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3878 - accuracy: 0.8892 - val_loss: 0.4001 - val_accuracy: 0.8881 - lr: 0.0010\n",
            "Epoch 12/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3785 - accuracy: 0.8917 - val_loss: 0.3945 - val_accuracy: 0.8877 - lr: 0.0010\n",
            "Epoch 13/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3714 - accuracy: 0.8966 - val_loss: 0.4042 - val_accuracy: 0.8780 - lr: 0.0010\n",
            "Epoch 14/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3725 - accuracy: 0.8962 - val_loss: 0.3906 - val_accuracy: 0.8910 - lr: 0.0010\n",
            "Epoch 15/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3670 - accuracy: 0.8978 - val_loss: 0.3845 - val_accuracy: 0.8943 - lr: 0.0010\n",
            "Epoch 16/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3630 - accuracy: 0.8991 - val_loss: 0.3837 - val_accuracy: 0.8910 - lr: 0.0010\n",
            "Epoch 17/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3585 - accuracy: 0.9009 - val_loss: 0.3852 - val_accuracy: 0.8925 - lr: 0.0010\n",
            "Epoch 18/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3560 - accuracy: 0.9022 - val_loss: 0.3732 - val_accuracy: 0.9004 - lr: 0.0010\n",
            "Epoch 19/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3569 - accuracy: 0.9013 - val_loss: 0.3772 - val_accuracy: 0.8931 - lr: 0.0010\n",
            "Epoch 20/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3460 - accuracy: 0.9051 - val_loss: 0.3695 - val_accuracy: 0.9007 - lr: 0.0010\n",
            "Epoch 21/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3435 - accuracy: 0.9082 - val_loss: 0.3812 - val_accuracy: 0.8971 - lr: 0.0010\n",
            "Epoch 22/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3467 - accuracy: 0.9070 - val_loss: 0.3747 - val_accuracy: 0.8983 - lr: 0.0010\n",
            "Epoch 23/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3426 - accuracy: 0.9088 - val_loss: 0.3763 - val_accuracy: 0.8971 - lr: 0.0010\n",
            "Epoch 24/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3363 - accuracy: 0.9090 - val_loss: 0.3694 - val_accuracy: 0.8987 - lr: 9.0000e-04\n",
            "Epoch 25/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3314 - accuracy: 0.9111 - val_loss: 0.3630 - val_accuracy: 0.9022 - lr: 9.0000e-04\n",
            "Epoch 26/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3307 - accuracy: 0.9110 - val_loss: 0.3692 - val_accuracy: 0.8990 - lr: 9.0000e-04\n",
            "Epoch 27/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3303 - accuracy: 0.9125 - val_loss: 0.3623 - val_accuracy: 0.9039 - lr: 9.0000e-04\n",
            "Epoch 28/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3288 - accuracy: 0.9162 - val_loss: 0.3617 - val_accuracy: 0.9044 - lr: 9.0000e-04\n",
            "Epoch 29/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3238 - accuracy: 0.9159 - val_loss: 0.3616 - val_accuracy: 0.9035 - lr: 9.0000e-04\n",
            "Epoch 30/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3233 - accuracy: 0.9167 - val_loss: 0.3594 - val_accuracy: 0.9031 - lr: 9.0000e-04\n",
            "Epoch 31/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3227 - accuracy: 0.9132 - val_loss: 0.3687 - val_accuracy: 0.8977 - lr: 9.0000e-04\n",
            "Epoch 32/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3240 - accuracy: 0.9164 - val_loss: 0.3590 - val_accuracy: 0.9023 - lr: 9.0000e-04\n",
            "Epoch 33/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3195 - accuracy: 0.9167 - val_loss: 0.3565 - val_accuracy: 0.9063 - lr: 9.0000e-04\n",
            "Epoch 34/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3177 - accuracy: 0.9203 - val_loss: 0.3516 - val_accuracy: 0.9066 - lr: 9.0000e-04\n",
            "Epoch 35/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3160 - accuracy: 0.9201 - val_loss: 0.3710 - val_accuracy: 0.9005 - lr: 9.0000e-04\n",
            "Epoch 36/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3138 - accuracy: 0.9200 - val_loss: 0.3559 - val_accuracy: 0.9040 - lr: 9.0000e-04\n",
            "Epoch 37/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3114 - accuracy: 0.9210 - val_loss: 0.3568 - val_accuracy: 0.9060 - lr: 9.0000e-04\n",
            "Epoch 38/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3071 - accuracy: 0.9230 - val_loss: 0.3592 - val_accuracy: 0.9037 - lr: 8.1000e-04\n",
            "Epoch 39/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3077 - accuracy: 0.9236 - val_loss: 0.3599 - val_accuracy: 0.9022 - lr: 8.1000e-04\n",
            "Epoch 40/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3045 - accuracy: 0.9229 - val_loss: 0.3539 - val_accuracy: 0.9079 - lr: 8.1000e-04\n",
            "Epoch 41/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.3008 - accuracy: 0.9238 - val_loss: 0.3567 - val_accuracy: 0.9020 - lr: 7.2900e-04\n",
            "Epoch 42/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2991 - accuracy: 0.9269 - val_loss: 0.3561 - val_accuracy: 0.9068 - lr: 7.2900e-04\n",
            "Epoch 43/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2971 - accuracy: 0.9281 - val_loss: 0.3578 - val_accuracy: 0.9031 - lr: 7.2900e-04\n",
            "Epoch 44/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2942 - accuracy: 0.9285 - val_loss: 0.3525 - val_accuracy: 0.9090 - lr: 6.5610e-04\n",
            "Epoch 45/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2903 - accuracy: 0.9296 - val_loss: 0.3492 - val_accuracy: 0.9096 - lr: 6.5610e-04\n",
            "Epoch 46/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2899 - accuracy: 0.9301 - val_loss: 0.3467 - val_accuracy: 0.9110 - lr: 6.5610e-04\n",
            "Epoch 47/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2887 - accuracy: 0.9311 - val_loss: 0.3599 - val_accuracy: 0.9090 - lr: 6.5610e-04\n",
            "Epoch 48/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2887 - accuracy: 0.9311 - val_loss: 0.3586 - val_accuracy: 0.9090 - lr: 6.5610e-04\n",
            "Epoch 49/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2877 - accuracy: 0.9325 - val_loss: 0.3457 - val_accuracy: 0.9095 - lr: 6.5610e-04\n",
            "Epoch 50/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2866 - accuracy: 0.9334 - val_loss: 0.3554 - val_accuracy: 0.9069 - lr: 6.5610e-04\n",
            "Epoch 51/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2848 - accuracy: 0.9343 - val_loss: 0.3437 - val_accuracy: 0.9098 - lr: 6.5610e-04\n",
            "Epoch 52/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2844 - accuracy: 0.9322 - val_loss: 0.3524 - val_accuracy: 0.9094 - lr: 6.5610e-04\n",
            "Epoch 53/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2854 - accuracy: 0.9326 - val_loss: 0.3569 - val_accuracy: 0.9053 - lr: 6.5610e-04\n",
            "Epoch 54/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2850 - accuracy: 0.9343 - val_loss: 0.3494 - val_accuracy: 0.9093 - lr: 6.5610e-04\n",
            "Epoch 55/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2793 - accuracy: 0.9364 - val_loss: 0.3500 - val_accuracy: 0.9072 - lr: 5.9049e-04\n",
            "Epoch 56/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2802 - accuracy: 0.9354 - val_loss: 0.3509 - val_accuracy: 0.9067 - lr: 5.9049e-04\n",
            "Epoch 57/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2760 - accuracy: 0.9367 - val_loss: 0.3493 - val_accuracy: 0.9104 - lr: 5.9049e-04\n",
            "Epoch 58/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2739 - accuracy: 0.9393 - val_loss: 0.3458 - val_accuracy: 0.9107 - lr: 5.3144e-04\n",
            "Epoch 59/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2737 - accuracy: 0.9403 - val_loss: 0.3483 - val_accuracy: 0.9110 - lr: 5.3144e-04\n",
            "Epoch 60/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2723 - accuracy: 0.9384 - val_loss: 0.3492 - val_accuracy: 0.9108 - lr: 5.3144e-04\n",
            "Epoch 61/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2714 - accuracy: 0.9425 - val_loss: 0.3476 - val_accuracy: 0.9077 - lr: 4.7830e-04\n",
            "Epoch 62/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2716 - accuracy: 0.9409 - val_loss: 0.3449 - val_accuracy: 0.9118 - lr: 4.7830e-04\n",
            "Epoch 63/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2677 - accuracy: 0.9409 - val_loss: 0.3483 - val_accuracy: 0.9149 - lr: 4.7830e-04\n",
            "Epoch 64/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2664 - accuracy: 0.9420 - val_loss: 0.3463 - val_accuracy: 0.9079 - lr: 4.3047e-04\n",
            "Epoch 65/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2627 - accuracy: 0.9444 - val_loss: 0.3529 - val_accuracy: 0.9076 - lr: 4.3047e-04\n",
            "Epoch 66/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2635 - accuracy: 0.9442 - val_loss: 0.3474 - val_accuracy: 0.9094 - lr: 4.3047e-04\n",
            "Epoch 67/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2619 - accuracy: 0.9445 - val_loss: 0.3427 - val_accuracy: 0.9121 - lr: 3.8742e-04\n",
            "Epoch 68/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2596 - accuracy: 0.9449 - val_loss: 0.3427 - val_accuracy: 0.9125 - lr: 3.8742e-04\n",
            "Epoch 69/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2591 - accuracy: 0.9476 - val_loss: 0.3438 - val_accuracy: 0.9091 - lr: 3.8742e-04\n",
            "Epoch 70/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2600 - accuracy: 0.9466 - val_loss: 0.3460 - val_accuracy: 0.9146 - lr: 3.8742e-04\n",
            "Epoch 71/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2582 - accuracy: 0.9462 - val_loss: 0.3406 - val_accuracy: 0.9121 - lr: 3.4868e-04\n",
            "Epoch 72/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2573 - accuracy: 0.9470 - val_loss: 0.3422 - val_accuracy: 0.9138 - lr: 3.4868e-04\n",
            "Epoch 73/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2556 - accuracy: 0.9496 - val_loss: 0.3464 - val_accuracy: 0.9108 - lr: 3.4868e-04\n",
            "Epoch 74/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2547 - accuracy: 0.9489 - val_loss: 0.3453 - val_accuracy: 0.9117 - lr: 3.4868e-04\n",
            "Epoch 75/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2531 - accuracy: 0.9493 - val_loss: 0.3430 - val_accuracy: 0.9114 - lr: 3.1381e-04\n",
            "Epoch 76/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2525 - accuracy: 0.9508 - val_loss: 0.3459 - val_accuracy: 0.9111 - lr: 3.1381e-04\n",
            "Epoch 77/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2517 - accuracy: 0.9507 - val_loss: 0.3531 - val_accuracy: 0.9113 - lr: 3.1381e-04\n",
            "Epoch 78/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2503 - accuracy: 0.9534 - val_loss: 0.3415 - val_accuracy: 0.9150 - lr: 2.8243e-04\n",
            "Epoch 79/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2493 - accuracy: 0.9493 - val_loss: 0.3412 - val_accuracy: 0.9116 - lr: 2.8243e-04\n",
            "Epoch 80/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2506 - accuracy: 0.9518 - val_loss: 0.3450 - val_accuracy: 0.9162 - lr: 2.8243e-04\n",
            "Epoch 81/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2470 - accuracy: 0.9521 - val_loss: 0.3422 - val_accuracy: 0.9117 - lr: 2.5419e-04\n",
            "Epoch 82/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2471 - accuracy: 0.9522 - val_loss: 0.3433 - val_accuracy: 0.9130 - lr: 2.5419e-04\n",
            "Epoch 83/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2468 - accuracy: 0.9522 - val_loss: 0.3405 - val_accuracy: 0.9133 - lr: 2.5419e-04\n",
            "Epoch 84/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2467 - accuracy: 0.9523 - val_loss: 0.3444 - val_accuracy: 0.9130 - lr: 2.2877e-04\n",
            "Epoch 85/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2456 - accuracy: 0.9533 - val_loss: 0.3416 - val_accuracy: 0.9097 - lr: 2.2877e-04\n",
            "Epoch 86/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2448 - accuracy: 0.9533 - val_loss: 0.3400 - val_accuracy: 0.9143 - lr: 2.2877e-04\n",
            "Epoch 87/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2439 - accuracy: 0.9549 - val_loss: 0.3394 - val_accuracy: 0.9148 - lr: 2.2877e-04\n",
            "Epoch 88/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2429 - accuracy: 0.9538 - val_loss: 0.3394 - val_accuracy: 0.9114 - lr: 2.2877e-04\n",
            "Epoch 89/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2440 - accuracy: 0.9549 - val_loss: 0.3395 - val_accuracy: 0.9130 - lr: 2.2877e-04\n",
            "Epoch 90/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2424 - accuracy: 0.9540 - val_loss: 0.3411 - val_accuracy: 0.9157 - lr: 2.2877e-04\n",
            "Epoch 91/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2416 - accuracy: 0.9555 - val_loss: 0.3461 - val_accuracy: 0.9118 - lr: 2.0589e-04\n",
            "Epoch 92/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2407 - accuracy: 0.9559 - val_loss: 0.3421 - val_accuracy: 0.9158 - lr: 2.0589e-04\n",
            "Epoch 93/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2418 - accuracy: 0.9533 - val_loss: 0.3381 - val_accuracy: 0.9156 - lr: 2.0589e-04\n",
            "Epoch 94/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2409 - accuracy: 0.9562 - val_loss: 0.3400 - val_accuracy: 0.9128 - lr: 2.0589e-04\n",
            "Epoch 95/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2411 - accuracy: 0.9560 - val_loss: 0.3379 - val_accuracy: 0.9140 - lr: 2.0589e-04\n",
            "Epoch 96/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2391 - accuracy: 0.9570 - val_loss: 0.3421 - val_accuracy: 0.9140 - lr: 2.0589e-04\n",
            "Epoch 97/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2389 - accuracy: 0.9570 - val_loss: 0.3432 - val_accuracy: 0.9129 - lr: 2.0589e-04\n",
            "Epoch 98/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2391 - accuracy: 0.9562 - val_loss: 0.3425 - val_accuracy: 0.9106 - lr: 2.0589e-04\n",
            "Epoch 99/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2379 - accuracy: 0.9567 - val_loss: 0.3414 - val_accuracy: 0.9144 - lr: 1.8530e-04\n",
            "Epoch 100/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2366 - accuracy: 0.9576 - val_loss: 0.3462 - val_accuracy: 0.9142 - lr: 1.8530e-04\n",
            "Epoch 101/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2379 - accuracy: 0.9572 - val_loss: 0.3420 - val_accuracy: 0.9156 - lr: 1.8530e-04\n",
            "Epoch 102/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2368 - accuracy: 0.9579 - val_loss: 0.3411 - val_accuracy: 0.9130 - lr: 1.6677e-04\n",
            "Epoch 103/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2355 - accuracy: 0.9574 - val_loss: 0.3390 - val_accuracy: 0.9171 - lr: 1.6677e-04\n",
            "Epoch 104/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2355 - accuracy: 0.9593 - val_loss: 0.3434 - val_accuracy: 0.9109 - lr: 1.6677e-04\n",
            "Epoch 105/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2347 - accuracy: 0.9601 - val_loss: 0.3378 - val_accuracy: 0.9120 - lr: 1.5009e-04\n",
            "Epoch 106/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2349 - accuracy: 0.9587 - val_loss: 0.3441 - val_accuracy: 0.9150 - lr: 1.5009e-04\n",
            "Epoch 107/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2338 - accuracy: 0.9600 - val_loss: 0.3378 - val_accuracy: 0.9168 - lr: 1.5009e-04\n",
            "Epoch 108/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2340 - accuracy: 0.9596 - val_loss: 0.3411 - val_accuracy: 0.9171 - lr: 1.3509e-04\n",
            "Epoch 109/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2329 - accuracy: 0.9588 - val_loss: 0.3346 - val_accuracy: 0.9115 - lr: 1.3509e-04\n",
            "Epoch 110/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2336 - accuracy: 0.9611 - val_loss: 0.3357 - val_accuracy: 0.9146 - lr: 1.3509e-04\n",
            "Epoch 111/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2317 - accuracy: 0.9608 - val_loss: 0.3413 - val_accuracy: 0.9137 - lr: 1.3509e-04\n",
            "Epoch 112/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2321 - accuracy: 0.9613 - val_loss: 0.3390 - val_accuracy: 0.9157 - lr: 1.3509e-04\n",
            "Epoch 113/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2314 - accuracy: 0.9618 - val_loss: 0.3394 - val_accuracy: 0.9147 - lr: 1.2158e-04\n",
            "Epoch 114/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2312 - accuracy: 0.9597 - val_loss: 0.3403 - val_accuracy: 0.9132 - lr: 1.2158e-04\n",
            "Epoch 115/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2303 - accuracy: 0.9619 - val_loss: 0.3405 - val_accuracy: 0.9151 - lr: 1.2158e-04\n",
            "Epoch 116/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2322 - accuracy: 0.9585 - val_loss: 0.3394 - val_accuracy: 0.9132 - lr: 1.0942e-04\n",
            "Epoch 117/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2315 - accuracy: 0.9592 - val_loss: 0.3424 - val_accuracy: 0.9140 - lr: 1.0942e-04\n",
            "Epoch 118/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2320 - accuracy: 0.9602 - val_loss: 0.3466 - val_accuracy: 0.9131 - lr: 1.0942e-04\n",
            "Epoch 119/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2300 - accuracy: 0.9604 - val_loss: 0.3363 - val_accuracy: 0.9170 - lr: 9.8477e-05\n",
            "Epoch 120/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2290 - accuracy: 0.9620 - val_loss: 0.3390 - val_accuracy: 0.9148 - lr: 9.8477e-05\n",
            "Epoch 121/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2294 - accuracy: 0.9605 - val_loss: 0.3371 - val_accuracy: 0.9162 - lr: 9.8477e-05\n",
            "Epoch 122/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2295 - accuracy: 0.9613 - val_loss: 0.3375 - val_accuracy: 0.9151 - lr: 8.8629e-05\n",
            "Epoch 123/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2292 - accuracy: 0.9609 - val_loss: 0.3371 - val_accuracy: 0.9137 - lr: 8.8629e-05\n",
            "Epoch 124/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2287 - accuracy: 0.9625 - val_loss: 0.3362 - val_accuracy: 0.9134 - lr: 8.8629e-05\n",
            "Epoch 125/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2274 - accuracy: 0.9640 - val_loss: 0.3401 - val_accuracy: 0.9132 - lr: 7.9766e-05\n",
            "Epoch 126/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2287 - accuracy: 0.9622 - val_loss: 0.3396 - val_accuracy: 0.9143 - lr: 7.9766e-05\n",
            "Epoch 127/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2272 - accuracy: 0.9643 - val_loss: 0.3393 - val_accuracy: 0.9139 - lr: 7.9766e-05\n",
            "Epoch 128/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2284 - accuracy: 0.9617 - val_loss: 0.3373 - val_accuracy: 0.9179 - lr: 7.1790e-05\n",
            "Epoch 129/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2286 - accuracy: 0.9627 - val_loss: 0.3404 - val_accuracy: 0.9162 - lr: 7.1790e-05\n",
            "Epoch 130/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2276 - accuracy: 0.9617 - val_loss: 0.3411 - val_accuracy: 0.9121 - lr: 7.1790e-05\n",
            "Epoch 131/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2277 - accuracy: 0.9627 - val_loss: 0.3406 - val_accuracy: 0.9147 - lr: 6.4611e-05\n",
            "Epoch 132/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2261 - accuracy: 0.9636 - val_loss: 0.3389 - val_accuracy: 0.9149 - lr: 6.4611e-05\n",
            "Epoch 133/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2269 - accuracy: 0.9635 - val_loss: 0.3363 - val_accuracy: 0.9130 - lr: 6.4611e-05\n",
            "Epoch 134/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2272 - accuracy: 0.9620 - val_loss: 0.3419 - val_accuracy: 0.9144 - lr: 5.8150e-05\n",
            "Epoch 135/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2271 - accuracy: 0.9616 - val_loss: 0.3391 - val_accuracy: 0.9142 - lr: 5.8150e-05\n",
            "Epoch 136/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2269 - accuracy: 0.9619 - val_loss: 0.3408 - val_accuracy: 0.9129 - lr: 5.8150e-05\n",
            "Epoch 137/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2248 - accuracy: 0.9648 - val_loss: 0.3350 - val_accuracy: 0.9151 - lr: 5.2335e-05\n",
            "Epoch 138/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2266 - accuracy: 0.9622 - val_loss: 0.3391 - val_accuracy: 0.9156 - lr: 5.2335e-05\n",
            "Epoch 139/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2261 - accuracy: 0.9630 - val_loss: 0.3417 - val_accuracy: 0.9135 - lr: 5.2335e-05\n",
            "Epoch 140/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2264 - accuracy: 0.9619 - val_loss: 0.3373 - val_accuracy: 0.9169 - lr: 4.7101e-05\n",
            "Epoch 141/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2251 - accuracy: 0.9639 - val_loss: 0.3368 - val_accuracy: 0.9148 - lr: 4.7101e-05\n",
            "Epoch 142/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2256 - accuracy: 0.9631 - val_loss: 0.3409 - val_accuracy: 0.9110 - lr: 4.7101e-05\n",
            "Epoch 143/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2259 - accuracy: 0.9624 - val_loss: 0.3479 - val_accuracy: 0.9110 - lr: 4.2391e-05\n",
            "Epoch 144/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2252 - accuracy: 0.9644 - val_loss: 0.3422 - val_accuracy: 0.9145 - lr: 4.2391e-05\n",
            "Epoch 145/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2267 - accuracy: 0.9628 - val_loss: 0.3413 - val_accuracy: 0.9143 - lr: 4.2391e-05\n",
            "Epoch 146/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2249 - accuracy: 0.9641 - val_loss: 0.3434 - val_accuracy: 0.9138 - lr: 3.8152e-05\n",
            "Epoch 147/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2264 - accuracy: 0.9639 - val_loss: 0.3418 - val_accuracy: 0.9114 - lr: 3.8152e-05\n",
            "Epoch 148/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2260 - accuracy: 0.9624 - val_loss: 0.3389 - val_accuracy: 0.9160 - lr: 3.8152e-05\n",
            "Epoch 149/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2247 - accuracy: 0.9635 - val_loss: 0.3388 - val_accuracy: 0.9137 - lr: 3.4337e-05\n",
            "Epoch 150/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2247 - accuracy: 0.9650 - val_loss: 0.3371 - val_accuracy: 0.9132 - lr: 3.4337e-05\n",
            "Epoch 151/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2247 - accuracy: 0.9636 - val_loss: 0.3376 - val_accuracy: 0.9140 - lr: 3.4337e-05\n",
            "Epoch 152/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2246 - accuracy: 0.9634 - val_loss: 0.3396 - val_accuracy: 0.9142 - lr: 3.0903e-05\n",
            "Epoch 153/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2242 - accuracy: 0.9641 - val_loss: 0.3365 - val_accuracy: 0.9161 - lr: 3.0903e-05\n",
            "Epoch 154/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2257 - accuracy: 0.9634 - val_loss: 0.3368 - val_accuracy: 0.9149 - lr: 3.0903e-05\n",
            "Epoch 155/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2235 - accuracy: 0.9635 - val_loss: 0.3375 - val_accuracy: 0.9135 - lr: 2.7813e-05\n",
            "Epoch 156/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2244 - accuracy: 0.9642 - val_loss: 0.3373 - val_accuracy: 0.9178 - lr: 2.7813e-05\n",
            "Epoch 157/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2243 - accuracy: 0.9639 - val_loss: 0.3402 - val_accuracy: 0.9157 - lr: 2.7813e-05\n",
            "Epoch 158/2000\n",
            "383/383 [==============================] - 4s 10ms/step - loss: 0.2238 - accuracy: 0.9636 - val_loss: 0.3382 - val_accuracy: 0.9132 - lr: 2.5032e-05\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0efe2442d0>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "#######HYPERPARAMATERS###########\n",
        "epochs = 2000\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "#################################\n",
        "\n",
        "model = Sequential()\n",
        "    \n",
        "model.add(Conv2D(32, kernel_size = (3,3), input_shape=(28,28,1), activation='relu'))\n",
        "\n",
        "model.add(Conv2D(64,(3,3), activation='relu'))\n",
        "model.add(Conv2D(64,(3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(PermaDropout(0.5))\n",
        "\n",
        "model.add(Conv2D(128,(3,3), activation='relu'))\n",
        "model.add(Conv2D(128,(3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(PermaDropout(0.5))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(PermaDropout(0.5))\n",
        "\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "adam = keras.optimizers.Adam(learning_rate)\n",
        "model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    \n",
        "# print(model.summary())\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
        "early_stopper = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=30, mode='auto')\n",
        "checkpointer = ModelCheckpoint('weights.hd5', monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "model.fit(\n",
        "          train_data_2,\n",
        "          train_labels_2,\n",
        "          epochs = epochs,\n",
        "          batch_size = batch_size,\n",
        "          validation_split = 0.3,\n",
        "          shuffle = True,\n",
        "          callbacks=[lr_reducer, early_stopper]\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d3W-766kt4H"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/MyDrive/파이썬스터디 프로젝트/labeling_cnn_no.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "it79cIfekt4H",
        "outputId": "6ebedc74-2c77-44a8-ad93-5d667eb10dd7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8908"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "model = keras.models.load_model('/content/drive/MyDrive/파이썬스터디 프로젝트/labeling_cnn_no.h5')\n",
        "model_eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Resnet"
      ],
      "metadata": {
        "id": "9u3DDkIzmpzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import Tensor\n",
        "from tensorflow.keras.layers import Input, Conv2D, ReLU, BatchNormalization,\\\n",
        "                                    Add, AveragePooling2D, Flatten, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def PermaDropout(rate):\n",
        "    return Lambda(lambda x: K.dropout(x, level=rate))\n",
        "\n",
        "def relu_bn(inputs: Tensor) -> Tensor:\n",
        "    relu = ReLU()(inputs)\n",
        "    bn = BatchNormalization()(relu)\n",
        "    return bn\n",
        "\n",
        "def residual_block(x: Tensor, downsample: bool, filters: int, kernel_size: int = 3) -> Tensor:\n",
        "    y = Conv2D(kernel_size=kernel_size,\n",
        "               strides= (1 if not downsample else 2),\n",
        "               filters=filters,\n",
        "               padding=\"same\")(x)\n",
        "    y = relu_bn(y)\n",
        "    y = Conv2D(kernel_size=kernel_size,\n",
        "               strides=1,\n",
        "               filters=filters,\n",
        "               padding=\"same\")(y)\n",
        "\n",
        "    if downsample:\n",
        "        x = Conv2D(kernel_size=1,\n",
        "                   strides=2,\n",
        "                   filters=filters,\n",
        "                   padding=\"same\")(x)\n",
        "    out = Add()([x, y])\n",
        "    out = relu_bn(out)\n",
        "    return out\n",
        "\n",
        "def create_res_net():\n",
        "    \n",
        "    inputs = Input(shape=(28, 28, 1))\n",
        "    num_filters = 64\n",
        "    \n",
        "    t = BatchNormalization()(inputs)\n",
        "    t = Conv2D(kernel_size=3,\n",
        "               strides=1,\n",
        "               filters=num_filters,\n",
        "               padding=\"same\")(t)\n",
        "    t = relu_bn(t)\n",
        "    t = PermaDropout(0.5)(t)\n",
        "    \n",
        "    num_blocks_list = [2, 5, 5, 2]\n",
        "    for i in range(len(num_blocks_list)):\n",
        "        num_blocks = num_blocks_list[i]\n",
        "        for j in range(num_blocks):\n",
        "            t = residual_block(t, downsample=(j==0 and i!=0), filters=num_filters)\n",
        "        num_filters *= 2\n",
        "        t = PermaDropout(0.5)(t)\n",
        "    \n",
        "    t = AveragePooling2D(4)(t)\n",
        "    t = Flatten()(t)\n",
        "    outputs = Dense(10, activation='softmax')(t)\n",
        "    \n",
        "    model = Model(inputs, outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "BrPuKrurotJ6"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_res_net()\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
        "early_stopper = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=30, mode='auto')\n",
        "\n",
        "model.fit(\n",
        "    x=train_data_1,\n",
        "    y=train_labels_1,\n",
        "    epochs=500,\n",
        "    verbose=1,\n",
        "    validation_split=0.3,\n",
        "    batch_size=64,\n",
        "    callbacks=[lr_reducer, early_stopper]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33b20f1d-0e60-46a3-c365-d7293eaf7b66",
        "id": "mQJhYaz-otKF"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "383/383 [==============================] - 125s 310ms/step - loss: 0.6916 - accuracy: 0.7503 - val_loss: 1.3493 - val_accuracy: 0.6064 - lr: 0.0010\n",
            "Epoch 2/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.3944 - accuracy: 0.8613 - val_loss: 0.5656 - val_accuracy: 0.8303 - lr: 0.0010\n",
            "Epoch 3/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.3401 - accuracy: 0.8839 - val_loss: 0.3568 - val_accuracy: 0.8836 - lr: 0.0010\n",
            "Epoch 4/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.3054 - accuracy: 0.8961 - val_loss: 0.4000 - val_accuracy: 0.8769 - lr: 0.0010\n",
            "Epoch 5/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.2870 - accuracy: 0.9028 - val_loss: 0.4595 - val_accuracy: 0.8676 - lr: 0.0010\n",
            "Epoch 6/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.2667 - accuracy: 0.9107 - val_loss: 0.3334 - val_accuracy: 0.8914 - lr: 0.0010\n",
            "Epoch 7/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.2544 - accuracy: 0.9165 - val_loss: 0.3432 - val_accuracy: 0.8911 - lr: 0.0010\n",
            "Epoch 8/500\n",
            "383/383 [==============================] - 124s 323ms/step - loss: 0.2488 - accuracy: 0.9177 - val_loss: 0.3190 - val_accuracy: 0.9015 - lr: 0.0010\n",
            "Epoch 9/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.2376 - accuracy: 0.9233 - val_loss: 0.3028 - val_accuracy: 0.9061 - lr: 0.0010\n",
            "Epoch 10/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.2251 - accuracy: 0.9278 - val_loss: 0.2862 - val_accuracy: 0.9130 - lr: 0.0010\n",
            "Epoch 11/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.2195 - accuracy: 0.9291 - val_loss: 0.2904 - val_accuracy: 0.9094 - lr: 0.0010\n",
            "Epoch 12/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.2118 - accuracy: 0.9344 - val_loss: 0.2733 - val_accuracy: 0.9142 - lr: 0.0010\n",
            "Epoch 13/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.1942 - accuracy: 0.9407 - val_loss: 0.2875 - val_accuracy: 0.9095 - lr: 0.0010\n",
            "Epoch 14/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.1942 - accuracy: 0.9398 - val_loss: 0.3202 - val_accuracy: 0.9010 - lr: 0.0010\n",
            "Epoch 15/500\n",
            "383/383 [==============================] - 116s 302ms/step - loss: 0.1866 - accuracy: 0.9424 - val_loss: 0.2685 - val_accuracy: 0.9176 - lr: 0.0010\n",
            "Epoch 16/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.1802 - accuracy: 0.9470 - val_loss: 0.2640 - val_accuracy: 0.9203 - lr: 0.0010\n",
            "Epoch 17/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.1690 - accuracy: 0.9511 - val_loss: 0.2379 - val_accuracy: 0.9274 - lr: 0.0010\n",
            "Epoch 18/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.1632 - accuracy: 0.9532 - val_loss: 0.2667 - val_accuracy: 0.9242 - lr: 0.0010\n",
            "Epoch 19/500\n",
            "383/383 [==============================] - 116s 302ms/step - loss: 0.1626 - accuracy: 0.9535 - val_loss: 0.2480 - val_accuracy: 0.9258 - lr: 0.0010\n",
            "Epoch 20/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.1513 - accuracy: 0.9577 - val_loss: 0.2450 - val_accuracy: 0.9261 - lr: 0.0010\n",
            "Epoch 21/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.1376 - accuracy: 0.9635 - val_loss: 0.2460 - val_accuracy: 0.9267 - lr: 9.0000e-04\n",
            "Epoch 22/500\n",
            "383/383 [==============================] - 116s 302ms/step - loss: 0.1351 - accuracy: 0.9659 - val_loss: 0.2147 - val_accuracy: 0.9421 - lr: 9.0000e-04\n",
            "Epoch 23/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.1329 - accuracy: 0.9663 - val_loss: 0.2216 - val_accuracy: 0.9403 - lr: 9.0000e-04\n",
            "Epoch 24/500\n",
            "383/383 [==============================] - 116s 302ms/step - loss: 0.1278 - accuracy: 0.9680 - val_loss: 0.2304 - val_accuracy: 0.9332 - lr: 9.0000e-04\n",
            "Epoch 25/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.1213 - accuracy: 0.9705 - val_loss: 0.2204 - val_accuracy: 0.9406 - lr: 9.0000e-04\n",
            "Epoch 26/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.1087 - accuracy: 0.9749 - val_loss: 0.2334 - val_accuracy: 0.9381 - lr: 8.1000e-04\n",
            "Epoch 27/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.1064 - accuracy: 0.9767 - val_loss: 0.2107 - val_accuracy: 0.9432 - lr: 8.1000e-04\n",
            "Epoch 28/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.1038 - accuracy: 0.9780 - val_loss: 0.2143 - val_accuracy: 0.9416 - lr: 8.1000e-04\n",
            "Epoch 29/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.1078 - accuracy: 0.9768 - val_loss: 0.3332 - val_accuracy: 0.9163 - lr: 8.1000e-04\n",
            "Epoch 30/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.1114 - accuracy: 0.9740 - val_loss: 0.2279 - val_accuracy: 0.9385 - lr: 8.1000e-04\n",
            "Epoch 31/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0961 - accuracy: 0.9806 - val_loss: 0.2036 - val_accuracy: 0.9463 - lr: 7.2900e-04\n",
            "Epoch 32/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0879 - accuracy: 0.9838 - val_loss: 0.1954 - val_accuracy: 0.9467 - lr: 7.2900e-04\n",
            "Epoch 33/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0896 - accuracy: 0.9827 - val_loss: 0.1998 - val_accuracy: 0.9463 - lr: 7.2900e-04\n",
            "Epoch 34/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0887 - accuracy: 0.9822 - val_loss: 0.2021 - val_accuracy: 0.9429 - lr: 7.2900e-04\n",
            "Epoch 35/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0904 - accuracy: 0.9819 - val_loss: 0.2106 - val_accuracy: 0.9447 - lr: 7.2900e-04\n",
            "Epoch 36/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0866 - accuracy: 0.9827 - val_loss: 0.2034 - val_accuracy: 0.9478 - lr: 6.5610e-04\n",
            "Epoch 37/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0831 - accuracy: 0.9840 - val_loss: 0.1981 - val_accuracy: 0.9472 - lr: 6.5610e-04\n",
            "Epoch 38/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0821 - accuracy: 0.9845 - val_loss: 0.2001 - val_accuracy: 0.9477 - lr: 6.5610e-04\n",
            "Epoch 39/500\n",
            "383/383 [==============================] - 116s 302ms/step - loss: 0.0785 - accuracy: 0.9866 - val_loss: 0.1993 - val_accuracy: 0.9450 - lr: 5.9049e-04\n",
            "Epoch 40/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0799 - accuracy: 0.9849 - val_loss: 0.2021 - val_accuracy: 0.9452 - lr: 5.9049e-04\n",
            "Epoch 41/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0777 - accuracy: 0.9853 - val_loss: 0.1968 - val_accuracy: 0.9480 - lr: 5.9049e-04\n",
            "Epoch 42/500\n",
            "383/383 [==============================] - 116s 302ms/step - loss: 0.0740 - accuracy: 0.9876 - val_loss: 0.1913 - val_accuracy: 0.9516 - lr: 5.3144e-04\n",
            "Epoch 43/500\n",
            "383/383 [==============================] - 116s 302ms/step - loss: 0.0732 - accuracy: 0.9883 - val_loss: 0.1888 - val_accuracy: 0.9499 - lr: 5.3144e-04\n",
            "Epoch 44/500\n",
            "383/383 [==============================] - 116s 302ms/step - loss: 0.0740 - accuracy: 0.9875 - val_loss: 0.1989 - val_accuracy: 0.9493 - lr: 5.3144e-04\n",
            "Epoch 45/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0742 - accuracy: 0.9870 - val_loss: 0.1990 - val_accuracy: 0.9502 - lr: 5.3144e-04\n",
            "Epoch 46/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0734 - accuracy: 0.9869 - val_loss: 0.1913 - val_accuracy: 0.9492 - lr: 5.3144e-04\n",
            "Epoch 47/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0708 - accuracy: 0.9885 - val_loss: 0.1888 - val_accuracy: 0.9531 - lr: 4.7830e-04\n",
            "Epoch 48/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0697 - accuracy: 0.9880 - val_loss: 0.1926 - val_accuracy: 0.9513 - lr: 4.7830e-04\n",
            "Epoch 49/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0707 - accuracy: 0.9888 - val_loss: 0.1987 - val_accuracy: 0.9501 - lr: 4.7830e-04\n",
            "Epoch 50/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0702 - accuracy: 0.9897 - val_loss: 0.1863 - val_accuracy: 0.9514 - lr: 4.3047e-04\n",
            "Epoch 51/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0682 - accuracy: 0.9889 - val_loss: 0.1945 - val_accuracy: 0.9538 - lr: 4.3047e-04\n",
            "Epoch 52/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0684 - accuracy: 0.9893 - val_loss: 0.1919 - val_accuracy: 0.9504 - lr: 4.3047e-04\n",
            "Epoch 53/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0685 - accuracy: 0.9893 - val_loss: 0.1960 - val_accuracy: 0.9506 - lr: 4.3047e-04\n",
            "Epoch 54/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0675 - accuracy: 0.9896 - val_loss: 0.2031 - val_accuracy: 0.9501 - lr: 3.8742e-04\n",
            "Epoch 55/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0666 - accuracy: 0.9911 - val_loss: 0.1904 - val_accuracy: 0.9538 - lr: 3.8742e-04\n",
            "Epoch 56/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0650 - accuracy: 0.9920 - val_loss: 0.1900 - val_accuracy: 0.9544 - lr: 3.8742e-04\n",
            "Epoch 57/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0659 - accuracy: 0.9915 - val_loss: 0.1942 - val_accuracy: 0.9524 - lr: 3.4868e-04\n",
            "Epoch 58/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0650 - accuracy: 0.9918 - val_loss: 0.1927 - val_accuracy: 0.9520 - lr: 3.4868e-04\n",
            "Epoch 59/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0645 - accuracy: 0.9927 - val_loss: 0.1882 - val_accuracy: 0.9532 - lr: 3.4868e-04\n",
            "Epoch 60/500\n",
            "383/383 [==============================] - 116s 302ms/step - loss: 0.0641 - accuracy: 0.9916 - val_loss: 0.1926 - val_accuracy: 0.9545 - lr: 3.1381e-04\n",
            "Epoch 61/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0634 - accuracy: 0.9929 - val_loss: 0.1905 - val_accuracy: 0.9559 - lr: 3.1381e-04\n",
            "Epoch 62/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0635 - accuracy: 0.9921 - val_loss: 0.1909 - val_accuracy: 0.9563 - lr: 3.1381e-04\n",
            "Epoch 63/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0633 - accuracy: 0.9926 - val_loss: 0.1909 - val_accuracy: 0.9565 - lr: 2.8243e-04\n",
            "Epoch 64/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0647 - accuracy: 0.9927 - val_loss: 0.1948 - val_accuracy: 0.9540 - lr: 2.8243e-04\n",
            "Epoch 65/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0631 - accuracy: 0.9929 - val_loss: 0.1937 - val_accuracy: 0.9560 - lr: 2.8243e-04\n",
            "Epoch 66/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0627 - accuracy: 0.9936 - val_loss: 0.1872 - val_accuracy: 0.9559 - lr: 2.5419e-04\n",
            "Epoch 67/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0627 - accuracy: 0.9933 - val_loss: 0.1959 - val_accuracy: 0.9557 - lr: 2.5419e-04\n",
            "Epoch 68/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0629 - accuracy: 0.9931 - val_loss: 0.1904 - val_accuracy: 0.9578 - lr: 2.5419e-04\n",
            "Epoch 69/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0621 - accuracy: 0.9947 - val_loss: 0.1954 - val_accuracy: 0.9553 - lr: 2.2877e-04\n",
            "Epoch 70/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0622 - accuracy: 0.9935 - val_loss: 0.1906 - val_accuracy: 0.9551 - lr: 2.2877e-04\n",
            "Epoch 71/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0620 - accuracy: 0.9937 - val_loss: 0.1923 - val_accuracy: 0.9567 - lr: 2.2877e-04\n",
            "Epoch 72/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0618 - accuracy: 0.9939 - val_loss: 0.1902 - val_accuracy: 0.9567 - lr: 2.0589e-04\n",
            "Epoch 73/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0617 - accuracy: 0.9935 - val_loss: 0.1922 - val_accuracy: 0.9556 - lr: 2.0589e-04\n",
            "Epoch 74/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0617 - accuracy: 0.9944 - val_loss: 0.1911 - val_accuracy: 0.9577 - lr: 2.0589e-04\n",
            "Epoch 75/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0616 - accuracy: 0.9943 - val_loss: 0.1915 - val_accuracy: 0.9581 - lr: 1.8530e-04\n",
            "Epoch 76/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0617 - accuracy: 0.9937 - val_loss: 0.1928 - val_accuracy: 0.9571 - lr: 1.8530e-04\n",
            "Epoch 77/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0613 - accuracy: 0.9945 - val_loss: 0.1919 - val_accuracy: 0.9576 - lr: 1.8530e-04\n",
            "Epoch 78/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0612 - accuracy: 0.9953 - val_loss: 0.1937 - val_accuracy: 0.9554 - lr: 1.6677e-04\n",
            "Epoch 79/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0611 - accuracy: 0.9941 - val_loss: 0.1951 - val_accuracy: 0.9567 - lr: 1.6677e-04\n",
            "Epoch 80/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0610 - accuracy: 0.9947 - val_loss: 0.1962 - val_accuracy: 0.9570 - lr: 1.6677e-04\n",
            "Epoch 81/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0611 - accuracy: 0.9944 - val_loss: 0.1938 - val_accuracy: 0.9567 - lr: 1.5009e-04\n",
            "Epoch 82/500\n",
            "383/383 [==============================] - 124s 323ms/step - loss: 0.0609 - accuracy: 0.9952 - val_loss: 0.1887 - val_accuracy: 0.9595 - lr: 1.5009e-04\n",
            "Epoch 83/500\n",
            "383/383 [==============================] - 116s 304ms/step - loss: 0.0608 - accuracy: 0.9942 - val_loss: 0.1911 - val_accuracy: 0.9568 - lr: 1.5009e-04\n",
            "Epoch 84/500\n",
            "383/383 [==============================] - 116s 304ms/step - loss: 0.0607 - accuracy: 0.9947 - val_loss: 0.1943 - val_accuracy: 0.9587 - lr: 1.3509e-04\n",
            "Epoch 85/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0608 - accuracy: 0.9949 - val_loss: 0.1976 - val_accuracy: 0.9564 - lr: 1.3509e-04\n",
            "Epoch 86/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0607 - accuracy: 0.9954 - val_loss: 0.1952 - val_accuracy: 0.9586 - lr: 1.3509e-04\n",
            "Epoch 87/500\n",
            "383/383 [==============================] - 116s 304ms/step - loss: 0.0606 - accuracy: 0.9951 - val_loss: 0.1963 - val_accuracy: 0.9562 - lr: 1.2158e-04\n",
            "Epoch 88/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0606 - accuracy: 0.9950 - val_loss: 0.1958 - val_accuracy: 0.9570 - lr: 1.2158e-04\n",
            "Epoch 89/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0605 - accuracy: 0.9956 - val_loss: 0.1907 - val_accuracy: 0.9572 - lr: 1.2158e-04\n",
            "Epoch 90/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0605 - accuracy: 0.9954 - val_loss: 0.1981 - val_accuracy: 0.9571 - lr: 1.0942e-04\n",
            "Epoch 91/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0604 - accuracy: 0.9951 - val_loss: 0.1972 - val_accuracy: 0.9593 - lr: 1.0942e-04\n",
            "Epoch 92/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0604 - accuracy: 0.9954 - val_loss: 0.1932 - val_accuracy: 0.9592 - lr: 1.0942e-04\n",
            "Epoch 93/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0604 - accuracy: 0.9956 - val_loss: 0.1942 - val_accuracy: 0.9574 - lr: 9.8477e-05\n",
            "Epoch 94/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0603 - accuracy: 0.9957 - val_loss: 0.1964 - val_accuracy: 0.9586 - lr: 9.8477e-05\n",
            "Epoch 95/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0604 - accuracy: 0.9952 - val_loss: 0.1984 - val_accuracy: 0.9582 - lr: 9.8477e-05\n",
            "Epoch 96/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0602 - accuracy: 0.9956 - val_loss: 0.1908 - val_accuracy: 0.9587 - lr: 8.8629e-05\n",
            "Epoch 97/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0602 - accuracy: 0.9960 - val_loss: 0.1922 - val_accuracy: 0.9590 - lr: 8.8629e-05\n",
            "Epoch 98/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0602 - accuracy: 0.9956 - val_loss: 0.1962 - val_accuracy: 0.9567 - lr: 8.8629e-05\n",
            "Epoch 99/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0601 - accuracy: 0.9959 - val_loss: 0.1981 - val_accuracy: 0.9590 - lr: 7.9766e-05\n",
            "Epoch 100/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0602 - accuracy: 0.9949 - val_loss: 0.1952 - val_accuracy: 0.9583 - lr: 7.9766e-05\n",
            "Epoch 101/500\n",
            "383/383 [==============================] - 116s 304ms/step - loss: 0.0601 - accuracy: 0.9964 - val_loss: 0.1925 - val_accuracy: 0.9571 - lr: 7.9766e-05\n",
            "Epoch 102/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0601 - accuracy: 0.9956 - val_loss: 0.1948 - val_accuracy: 0.9581 - lr: 7.1790e-05\n",
            "Epoch 103/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0601 - accuracy: 0.9948 - val_loss: 0.1948 - val_accuracy: 0.9575 - lr: 7.1790e-05\n",
            "Epoch 104/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0601 - accuracy: 0.9956 - val_loss: 0.1990 - val_accuracy: 0.9581 - lr: 7.1790e-05\n",
            "Epoch 105/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0601 - accuracy: 0.9957 - val_loss: 0.1958 - val_accuracy: 0.9578 - lr: 6.4611e-05\n",
            "Epoch 106/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0600 - accuracy: 0.9956 - val_loss: 0.1959 - val_accuracy: 0.9593 - lr: 6.4611e-05\n",
            "Epoch 107/500\n",
            "383/383 [==============================] - 116s 304ms/step - loss: 0.0600 - accuracy: 0.9964 - val_loss: 0.1963 - val_accuracy: 0.9574 - lr: 6.4611e-05\n",
            "Epoch 108/500\n",
            "383/383 [==============================] - 116s 304ms/step - loss: 0.0600 - accuracy: 0.9953 - val_loss: 0.1934 - val_accuracy: 0.9598 - lr: 5.8150e-05\n",
            "Epoch 109/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0600 - accuracy: 0.9956 - val_loss: 0.1982 - val_accuracy: 0.9584 - lr: 5.8150e-05\n",
            "Epoch 110/500\n",
            "383/383 [==============================] - 116s 304ms/step - loss: 0.0599 - accuracy: 0.9961 - val_loss: 0.1969 - val_accuracy: 0.9584 - lr: 5.8150e-05\n",
            "Epoch 111/500\n",
            "383/383 [==============================] - 124s 324ms/step - loss: 0.0600 - accuracy: 0.9959 - val_loss: 0.1948 - val_accuracy: 0.9603 - lr: 5.2335e-05\n",
            "Epoch 112/500\n",
            "383/383 [==============================] - 124s 325ms/step - loss: 0.0599 - accuracy: 0.9965 - val_loss: 0.1943 - val_accuracy: 0.9580 - lr: 5.2335e-05\n",
            "Epoch 113/500\n",
            "383/383 [==============================] - 117s 305ms/step - loss: 0.0599 - accuracy: 0.9959 - val_loss: 0.1953 - val_accuracy: 0.9577 - lr: 5.2335e-05\n",
            "Epoch 114/500\n",
            "383/383 [==============================] - 117s 305ms/step - loss: 0.0599 - accuracy: 0.9960 - val_loss: 0.1981 - val_accuracy: 0.9573 - lr: 4.7101e-05\n",
            "Epoch 115/500\n",
            "383/383 [==============================] - 116s 304ms/step - loss: 0.0598 - accuracy: 0.9960 - val_loss: 0.1931 - val_accuracy: 0.9587 - lr: 4.7101e-05\n",
            "Epoch 116/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0599 - accuracy: 0.9956 - val_loss: 0.1939 - val_accuracy: 0.9573 - lr: 4.7101e-05\n",
            "Epoch 117/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0599 - accuracy: 0.9958 - val_loss: 0.1954 - val_accuracy: 0.9587 - lr: 4.2391e-05\n",
            "Epoch 118/500\n",
            "383/383 [==============================] - 116s 304ms/step - loss: 0.0598 - accuracy: 0.9964 - val_loss: 0.1981 - val_accuracy: 0.9595 - lr: 4.2391e-05\n",
            "Epoch 119/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0598 - accuracy: 0.9955 - val_loss: 0.1993 - val_accuracy: 0.9589 - lr: 4.2391e-05\n",
            "Epoch 120/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0598 - accuracy: 0.9961 - val_loss: 0.1985 - val_accuracy: 0.9579 - lr: 3.8152e-05\n",
            "Epoch 121/500\n",
            "383/383 [==============================] - 116s 304ms/step - loss: 0.0598 - accuracy: 0.9959 - val_loss: 0.1944 - val_accuracy: 0.9598 - lr: 3.8152e-05\n",
            "Epoch 122/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0598 - accuracy: 0.9961 - val_loss: 0.1942 - val_accuracy: 0.9576 - lr: 3.8152e-05\n",
            "Epoch 123/500\n",
            "383/383 [==============================] - 116s 304ms/step - loss: 0.0597 - accuracy: 0.9962 - val_loss: 0.1990 - val_accuracy: 0.9590 - lr: 3.4337e-05\n",
            "Epoch 124/500\n",
            "383/383 [==============================] - 124s 324ms/step - loss: 0.0598 - accuracy: 0.9971 - val_loss: 0.1959 - val_accuracy: 0.9588 - lr: 3.4337e-05\n",
            "Epoch 125/500\n",
            "383/383 [==============================] - 116s 303ms/step - loss: 0.0597 - accuracy: 0.9964 - val_loss: 0.1955 - val_accuracy: 0.9586 - lr: 3.4337e-05\n",
            "Epoch 126/500\n",
            "383/383 [==============================] - 116s 304ms/step - loss: 0.0597 - accuracy: 0.9962 - val_loss: 0.1988 - val_accuracy: 0.9579 - lr: 3.0903e-05\n",
            "Epoch 127/500\n",
            "383/383 [==============================] - 116s 304ms/step - loss: 0.0597 - accuracy: 0.9964 - val_loss: 0.2007 - val_accuracy: 0.9571 - lr: 3.0903e-05\n",
            "Epoch 128/500\n",
            "383/383 [==============================] - 117s 304ms/step - loss: 0.0597 - accuracy: 0.9959 - val_loss: 0.1976 - val_accuracy: 0.9574 - lr: 3.0903e-05\n",
            "Epoch 129/500\n",
            "383/383 [==============================] - 116s 304ms/step - loss: 0.0597 - accuracy: 0.9960 - val_loss: 0.1976 - val_accuracy: 0.9593 - lr: 2.7813e-05\n",
            "Epoch 130/500\n",
            "383/383 [==============================] - 124s 324ms/step - loss: 0.0597 - accuracy: 0.9963 - val_loss: 0.1983 - val_accuracy: 0.9589 - lr: 2.7813e-05\n",
            "Epoch 131/500\n",
            "383/383 [==============================] - 117s 305ms/step - loss: 0.0597 - accuracy: 0.9964 - val_loss: 0.2019 - val_accuracy: 0.9561 - lr: 2.7813e-05\n",
            "Epoch 132/500\n",
            "383/383 [==============================] - 116s 304ms/step - loss: 0.0597 - accuracy: 0.9966 - val_loss: 0.1973 - val_accuracy: 0.9590 - lr: 2.5032e-05\n",
            "Epoch 133/500\n",
            "383/383 [==============================] - 117s 304ms/step - loss: 0.0597 - accuracy: 0.9962 - val_loss: 0.2005 - val_accuracy: 0.9590 - lr: 2.5032e-05\n",
            "Epoch 134/500\n",
            "383/383 [==============================] - 116s 304ms/step - loss: 0.0597 - accuracy: 0.9965 - val_loss: 0.1980 - val_accuracy: 0.9610 - lr: 2.5032e-05\n",
            "Epoch 135/500\n",
            "383/383 [==============================] - 116s 304ms/step - loss: 0.0597 - accuracy: 0.9962 - val_loss: 0.1983 - val_accuracy: 0.9582 - lr: 2.2528e-05\n",
            "Epoch 136/500\n",
            "383/383 [==============================] - 116s 304ms/step - loss: 0.0596 - accuracy: 0.9960 - val_loss: 0.1992 - val_accuracy: 0.9580 - lr: 2.2528e-05\n",
            "Epoch 137/500\n",
            "383/383 [==============================] - 116s 304ms/step - loss: 0.0597 - accuracy: 0.9964 - val_loss: 0.1976 - val_accuracy: 0.9590 - lr: 2.2528e-05\n",
            "Epoch 138/500\n",
            "383/383 [==============================] - 116s 304ms/step - loss: 0.0597 - accuracy: 0.9969 - val_loss: 0.1979 - val_accuracy: 0.9579 - lr: 2.0276e-05\n",
            "Epoch 139/500\n",
            "383/383 [==============================] - 116s 304ms/step - loss: 0.0597 - accuracy: 0.9962 - val_loss: 0.1986 - val_accuracy: 0.9589 - lr: 2.0276e-05\n",
            "Epoch 140/500\n",
            "383/383 [==============================] - 116s 304ms/step - loss: 0.0596 - accuracy: 0.9965 - val_loss: 0.1972 - val_accuracy: 0.9580 - lr: 2.0276e-05\n",
            "Epoch 141/500\n",
            "383/383 [==============================] - 116s 304ms/step - loss: 0.0596 - accuracy: 0.9960 - val_loss: 0.2021 - val_accuracy: 0.9582 - lr: 1.8248e-05\n",
            "Epoch 142/500\n",
            "383/383 [==============================] - 116s 304ms/step - loss: 0.0596 - accuracy: 0.9963 - val_loss: 0.1987 - val_accuracy: 0.9598 - lr: 1.8248e-05\n",
            "Epoch 143/500\n",
            "383/383 [==============================] - 117s 304ms/step - loss: 0.0597 - accuracy: 0.9962 - val_loss: 0.1990 - val_accuracy: 0.9585 - lr: 1.8248e-05\n",
            "Epoch 144/500\n",
            "235/383 [=================>............] - ETA: 40s - loss: 0.0593 - accuracy: 0.9960"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/파이썬스터디 프로젝트/labeling_resnet_a1.h5')"
      ],
      "metadata": {
        "id": "Rq55UzIFotKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_eval()"
      ],
      "metadata": {
        "id": "IkejtLJno4Pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_res_net()\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
        "early_stopper = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=30, mode='auto')\n",
        "\n",
        "model.fit(\n",
        "    x=train_data_2,\n",
        "    y=train_labels_2,\n",
        "    epochs=500,\n",
        "    verbose=1,\n",
        "    validation_split=0.3,\n",
        "    batch_size=64,\n",
        "    callbacks=[lr_reducer, early_stopper]\n",
        ")"
      ],
      "metadata": {
        "id": "tpfArulIo1ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/파이썬스터디 프로젝트/labeling_resnet_b1.h5')"
      ],
      "metadata": {
        "id": "3az2Tj6vo1ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_eval()"
      ],
      "metadata": {
        "id": "pSFsarpfo7px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5kdx4FDw7ED"
      },
      "source": [
        "# 남은 데이터 라벨링하기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8F_ifi6w9Uq"
      },
      "outputs": [],
      "source": [
        "model1 = keras.models.load_model('/content/drive/MyDrive/파이썬스터디 프로젝트/labeling_resnet_a1.h5')\n",
        "model2 = keras.models.load_model('/content/drive/MyDrive/파이썬스터디 프로젝트/labeling_resnet_b1.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74E6i_eCxCNG"
      },
      "outputs": [],
      "source": [
        "unlab_label_1 = model1.predict(unlab_data_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tz_MEzhxXos"
      },
      "outputs": [],
      "source": [
        "unlab_label_2 = model2.predict(unlab_data_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiz6rZkT5ORw"
      },
      "outputs": [],
      "source": [
        "train_data_1 = np.concatenate([train_data_1, unlab_data_1], axis=0)\n",
        "train_labels_1 = np.concatenate([train_labels_1, unlab_label_1], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIe8iZLkD8pJ"
      },
      "outputs": [],
      "source": [
        "train_data_2 = np.concatenate([train_data_2, unlab_data_2], axis=0)\n",
        "train_labels_2 = np.concatenate([train_labels_2, unlab_label_2], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_1.shape"
      ],
      "metadata": {
        "id": "oiL7N4BM3zxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_2.shape"
      ],
      "metadata": {
        "id": "Iyw46HeL31p3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### cnn"
      ],
      "metadata": {
        "id": "YEpEOf-k3eET"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ru605cBREQ_8",
        "outputId": "f253523a-77a0-4a7a-b781-4ff92031ad92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2000\n",
            "657/657 [==============================] - 8s 11ms/step - loss: 1.0349 - accuracy: 0.6429 - val_loss: 0.6362 - val_accuracy: 0.7989 - lr: 0.0010\n",
            "Epoch 2/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.5660 - accuracy: 0.8134 - val_loss: 0.5578 - val_accuracy: 0.8264 - lr: 0.0010\n",
            "Epoch 3/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.5058 - accuracy: 0.8400 - val_loss: 0.4932 - val_accuracy: 0.8588 - lr: 0.0010\n",
            "Epoch 4/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.4663 - accuracy: 0.8589 - val_loss: 0.4715 - val_accuracy: 0.8762 - lr: 0.0010\n",
            "Epoch 5/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.4360 - accuracy: 0.8760 - val_loss: 0.4621 - val_accuracy: 0.8806 - lr: 0.0010\n",
            "Epoch 6/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.4188 - accuracy: 0.8819 - val_loss: 0.4362 - val_accuracy: 0.8884 - lr: 0.0010\n",
            "Epoch 7/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.4075 - accuracy: 0.8878 - val_loss: 0.4221 - val_accuracy: 0.8981 - lr: 0.0010\n",
            "Epoch 8/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3945 - accuracy: 0.8931 - val_loss: 0.4406 - val_accuracy: 0.8882 - lr: 0.0010\n",
            "Epoch 9/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3879 - accuracy: 0.8967 - val_loss: 0.4203 - val_accuracy: 0.8972 - lr: 0.0010\n",
            "Epoch 10/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3824 - accuracy: 0.8983 - val_loss: 0.4495 - val_accuracy: 0.8929 - lr: 0.0010\n",
            "Epoch 11/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3804 - accuracy: 0.8999 - val_loss: 0.4114 - val_accuracy: 0.9031 - lr: 0.0010\n",
            "Epoch 12/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3739 - accuracy: 0.9029 - val_loss: 0.4107 - val_accuracy: 0.9036 - lr: 0.0010\n",
            "Epoch 13/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3653 - accuracy: 0.9057 - val_loss: 0.4149 - val_accuracy: 0.9018 - lr: 0.0010\n",
            "Epoch 14/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3657 - accuracy: 0.9074 - val_loss: 0.4064 - val_accuracy: 0.9073 - lr: 0.0010\n",
            "Epoch 15/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3645 - accuracy: 0.9076 - val_loss: 0.3977 - val_accuracy: 0.9104 - lr: 0.0010\n",
            "Epoch 16/2000\n",
            "657/657 [==============================] - 8s 12ms/step - loss: 0.3595 - accuracy: 0.9079 - val_loss: 0.4338 - val_accuracy: 0.8896 - lr: 0.0010\n",
            "Epoch 17/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3605 - accuracy: 0.9070 - val_loss: 0.4013 - val_accuracy: 0.9097 - lr: 0.0010\n",
            "Epoch 18/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3547 - accuracy: 0.9111 - val_loss: 0.4021 - val_accuracy: 0.9104 - lr: 0.0010\n",
            "Epoch 19/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3479 - accuracy: 0.9132 - val_loss: 0.4032 - val_accuracy: 0.9084 - lr: 9.0000e-04\n",
            "Epoch 20/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3445 - accuracy: 0.9140 - val_loss: 0.3989 - val_accuracy: 0.9122 - lr: 9.0000e-04\n",
            "Epoch 21/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3432 - accuracy: 0.9144 - val_loss: 0.3947 - val_accuracy: 0.9125 - lr: 9.0000e-04\n",
            "Epoch 22/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3464 - accuracy: 0.9144 - val_loss: 0.4070 - val_accuracy: 0.9089 - lr: 9.0000e-04\n",
            "Epoch 23/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3423 - accuracy: 0.9140 - val_loss: 0.3984 - val_accuracy: 0.9092 - lr: 9.0000e-04\n",
            "Epoch 24/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3396 - accuracy: 0.9179 - val_loss: 0.3976 - val_accuracy: 0.9098 - lr: 9.0000e-04\n",
            "Epoch 25/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3363 - accuracy: 0.9189 - val_loss: 0.3916 - val_accuracy: 0.9143 - lr: 8.1000e-04\n",
            "Epoch 26/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3355 - accuracy: 0.9193 - val_loss: 0.3916 - val_accuracy: 0.9160 - lr: 8.1000e-04\n",
            "Epoch 27/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3305 - accuracy: 0.9207 - val_loss: 0.3888 - val_accuracy: 0.9130 - lr: 8.1000e-04\n",
            "Epoch 28/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3321 - accuracy: 0.9215 - val_loss: 0.3890 - val_accuracy: 0.9139 - lr: 8.1000e-04\n",
            "Epoch 29/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3265 - accuracy: 0.9217 - val_loss: 0.3850 - val_accuracy: 0.9169 - lr: 8.1000e-04\n",
            "Epoch 30/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3255 - accuracy: 0.9227 - val_loss: 0.3844 - val_accuracy: 0.9203 - lr: 8.1000e-04\n",
            "Epoch 31/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3250 - accuracy: 0.9235 - val_loss: 0.3862 - val_accuracy: 0.9177 - lr: 8.1000e-04\n",
            "Epoch 32/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3284 - accuracy: 0.9226 - val_loss: 0.3794 - val_accuracy: 0.9196 - lr: 8.1000e-04\n",
            "Epoch 33/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3231 - accuracy: 0.9250 - val_loss: 0.3897 - val_accuracy: 0.9161 - lr: 8.1000e-04\n",
            "Epoch 34/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3239 - accuracy: 0.9242 - val_loss: 0.3905 - val_accuracy: 0.9145 - lr: 8.1000e-04\n",
            "Epoch 35/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3224 - accuracy: 0.9264 - val_loss: 0.3840 - val_accuracy: 0.9191 - lr: 8.1000e-04\n",
            "Epoch 36/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3166 - accuracy: 0.9279 - val_loss: 0.3848 - val_accuracy: 0.9203 - lr: 7.2900e-04\n",
            "Epoch 37/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3189 - accuracy: 0.9262 - val_loss: 0.3813 - val_accuracy: 0.9176 - lr: 7.2900e-04\n",
            "Epoch 38/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3157 - accuracy: 0.9278 - val_loss: 0.3753 - val_accuracy: 0.9198 - lr: 7.2900e-04\n",
            "Epoch 39/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3156 - accuracy: 0.9295 - val_loss: 0.3778 - val_accuracy: 0.9213 - lr: 7.2900e-04\n",
            "Epoch 40/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3135 - accuracy: 0.9290 - val_loss: 0.3792 - val_accuracy: 0.9229 - lr: 7.2900e-04\n",
            "Epoch 41/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3122 - accuracy: 0.9289 - val_loss: 0.3781 - val_accuracy: 0.9200 - lr: 7.2900e-04\n",
            "Epoch 42/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3099 - accuracy: 0.9312 - val_loss: 0.3746 - val_accuracy: 0.9222 - lr: 6.5610e-04\n",
            "Epoch 43/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3071 - accuracy: 0.9320 - val_loss: 0.3788 - val_accuracy: 0.9213 - lr: 6.5610e-04\n",
            "Epoch 44/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3101 - accuracy: 0.9308 - val_loss: 0.3776 - val_accuracy: 0.9195 - lr: 6.5610e-04\n",
            "Epoch 45/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3040 - accuracy: 0.9335 - val_loss: 0.3768 - val_accuracy: 0.9241 - lr: 6.5610e-04\n",
            "Epoch 46/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3033 - accuracy: 0.9340 - val_loss: 0.3713 - val_accuracy: 0.9253 - lr: 5.9049e-04\n",
            "Epoch 47/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3011 - accuracy: 0.9339 - val_loss: 0.3764 - val_accuracy: 0.9236 - lr: 5.9049e-04\n",
            "Epoch 48/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3015 - accuracy: 0.9347 - val_loss: 0.3750 - val_accuracy: 0.9218 - lr: 5.9049e-04\n",
            "Epoch 49/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3002 - accuracy: 0.9348 - val_loss: 0.3733 - val_accuracy: 0.9253 - lr: 5.9049e-04\n",
            "Epoch 50/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2971 - accuracy: 0.9364 - val_loss: 0.3712 - val_accuracy: 0.9242 - lr: 5.3144e-04\n",
            "Epoch 51/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2976 - accuracy: 0.9374 - val_loss: 0.3719 - val_accuracy: 0.9240 - lr: 5.3144e-04\n",
            "Epoch 52/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2941 - accuracy: 0.9384 - val_loss: 0.3702 - val_accuracy: 0.9244 - lr: 5.3144e-04\n",
            "Epoch 53/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2943 - accuracy: 0.9386 - val_loss: 0.3730 - val_accuracy: 0.9252 - lr: 5.3144e-04\n",
            "Epoch 54/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2925 - accuracy: 0.9382 - val_loss: 0.3677 - val_accuracy: 0.9264 - lr: 5.3144e-04\n",
            "Epoch 55/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2910 - accuracy: 0.9391 - val_loss: 0.3702 - val_accuracy: 0.9233 - lr: 5.3144e-04\n",
            "Epoch 56/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2920 - accuracy: 0.9385 - val_loss: 0.3670 - val_accuracy: 0.9262 - lr: 5.3144e-04\n",
            "Epoch 57/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2920 - accuracy: 0.9393 - val_loss: 0.3730 - val_accuracy: 0.9249 - lr: 5.3144e-04\n",
            "Epoch 58/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2899 - accuracy: 0.9392 - val_loss: 0.3688 - val_accuracy: 0.9252 - lr: 5.3144e-04\n",
            "Epoch 59/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2910 - accuracy: 0.9398 - val_loss: 0.3684 - val_accuracy: 0.9271 - lr: 5.3144e-04\n",
            "Epoch 60/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2865 - accuracy: 0.9415 - val_loss: 0.3647 - val_accuracy: 0.9273 - lr: 4.7830e-04\n",
            "Epoch 61/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2865 - accuracy: 0.9410 - val_loss: 0.3661 - val_accuracy: 0.9257 - lr: 4.7830e-04\n",
            "Epoch 62/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2862 - accuracy: 0.9421 - val_loss: 0.3632 - val_accuracy: 0.9273 - lr: 4.7830e-04\n",
            "Epoch 63/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2841 - accuracy: 0.9431 - val_loss: 0.3657 - val_accuracy: 0.9278 - lr: 4.7830e-04\n",
            "Epoch 64/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2824 - accuracy: 0.9441 - val_loss: 0.3656 - val_accuracy: 0.9259 - lr: 4.7830e-04\n",
            "Epoch 65/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2821 - accuracy: 0.9440 - val_loss: 0.3645 - val_accuracy: 0.9300 - lr: 4.7830e-04\n",
            "Epoch 66/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2825 - accuracy: 0.9432 - val_loss: 0.3630 - val_accuracy: 0.9305 - lr: 4.3047e-04\n",
            "Epoch 67/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2828 - accuracy: 0.9436 - val_loss: 0.3631 - val_accuracy: 0.9291 - lr: 4.3047e-04\n",
            "Epoch 68/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2809 - accuracy: 0.9440 - val_loss: 0.3626 - val_accuracy: 0.9302 - lr: 4.3047e-04\n",
            "Epoch 69/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2796 - accuracy: 0.9463 - val_loss: 0.3612 - val_accuracy: 0.9288 - lr: 4.3047e-04\n",
            "Epoch 70/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2784 - accuracy: 0.9451 - val_loss: 0.3621 - val_accuracy: 0.9319 - lr: 4.3047e-04\n",
            "Epoch 71/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2800 - accuracy: 0.9465 - val_loss: 0.3661 - val_accuracy: 0.9269 - lr: 4.3047e-04\n",
            "Epoch 72/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2784 - accuracy: 0.9453 - val_loss: 0.3638 - val_accuracy: 0.9308 - lr: 4.3047e-04\n",
            "Epoch 73/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2772 - accuracy: 0.9450 - val_loss: 0.3637 - val_accuracy: 0.9294 - lr: 3.8742e-04\n",
            "Epoch 74/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2764 - accuracy: 0.9468 - val_loss: 0.3625 - val_accuracy: 0.9301 - lr: 3.8742e-04\n",
            "Epoch 75/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2757 - accuracy: 0.9464 - val_loss: 0.3637 - val_accuracy: 0.9282 - lr: 3.8742e-04\n",
            "Epoch 76/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2738 - accuracy: 0.9480 - val_loss: 0.3588 - val_accuracy: 0.9306 - lr: 3.4868e-04\n",
            "Epoch 77/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2719 - accuracy: 0.9505 - val_loss: 0.3589 - val_accuracy: 0.9309 - lr: 3.4868e-04\n",
            "Epoch 78/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2707 - accuracy: 0.9493 - val_loss: 0.3649 - val_accuracy: 0.9273 - lr: 3.4868e-04\n",
            "Epoch 79/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2712 - accuracy: 0.9490 - val_loss: 0.3612 - val_accuracy: 0.9303 - lr: 3.4868e-04\n",
            "Epoch 80/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2700 - accuracy: 0.9493 - val_loss: 0.3599 - val_accuracy: 0.9311 - lr: 3.1381e-04\n",
            "Epoch 81/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2688 - accuracy: 0.9492 - val_loss: 0.3604 - val_accuracy: 0.9299 - lr: 3.1381e-04\n",
            "Epoch 82/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2688 - accuracy: 0.9502 - val_loss: 0.3616 - val_accuracy: 0.9308 - lr: 3.1381e-04\n",
            "Epoch 83/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2669 - accuracy: 0.9496 - val_loss: 0.3589 - val_accuracy: 0.9283 - lr: 2.8243e-04\n",
            "Epoch 84/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2673 - accuracy: 0.9512 - val_loss: 0.3583 - val_accuracy: 0.9299 - lr: 2.8243e-04\n",
            "Epoch 85/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2667 - accuracy: 0.9526 - val_loss: 0.3605 - val_accuracy: 0.9291 - lr: 2.8243e-04\n",
            "Epoch 86/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2658 - accuracy: 0.9510 - val_loss: 0.3559 - val_accuracy: 0.9319 - lr: 2.8243e-04\n",
            "Epoch 87/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2656 - accuracy: 0.9520 - val_loss: 0.3585 - val_accuracy: 0.9313 - lr: 2.8243e-04\n",
            "Epoch 88/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2656 - accuracy: 0.9516 - val_loss: 0.3580 - val_accuracy: 0.9334 - lr: 2.8243e-04\n",
            "Epoch 89/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2651 - accuracy: 0.9525 - val_loss: 0.3561 - val_accuracy: 0.9313 - lr: 2.8243e-04\n",
            "Epoch 90/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2638 - accuracy: 0.9515 - val_loss: 0.3595 - val_accuracy: 0.9306 - lr: 2.5419e-04\n",
            "Epoch 91/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2623 - accuracy: 0.9529 - val_loss: 0.3588 - val_accuracy: 0.9328 - lr: 2.5419e-04\n",
            "Epoch 92/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2617 - accuracy: 0.9531 - val_loss: 0.3600 - val_accuracy: 0.9322 - lr: 2.5419e-04\n",
            "Epoch 93/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2616 - accuracy: 0.9531 - val_loss: 0.3554 - val_accuracy: 0.9327 - lr: 2.2877e-04\n",
            "Epoch 94/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2614 - accuracy: 0.9520 - val_loss: 0.3550 - val_accuracy: 0.9311 - lr: 2.2877e-04\n",
            "Epoch 95/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2601 - accuracy: 0.9534 - val_loss: 0.3542 - val_accuracy: 0.9319 - lr: 2.2877e-04\n",
            "Epoch 96/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2605 - accuracy: 0.9554 - val_loss: 0.3571 - val_accuracy: 0.9348 - lr: 2.2877e-04\n",
            "Epoch 97/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2609 - accuracy: 0.9528 - val_loss: 0.3578 - val_accuracy: 0.9330 - lr: 2.2877e-04\n",
            "Epoch 98/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2592 - accuracy: 0.9554 - val_loss: 0.3551 - val_accuracy: 0.9324 - lr: 2.2877e-04\n",
            "Epoch 99/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2575 - accuracy: 0.9551 - val_loss: 0.3552 - val_accuracy: 0.9323 - lr: 2.0589e-04\n",
            "Epoch 100/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2582 - accuracy: 0.9558 - val_loss: 0.3549 - val_accuracy: 0.9324 - lr: 2.0589e-04\n",
            "Epoch 101/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2578 - accuracy: 0.9548 - val_loss: 0.3533 - val_accuracy: 0.9350 - lr: 2.0589e-04\n",
            "Epoch 102/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2583 - accuracy: 0.9551 - val_loss: 0.3556 - val_accuracy: 0.9323 - lr: 2.0589e-04\n",
            "Epoch 103/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2584 - accuracy: 0.9553 - val_loss: 0.3560 - val_accuracy: 0.9309 - lr: 2.0589e-04\n",
            "Epoch 104/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2566 - accuracy: 0.9561 - val_loss: 0.3519 - val_accuracy: 0.9374 - lr: 2.0589e-04\n",
            "Epoch 105/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2569 - accuracy: 0.9555 - val_loss: 0.3543 - val_accuracy: 0.9335 - lr: 2.0589e-04\n",
            "Epoch 106/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2555 - accuracy: 0.9571 - val_loss: 0.3546 - val_accuracy: 0.9334 - lr: 2.0589e-04\n",
            "Epoch 107/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2559 - accuracy: 0.9572 - val_loss: 0.3553 - val_accuracy: 0.9337 - lr: 2.0589e-04\n",
            "Epoch 108/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2552 - accuracy: 0.9570 - val_loss: 0.3550 - val_accuracy: 0.9334 - lr: 1.8530e-04\n",
            "Epoch 109/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2549 - accuracy: 0.9557 - val_loss: 0.3561 - val_accuracy: 0.9307 - lr: 1.8530e-04\n",
            "Epoch 110/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2551 - accuracy: 0.9569 - val_loss: 0.3561 - val_accuracy: 0.9334 - lr: 1.8530e-04\n",
            "Epoch 111/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2537 - accuracy: 0.9558 - val_loss: 0.3543 - val_accuracy: 0.9321 - lr: 1.6677e-04\n",
            "Epoch 112/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2532 - accuracy: 0.9581 - val_loss: 0.3538 - val_accuracy: 0.9329 - lr: 1.6677e-04\n",
            "Epoch 113/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2537 - accuracy: 0.9568 - val_loss: 0.3550 - val_accuracy: 0.9324 - lr: 1.6677e-04\n",
            "Epoch 114/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2519 - accuracy: 0.9597 - val_loss: 0.3542 - val_accuracy: 0.9353 - lr: 1.5009e-04\n",
            "Epoch 115/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2533 - accuracy: 0.9580 - val_loss: 0.3534 - val_accuracy: 0.9320 - lr: 1.5009e-04\n",
            "Epoch 116/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2519 - accuracy: 0.9599 - val_loss: 0.3545 - val_accuracy: 0.9334 - lr: 1.5009e-04\n",
            "Epoch 117/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2519 - accuracy: 0.9579 - val_loss: 0.3550 - val_accuracy: 0.9341 - lr: 1.3509e-04\n",
            "Epoch 118/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2509 - accuracy: 0.9594 - val_loss: 0.3542 - val_accuracy: 0.9334 - lr: 1.3509e-04\n",
            "Epoch 119/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2517 - accuracy: 0.9594 - val_loss: 0.3530 - val_accuracy: 0.9344 - lr: 1.3509e-04\n",
            "Epoch 120/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2503 - accuracy: 0.9593 - val_loss: 0.3533 - val_accuracy: 0.9349 - lr: 1.2158e-04\n",
            "Epoch 121/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2508 - accuracy: 0.9589 - val_loss: 0.3538 - val_accuracy: 0.9336 - lr: 1.2158e-04\n",
            "Epoch 122/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2503 - accuracy: 0.9594 - val_loss: 0.3541 - val_accuracy: 0.9332 - lr: 1.2158e-04\n",
            "Epoch 123/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2493 - accuracy: 0.9593 - val_loss: 0.3510 - val_accuracy: 0.9339 - lr: 1.0942e-04\n",
            "Epoch 124/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2495 - accuracy: 0.9602 - val_loss: 0.3531 - val_accuracy: 0.9323 - lr: 1.0942e-04\n",
            "Epoch 125/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2486 - accuracy: 0.9603 - val_loss: 0.3527 - val_accuracy: 0.9360 - lr: 1.0942e-04\n",
            "Epoch 126/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2496 - accuracy: 0.9589 - val_loss: 0.3519 - val_accuracy: 0.9342 - lr: 1.0942e-04\n",
            "Epoch 127/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2486 - accuracy: 0.9600 - val_loss: 0.3533 - val_accuracy: 0.9313 - lr: 9.8477e-05\n",
            "Epoch 128/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2483 - accuracy: 0.9606 - val_loss: 0.3491 - val_accuracy: 0.9343 - lr: 9.8477e-05\n",
            "Epoch 129/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2474 - accuracy: 0.9613 - val_loss: 0.3521 - val_accuracy: 0.9345 - lr: 9.8477e-05\n",
            "Epoch 130/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2493 - accuracy: 0.9594 - val_loss: 0.3505 - val_accuracy: 0.9316 - lr: 9.8477e-05\n",
            "Epoch 131/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2481 - accuracy: 0.9604 - val_loss: 0.3490 - val_accuracy: 0.9349 - lr: 9.8477e-05\n",
            "Epoch 132/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2478 - accuracy: 0.9596 - val_loss: 0.3484 - val_accuracy: 0.9358 - lr: 8.8629e-05\n",
            "Epoch 133/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2480 - accuracy: 0.9596 - val_loss: 0.3502 - val_accuracy: 0.9344 - lr: 8.8629e-05\n",
            "Epoch 134/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2471 - accuracy: 0.9603 - val_loss: 0.3519 - val_accuracy: 0.9346 - lr: 8.8629e-05\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0e0cf71f90>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "#######HYPERPARAMATERS###########\n",
        "epochs = 2000\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "#################################\n",
        "  \n",
        "model = Sequential()\n",
        "    \n",
        "model.add(Conv2D(32, kernel_size = (3,3), input_shape=(28,28,1), activation='relu'))\n",
        "\n",
        "model.add(Conv2D(64,(3,3), activation='relu'))\n",
        "model.add(Conv2D(64,(3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(PermaDropout(0.5))\n",
        "\n",
        "model.add(Conv2D(128,(3,3), activation='relu'))\n",
        "model.add(Conv2D(128,(3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(PermaDropout(0.5))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(PermaDropout(0.5))\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(PermaDropout(0.5))\n",
        "\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "adam = keras.optimizers.Adam(learning_rate)\n",
        "model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    \n",
        "# print(model.summary())\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
        "early_stopper = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=30, mode='auto')\n",
        "checkpointer = ModelCheckpoint('weights.hd5', monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "\n",
        "model.fit(\n",
        "          train_data_1,\n",
        "          train_labels_1,\n",
        "          epochs = epochs,\n",
        "          batch_size = batch_size,\n",
        "          validation_split = 0.3,\n",
        "          shuffle = True,\n",
        "          callbacks=[lr_reducer, early_stopper]\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuG_m7tXEi2g"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/MyDrive/파이썬스터디 프로젝트/labeling_cnn3.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDtQKCLqElcC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47e90a68-44be-4188-9c07-123c246698ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8952"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "model = keras.models.load_model('/content/drive/MyDrive/파이썬스터디 프로젝트/labeling_cnn3.h5')\n",
        "model_eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJwnKwlXEVf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d194acc3-e027-4024-db1c-31a4b25af6eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2000\n",
            "657/657 [==============================] - 8s 11ms/step - loss: 1.0688 - accuracy: 0.6314 - val_loss: 0.6529 - val_accuracy: 0.7936 - lr: 0.0010\n",
            "Epoch 2/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.5860 - accuracy: 0.8049 - val_loss: 0.5950 - val_accuracy: 0.8197 - lr: 0.0010\n",
            "Epoch 3/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.5214 - accuracy: 0.8291 - val_loss: 0.5220 - val_accuracy: 0.8512 - lr: 0.0010\n",
            "Epoch 4/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.4783 - accuracy: 0.8530 - val_loss: 0.4864 - val_accuracy: 0.8728 - lr: 0.0010\n",
            "Epoch 5/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.4484 - accuracy: 0.8699 - val_loss: 0.4764 - val_accuracy: 0.8792 - lr: 0.0010\n",
            "Epoch 6/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.4260 - accuracy: 0.8772 - val_loss: 0.4519 - val_accuracy: 0.8859 - lr: 0.0010\n",
            "Epoch 7/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.4166 - accuracy: 0.8819 - val_loss: 0.4441 - val_accuracy: 0.8887 - lr: 0.0010\n",
            "Epoch 8/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.4078 - accuracy: 0.8865 - val_loss: 0.4440 - val_accuracy: 0.8864 - lr: 0.0010\n",
            "Epoch 9/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3984 - accuracy: 0.8913 - val_loss: 0.4384 - val_accuracy: 0.8937 - lr: 0.0010\n",
            "Epoch 10/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3932 - accuracy: 0.8922 - val_loss: 0.4271 - val_accuracy: 0.9011 - lr: 0.0010\n",
            "Epoch 11/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3881 - accuracy: 0.8945 - val_loss: 0.4356 - val_accuracy: 0.8978 - lr: 0.0010\n",
            "Epoch 12/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3850 - accuracy: 0.8980 - val_loss: 0.4219 - val_accuracy: 0.8992 - lr: 0.0010\n",
            "Epoch 13/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3761 - accuracy: 0.9014 - val_loss: 0.4179 - val_accuracy: 0.9047 - lr: 0.0010\n",
            "Epoch 14/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3756 - accuracy: 0.9012 - val_loss: 0.4159 - val_accuracy: 0.9027 - lr: 0.0010\n",
            "Epoch 15/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3696 - accuracy: 0.9022 - val_loss: 0.4225 - val_accuracy: 0.9007 - lr: 0.0010\n",
            "Epoch 16/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3728 - accuracy: 0.9026 - val_loss: 0.4202 - val_accuracy: 0.9021 - lr: 0.0010\n",
            "Epoch 17/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3693 - accuracy: 0.9046 - val_loss: 0.4044 - val_accuracy: 0.9077 - lr: 0.0010\n",
            "Epoch 18/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3656 - accuracy: 0.9039 - val_loss: 0.4073 - val_accuracy: 0.9074 - lr: 0.0010\n",
            "Epoch 19/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3623 - accuracy: 0.9044 - val_loss: 0.4049 - val_accuracy: 0.9083 - lr: 0.0010\n",
            "Epoch 20/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3633 - accuracy: 0.9053 - val_loss: 0.4134 - val_accuracy: 0.9076 - lr: 0.0010\n",
            "Epoch 21/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3554 - accuracy: 0.9086 - val_loss: 0.4068 - val_accuracy: 0.9076 - lr: 9.0000e-04\n",
            "Epoch 22/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3549 - accuracy: 0.9097 - val_loss: 0.3987 - val_accuracy: 0.9097 - lr: 9.0000e-04\n",
            "Epoch 23/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3537 - accuracy: 0.9105 - val_loss: 0.4031 - val_accuracy: 0.9113 - lr: 9.0000e-04\n",
            "Epoch 24/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3537 - accuracy: 0.9093 - val_loss: 0.4147 - val_accuracy: 0.9077 - lr: 9.0000e-04\n",
            "Epoch 25/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3523 - accuracy: 0.9097 - val_loss: 0.4105 - val_accuracy: 0.9069 - lr: 9.0000e-04\n",
            "Epoch 26/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3455 - accuracy: 0.9129 - val_loss: 0.4107 - val_accuracy: 0.9091 - lr: 8.1000e-04\n",
            "Epoch 27/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3399 - accuracy: 0.9160 - val_loss: 0.4048 - val_accuracy: 0.9088 - lr: 8.1000e-04\n",
            "Epoch 28/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3410 - accuracy: 0.9159 - val_loss: 0.3982 - val_accuracy: 0.9119 - lr: 8.1000e-04\n",
            "Epoch 29/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3395 - accuracy: 0.9152 - val_loss: 0.4003 - val_accuracy: 0.9111 - lr: 8.1000e-04\n",
            "Epoch 30/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3427 - accuracy: 0.9149 - val_loss: 0.3980 - val_accuracy: 0.9114 - lr: 8.1000e-04\n",
            "Epoch 31/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3391 - accuracy: 0.9153 - val_loss: 0.3956 - val_accuracy: 0.9116 - lr: 8.1000e-04\n",
            "Epoch 32/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3374 - accuracy: 0.9145 - val_loss: 0.3957 - val_accuracy: 0.9136 - lr: 8.1000e-04\n",
            "Epoch 33/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3360 - accuracy: 0.9185 - val_loss: 0.3955 - val_accuracy: 0.9139 - lr: 8.1000e-04\n",
            "Epoch 34/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3401 - accuracy: 0.9149 - val_loss: 0.3986 - val_accuracy: 0.9129 - lr: 8.1000e-04\n",
            "Epoch 35/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3328 - accuracy: 0.9174 - val_loss: 0.3948 - val_accuracy: 0.9157 - lr: 8.1000e-04\n",
            "Epoch 36/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3359 - accuracy: 0.9184 - val_loss: 0.4005 - val_accuracy: 0.9149 - lr: 8.1000e-04\n",
            "Epoch 37/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3346 - accuracy: 0.9186 - val_loss: 0.3958 - val_accuracy: 0.9137 - lr: 8.1000e-04\n",
            "Epoch 38/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3355 - accuracy: 0.9182 - val_loss: 0.3938 - val_accuracy: 0.9162 - lr: 8.1000e-04\n",
            "Epoch 39/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3314 - accuracy: 0.9185 - val_loss: 0.4020 - val_accuracy: 0.9107 - lr: 8.1000e-04\n",
            "Epoch 40/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3303 - accuracy: 0.9198 - val_loss: 0.4057 - val_accuracy: 0.9082 - lr: 8.1000e-04\n",
            "Epoch 41/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3319 - accuracy: 0.9191 - val_loss: 0.3953 - val_accuracy: 0.9151 - lr: 8.1000e-04\n",
            "Epoch 42/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3276 - accuracy: 0.9220 - val_loss: 0.3935 - val_accuracy: 0.9158 - lr: 7.2900e-04\n",
            "Epoch 43/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3263 - accuracy: 0.9219 - val_loss: 0.3899 - val_accuracy: 0.9171 - lr: 7.2900e-04\n",
            "Epoch 44/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3248 - accuracy: 0.9222 - val_loss: 0.3928 - val_accuracy: 0.9144 - lr: 7.2900e-04\n",
            "Epoch 45/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3232 - accuracy: 0.9239 - val_loss: 0.3961 - val_accuracy: 0.9157 - lr: 7.2900e-04\n",
            "Epoch 46/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3248 - accuracy: 0.9233 - val_loss: 0.3888 - val_accuracy: 0.9166 - lr: 7.2900e-04\n",
            "Epoch 47/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3226 - accuracy: 0.9237 - val_loss: 0.3943 - val_accuracy: 0.9167 - lr: 7.2900e-04\n",
            "Epoch 48/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3246 - accuracy: 0.9236 - val_loss: 0.3903 - val_accuracy: 0.9163 - lr: 7.2900e-04\n",
            "Epoch 49/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3206 - accuracy: 0.9241 - val_loss: 0.3982 - val_accuracy: 0.9161 - lr: 7.2900e-04\n",
            "Epoch 50/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3195 - accuracy: 0.9256 - val_loss: 0.3924 - val_accuracy: 0.9170 - lr: 6.5610e-04\n",
            "Epoch 51/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3180 - accuracy: 0.9254 - val_loss: 0.3910 - val_accuracy: 0.9184 - lr: 6.5610e-04\n",
            "Epoch 52/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3180 - accuracy: 0.9252 - val_loss: 0.3904 - val_accuracy: 0.9168 - lr: 6.5610e-04\n",
            "Epoch 53/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3139 - accuracy: 0.9265 - val_loss: 0.3890 - val_accuracy: 0.9174 - lr: 5.9049e-04\n",
            "Epoch 54/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3163 - accuracy: 0.9250 - val_loss: 0.3912 - val_accuracy: 0.9167 - lr: 5.9049e-04\n",
            "Epoch 55/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3152 - accuracy: 0.9272 - val_loss: 0.3858 - val_accuracy: 0.9173 - lr: 5.9049e-04\n",
            "Epoch 56/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3141 - accuracy: 0.9275 - val_loss: 0.3877 - val_accuracy: 0.9188 - lr: 5.9049e-04\n",
            "Epoch 57/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3139 - accuracy: 0.9274 - val_loss: 0.3859 - val_accuracy: 0.9188 - lr: 5.9049e-04\n",
            "Epoch 58/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3106 - accuracy: 0.9285 - val_loss: 0.3875 - val_accuracy: 0.9164 - lr: 5.9049e-04\n",
            "Epoch 59/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3098 - accuracy: 0.9313 - val_loss: 0.3920 - val_accuracy: 0.9182 - lr: 5.3144e-04\n",
            "Epoch 60/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3083 - accuracy: 0.9295 - val_loss: 0.3885 - val_accuracy: 0.9189 - lr: 5.3144e-04\n",
            "Epoch 61/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3062 - accuracy: 0.9327 - val_loss: 0.3894 - val_accuracy: 0.9156 - lr: 5.3144e-04\n",
            "Epoch 62/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3051 - accuracy: 0.9320 - val_loss: 0.3883 - val_accuracy: 0.9191 - lr: 4.7830e-04\n",
            "Epoch 63/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3015 - accuracy: 0.9335 - val_loss: 0.3857 - val_accuracy: 0.9209 - lr: 4.7830e-04\n",
            "Epoch 64/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3023 - accuracy: 0.9327 - val_loss: 0.3860 - val_accuracy: 0.9165 - lr: 4.7830e-04\n",
            "Epoch 65/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.3020 - accuracy: 0.9331 - val_loss: 0.3875 - val_accuracy: 0.9201 - lr: 4.7830e-04\n",
            "Epoch 66/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3004 - accuracy: 0.9329 - val_loss: 0.3820 - val_accuracy: 0.9196 - lr: 4.7830e-04\n",
            "Epoch 67/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3010 - accuracy: 0.9333 - val_loss: 0.3901 - val_accuracy: 0.9155 - lr: 4.7830e-04\n",
            "Epoch 68/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2991 - accuracy: 0.9351 - val_loss: 0.3918 - val_accuracy: 0.9172 - lr: 4.7830e-04\n",
            "Epoch 69/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.3002 - accuracy: 0.9335 - val_loss: 0.3857 - val_accuracy: 0.9192 - lr: 4.7830e-04\n",
            "Epoch 70/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2981 - accuracy: 0.9335 - val_loss: 0.3854 - val_accuracy: 0.9188 - lr: 4.3047e-04\n",
            "Epoch 71/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2967 - accuracy: 0.9363 - val_loss: 0.3805 - val_accuracy: 0.9256 - lr: 4.3047e-04\n",
            "Epoch 72/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2938 - accuracy: 0.9364 - val_loss: 0.3851 - val_accuracy: 0.9182 - lr: 4.3047e-04\n",
            "Epoch 73/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2973 - accuracy: 0.9347 - val_loss: 0.3819 - val_accuracy: 0.9199 - lr: 4.3047e-04\n",
            "Epoch 74/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2953 - accuracy: 0.9349 - val_loss: 0.3812 - val_accuracy: 0.9188 - lr: 4.3047e-04\n",
            "Epoch 75/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2929 - accuracy: 0.9377 - val_loss: 0.3794 - val_accuracy: 0.9208 - lr: 3.8742e-04\n",
            "Epoch 76/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2916 - accuracy: 0.9386 - val_loss: 0.3802 - val_accuracy: 0.9212 - lr: 3.8742e-04\n",
            "Epoch 77/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2913 - accuracy: 0.9379 - val_loss: 0.3842 - val_accuracy: 0.9192 - lr: 3.8742e-04\n",
            "Epoch 78/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2907 - accuracy: 0.9391 - val_loss: 0.3850 - val_accuracy: 0.9208 - lr: 3.8742e-04\n",
            "Epoch 79/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2895 - accuracy: 0.9392 - val_loss: 0.3817 - val_accuracy: 0.9195 - lr: 3.4868e-04\n",
            "Epoch 80/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2893 - accuracy: 0.9392 - val_loss: 0.3790 - val_accuracy: 0.9233 - lr: 3.4868e-04\n",
            "Epoch 81/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2883 - accuracy: 0.9385 - val_loss: 0.3772 - val_accuracy: 0.9219 - lr: 3.4868e-04\n",
            "Epoch 82/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2863 - accuracy: 0.9404 - val_loss: 0.3789 - val_accuracy: 0.9228 - lr: 3.4868e-04\n",
            "Epoch 83/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2858 - accuracy: 0.9393 - val_loss: 0.3812 - val_accuracy: 0.9203 - lr: 3.4868e-04\n",
            "Epoch 84/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2862 - accuracy: 0.9415 - val_loss: 0.3820 - val_accuracy: 0.9206 - lr: 3.4868e-04\n",
            "Epoch 85/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2858 - accuracy: 0.9394 - val_loss: 0.3832 - val_accuracy: 0.9213 - lr: 3.1381e-04\n",
            "Epoch 86/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2853 - accuracy: 0.9421 - val_loss: 0.3784 - val_accuracy: 0.9202 - lr: 3.1381e-04\n",
            "Epoch 87/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2861 - accuracy: 0.9412 - val_loss: 0.3815 - val_accuracy: 0.9209 - lr: 3.1381e-04\n",
            "Epoch 88/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2825 - accuracy: 0.9436 - val_loss: 0.3803 - val_accuracy: 0.9222 - lr: 2.8243e-04\n",
            "Epoch 89/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2818 - accuracy: 0.9431 - val_loss: 0.3796 - val_accuracy: 0.9226 - lr: 2.8243e-04\n",
            "Epoch 90/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2813 - accuracy: 0.9430 - val_loss: 0.3806 - val_accuracy: 0.9227 - lr: 2.8243e-04\n",
            "Epoch 91/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2805 - accuracy: 0.9430 - val_loss: 0.3759 - val_accuracy: 0.9234 - lr: 2.5419e-04\n",
            "Epoch 92/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2787 - accuracy: 0.9440 - val_loss: 0.3788 - val_accuracy: 0.9227 - lr: 2.5419e-04\n",
            "Epoch 93/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2781 - accuracy: 0.9443 - val_loss: 0.3758 - val_accuracy: 0.9244 - lr: 2.5419e-04\n",
            "Epoch 94/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2788 - accuracy: 0.9458 - val_loss: 0.3818 - val_accuracy: 0.9207 - lr: 2.5419e-04\n",
            "Epoch 95/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2790 - accuracy: 0.9448 - val_loss: 0.3820 - val_accuracy: 0.9216 - lr: 2.5419e-04\n",
            "Epoch 96/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2777 - accuracy: 0.9461 - val_loss: 0.3779 - val_accuracy: 0.9218 - lr: 2.5419e-04\n",
            "Epoch 97/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2779 - accuracy: 0.9443 - val_loss: 0.3788 - val_accuracy: 0.9233 - lr: 2.2877e-04\n",
            "Epoch 98/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2770 - accuracy: 0.9444 - val_loss: 0.3805 - val_accuracy: 0.9223 - lr: 2.2877e-04\n",
            "Epoch 99/2000\n",
            "657/657 [==============================] - 7s 10ms/step - loss: 0.2763 - accuracy: 0.9441 - val_loss: 0.3804 - val_accuracy: 0.9230 - lr: 2.2877e-04\n",
            "Epoch 100/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2756 - accuracy: 0.9453 - val_loss: 0.3756 - val_accuracy: 0.9251 - lr: 2.0589e-04\n",
            "Epoch 101/2000\n",
            "657/657 [==============================] - 7s 11ms/step - loss: 0.2741 - accuracy: 0.9468 - val_loss: 0.3753 - val_accuracy: 0.9209 - lr: 2.0589e-04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0efe2a0b90>"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "#######HYPERPARAMATERS###########\n",
        "epochs = 2000\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "#################################\n",
        "  \n",
        "model = Sequential()\n",
        "    \n",
        "model.add(Conv2D(32, kernel_size = (3,3), input_shape=(28,28,1), activation='relu'))\n",
        "\n",
        "model.add(Conv2D(64,(3,3), activation='relu'))\n",
        "model.add(Conv2D(64,(3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(PermaDropout(0.5))\n",
        "\n",
        "model.add(Conv2D(128,(3,3), activation='relu'))\n",
        "model.add(Conv2D(128,(3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(PermaDropout(0.5))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(PermaDropout(0.5))\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(PermaDropout(0.5))\n",
        "\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "adam = keras.optimizers.Adam(learning_rate)\n",
        "model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    \n",
        "# print(model.summary())\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
        "early_stopper = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=30, mode='auto')\n",
        "checkpointer = ModelCheckpoint('weights.hd5', monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "\n",
        "model.fit(\n",
        "          train_data_2,\n",
        "          train_labels_2,\n",
        "          epochs = epochs,\n",
        "          batch_size = batch_size,\n",
        "          validation_split = 0.3,\n",
        "          shuffle = True,\n",
        "          callbacks=[lr_reducer, early_stopper]\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xd-ishoDEnrl"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/MyDrive/파이썬스터디 프로젝트/labeling_cnn_no2.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hImsZIESEnrl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f50ae378-cd3d-4a3a-f889-c67710983706"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8944"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "model = keras.models.load_model('/content/drive/MyDrive/파이썬스터디 프로젝트/labeling_cnn_no2.h5')\n",
        "model_eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### resnet"
      ],
      "metadata": {
        "id": "alBqvm5E3bd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import Tensor\n",
        "from tensorflow.keras.layers import Input, Conv2D, ReLU, BatchNormalization,\\\n",
        "                                    Add, AveragePooling2D, Flatten, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def PermaDropout(rate):\n",
        "    return Lambda(lambda x: K.dropout(x, level=rate))\n",
        "\n",
        "def relu_bn(inputs: Tensor) -> Tensor:\n",
        "    relu = ReLU()(inputs)\n",
        "    bn = BatchNormalization()(relu)\n",
        "    return bn\n",
        "\n",
        "def residual_block(x: Tensor, downsample: bool, filters: int, kernel_size: int = 3) -> Tensor:\n",
        "    y = Conv2D(kernel_size=kernel_size,\n",
        "               strides= (1 if not downsample else 2),\n",
        "               filters=filters,\n",
        "               padding=\"same\")(x)\n",
        "    y = relu_bn(y)\n",
        "    y = Conv2D(kernel_size=kernel_size,\n",
        "               strides=1,\n",
        "               filters=filters,\n",
        "               padding=\"same\")(y)\n",
        "\n",
        "    if downsample:\n",
        "        x = Conv2D(kernel_size=1,\n",
        "                   strides=2,\n",
        "                   filters=filters,\n",
        "                   padding=\"same\")(x)\n",
        "    out = Add()([x, y])\n",
        "    out = relu_bn(out)\n",
        "    return out\n",
        "\n",
        "def create_res_net():\n",
        "    \n",
        "    inputs = Input(shape=(28, 28, 1))\n",
        "    num_filters = 64\n",
        "    \n",
        "    t = BatchNormalization()(inputs)\n",
        "    t = Conv2D(kernel_size=3,\n",
        "               strides=1,\n",
        "               filters=num_filters,\n",
        "               padding=\"same\")(t)\n",
        "    t = relu_bn(t)\n",
        "    t = PermaDropout(0.5)(t)\n",
        "    \n",
        "    num_blocks_list = [2, 5, 5, 2]\n",
        "    for i in range(len(num_blocks_list)):\n",
        "        num_blocks = num_blocks_list[i]\n",
        "        for j in range(num_blocks):\n",
        "            t = residual_block(t, downsample=(j==0 and i!=0), filters=num_filters)\n",
        "        num_filters *= 2\n",
        "        t = PermaDropout(0.5)(t)\n",
        "    \n",
        "    t = AveragePooling2D(4)(t)\n",
        "    t = Flatten()(t)\n",
        "    outputs = Dense(10, activation='softmax')(t)\n",
        "    \n",
        "    model = Model(inputs, outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "6EmZ6sdC3ioe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_res_net()\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
        "early_stopper = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=30, mode='auto')\n",
        "\n",
        "model.fit(\n",
        "    x=train_data_1,\n",
        "    y=train_labels_1,\n",
        "    epochs=500,\n",
        "    verbose=1,\n",
        "    validation_split=0.3,\n",
        "    batch_size=64,\n",
        "    callbacks=[lr_reducer, early_stopper]\n",
        ")"
      ],
      "metadata": {
        "id": "ZGTGicAJ3iof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/파이썬스터디 프로젝트/labeling_resnet_a2.h5')"
      ],
      "metadata": {
        "id": "bIqztqCd3iog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_eval()"
      ],
      "metadata": {
        "id": "gRVgsApR3iog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_res_net()\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
        "early_stopper = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=30, mode='auto')\n",
        "\n",
        "model.fit(\n",
        "    x=train_data_2,\n",
        "    y=train_labels_2,\n",
        "    epochs=500,\n",
        "    verbose=1,\n",
        "    validation_split=0.3,\n",
        "    batch_size=64,\n",
        "    callbacks=[lr_reducer, early_stopper]\n",
        ")"
      ],
      "metadata": {
        "id": "Ko6EgtS13iog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/파이썬스터디 프로젝트/labeling_resnet_b2.h5')"
      ],
      "metadata": {
        "id": "brNeFnIz3ioh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_eval()"
      ],
      "metadata": {
        "id": "C5mCztBs3ioh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZvLar-vR36Rn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "BUrklzVkE66r",
        "IQb_xC_smn88",
        "YEpEOf-k3eET"
      ],
      "name": "uncertainty 기반 labeling.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}